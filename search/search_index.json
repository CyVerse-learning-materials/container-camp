{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-cyverse-container-cloud-native-camp-documentation","title":"Welcome to CyVerse Container &amp; Cloud Native Camp Documentation","text":""},{"location":"#go-to-816-818-workshop-agenda","title":"Go to 8/16 - 8/18 Workshop Agenda","text":"<p>Welcome to our documentation on containers and orchestration in academic research and education.</p>"},{"location":"#basic-container-camp","title":"Basic Container Camp","text":""},{"location":"#docker-containers-for-scientific-research","title":"Docker Containers for Scientific Research","text":"<ul> <li>Introduction to Docker</li> <li>Finding the Right Container</li> </ul>"},{"location":"#containers-on-high-performance-and-high-throughput-computing","title":"Containers on High Performance and High Throughput Computing","text":"<ul> <li>Basics of SingularityCE</li> <li>Using SingularityCE on HPC and HTC</li> </ul>"},{"location":"#cloud-native-camp","title":"Cloud Native Camp","text":""},{"location":"#orchestration","title":"Orchestration","text":"<ul> <li>Container Orchestration with Kubernetes</li> </ul>"},{"location":"#infrastructure-as-code","title":"Infrastructure as Code","text":"<ul> <li>Infrastructure as Code with Hashicorp Terraform</li> <li>CyVerse's Cloud Automation &amp; Continuous Analysis Orchestration (CACAO)</li> </ul> <p>Learning Objectives</p> <pre><code>After completing these two paired workshops, you should be able to:\n\n* Explain why containers and orchestration are used in research computing\n* Create your own containers and deploy your own orchestrated frameworks\n* Understand how and when to use containers and IaC in your daily work\n\nYou will also leave with\n\n- Understanding of how to use the most powerful public research computing infrastructure in the world via [ACCESS-CI](https://access-ci.org).\n- The ability to launch and manage distributed resources using IaC templates on commercial (AWS, GCloud, Azure) or public research (OpenStack) clouds.\n\n- Insight into commercial cloud services, their costs, and how to best utilize them for scientific research.\n</code></pre> <p>Funding and Citations:</p> <p>CyVerse is generously funded by the National Science Foundation . </p> <p>Here are our award numbers:</p> <ul> <li></li> <li></li> <li></li> </ul> <p>For any utilization of our resources, please adhere to the CyVerse citation policy.</p>"},{"location":"cloud/aws/","title":"Amazon Web Services","text":""},{"location":"cloud/aws/#creating-your-first-aws-account","title":"Creating Your First AWS Account","text":""},{"location":"cloud/aws/#introduction","title":"Introduction","text":"<p>This tutorial will guide you through the basics of creating your first AWS account.</p>"},{"location":"cloud/aws/#step-1-visit-the-aws-homepage","title":"Step 1: Visit the AWS Homepage","text":"<ul> <li>Navigate to the AWS homepage.</li> <li>Click on the \"Create an AWS Account\" button, usually located at the top right corner.</li> </ul>"},{"location":"cloud/aws/#step-2-sign-up","title":"Step 2: Sign Up","text":"<ul> <li>You'll be prompted to enter your email address. If you already have an Amazon account, you can use that email. Otherwise, create a new one.</li> <li>Choose \"I am a new user\" and click on \"Sign in using our secure server\".</li> <li>Fill in the required details, including your name, email, and desired password.</li> </ul>"},{"location":"cloud/aws/#step-3-contact-information","title":"Step 3: Contact Information","text":"<ul> <li>Choose \"Professional\" if you're setting this up for your organization or work. Otherwise, choose \"Personal\".</li> <li>Fill in the required contact details. Ensure the information is accurate, as AWS might use it for verification purposes.</li> </ul>"},{"location":"cloud/aws/#step-4-payment-information","title":"Step 4: Payment Information","text":"<ul> <li>Enter your credit card details. AWS requires this even for the Free Tier, but you won't be charged unless you exceed the Free Tier limits.</li> </ul>"},{"location":"cloud/aws/#step-5-identity-verification","title":"Step 5: Identity Verification","text":"<ul> <li>AWS will call you to verify your identity. Enter the PIN you receive during the call.</li> </ul>"},{"location":"cloud/aws/#step-6-choose-a-support-plan","title":"Step 6: Choose a Support Plan","text":"<ul> <li>AWS offers various support plans. For starters, you can choose the \"Basic Plan\" which is free.</li> </ul>"},{"location":"cloud/aws/#step-7-confirmation","title":"Step 7: Confirmation","text":"<ul> <li>Once all steps are completed, you'll receive a confirmation message. Congratulations, you now have an AWS account!</li> </ul>"},{"location":"cloud/aws/#step-8-access-aws-services","title":"Step 8: Access AWS Services","text":"<ul> <li>After setting up your account, you can access the AWS Management Console to explore various AWS services.</li> </ul> <p>Note: Always set a budget to monitor your usage to avoid unexpected charges. AWS Free Tier offers a generous amount of resources, but exceeding those limits can result in charges.</p>"},{"location":"cloud/codespaces/","title":"GitHub  CodeSpaces","text":"<p>For this workshop, we are working in GitHub  CodeSpaces. You will be given access to an organization where CodeSpaces have been enabled for the duration of the workshop. </p> What is a 'Development Environment'? <p>A development environment or \"dev environment\" for short, is a place where software can be written and tested without impacting users or \"production environments\" as part of the software lifecycle.</p> <p>Containers-within-containers are another way to think about this paradigm. We create a containerized Integrated Development Environment (IDE) container which has all of our favorite software development tools and visual software (e.g.,  VSCode) where we can write our program and try out new package installations.</p> <p>Terminology:</p> <p> Development Environment: Environment for development tasks such as designing, programming, debugging, etc.</p> <p> Test Environment: an environment with the same configuration as the \"production environment\" where testing can be done without interrupting the main service, also called Q/A or \"Quality Assurance Environment\". </p> <p> Staging Environment: Where the work from testing is merged into the built system before public release. </p> <p> Production Environment: The environment where users interact with the now-public tools. </p> <p> GitHub  CodeSpaces provides you with a fully featured Dev Environment running on  Microsoft Azure. </p> <p>You have been granted access to GitHub Education, and can use CodeSpaces and  CoPilot during the workshop.</p> <p> CodeSpace links with your GitHub account for a seamless experience working on code in a Git repository.</p> <p> CoPilot is an AI programmer assistant that can help you write code using comments as prompts.</p> How can you get your own educational GitHub &amp; CodeSpaces? <p>You can gain access to discounted GitHub CodeSpaces by enrolling your GitHub account with GitHub Education and then applying the educator discount to your organizations and repos.</p> <p>Not interested in using GitHub based dev environments? Check out GitPod or GitLub Education and Developer Environments</p> How do I use CoPilot? <p>Install and enable CoPilot on your VSCode (locally, logging into GitHub, or remotely in CodeSpace).</p> <p>Try adding a comment to the top of a file, like your new <code>Dockerfile</code>, wait a few seconds </p> <pre><code># Create a Dockerfile which uses an Ubuntu 22.04 featured base image, \nFROM ubuntu:22.04\n\n# then install Python 3.10 and pip,\nRUN apt-get update \\\n&amp;&amp; apt-get install -y python3.10 python3-pip\n\n#  and give it an ENTRYPOINT to run Python 3.10\nENTRYPOINT [ \"python3.10\" ]\n</code></pre>"},{"location":"cloud/codespaces/#starting-a-codespace","title":"Starting a CodeSpace","text":"<p>When a GitHub Organization and Repository have CodeSpaces enabled you will see a \"Code\" button above the README.md</p> <p></p> <p>Click on the \"Code\" button and start a new CodeSpace</p> <p></p> <p>Select the size of the CodeSpace you want (2-4 cores and 4GB to 8GB of RAM should be plenty for today)</p> <p></p> <p>Click \"Create CodeSpace\"</p> <p>You will be taken to a loading screen, and after a few moments (&lt;2 minutes) your browser will change to a VS Code instance in your browser.</p> <p></p> <p>Notice, the GitHub repository where you initiated the CodeSpace is set as the working directory of the EXPLORER  in the upper left side of VS Code interface. You're in your Git repo, and are able to work with Python, Docker, Node, or any one of many featured developer tools. Further, you can install any tools you like!</p> <p></p>"},{"location":"cloud/js2/","title":"Jetstream-2","text":""},{"location":"cloud/js2/#getting-started-with-nsf-access-ci-and-jetstream2","title":"Getting Started with NSF ACCESS-CI and Jetstream2","text":""},{"location":"cloud/js2/#1-creating-an-account-on-nsf-access-ci","title":"1. Creating an Account on NSF ACCESS-CI","text":"<p>Step 1: Navigate to the ACCESS User Registration page.</p> <p>Step 2: If you had an XSEDE Portal account, your ACCESS ID is the same. Do not create a new ACCESS ID. Instead, select the \u201cACCESS CI (XSEDE)\u201d identity provider to log in using your XSEDE account.</p> <p>Step 3: If you don't have an XSEDE or ACCESS account, you have two options:</p> <ul> <li>Register with an existing identity: Use an existing University account to simplify the sign-up process. With this option, creating an ACCESS-specific password is optional.</li> <li>Register without an existing identity: You'll be prompted to enter all your registration info and select an ACCESS-specific password.</li> </ul> <p>Step 4: During registration, ensure you complete the email verification. You'll receive an email from registry@cilogon.org with a URL to complete your registration.</p> <p>Note: Avoid exiting your browser during registration. If you face any issues, please contact the support team.</p>"},{"location":"cloud/js2/#2-connecting-to-jetstream2","title":"2. Connecting to Jetstream2","text":"<p>Step 1: Visit the Jetstream2 official End User documentation.</p> <p>Step 2: Jetstream2 offers multiple interfaces for creating, deploying, and managing Virtual Machines (VMs) and Storage. Familiarize yourself with the available interfaces:</p> <ul> <li>Exosphere: A web-based GUI for VM management.</li> <li>Horizon: Another web-based GUI with a different set of features.</li> <li>CACAO: Still in development, but offers unique capabilities.</li> <li>OpenStack CLI: For those who prefer command-line interfaces.</li> </ul> <p>Step 3: For beginners, it's recommended to start with the Exosphere interface. Here's a brief guide:</p> <ul> <li>Logging In: Use your ACCESS-CI credentials.</li> <li>Creating an Instance: Follow the on-screen instructions to deploy a VM.</li> <li>Accessing an Exosphere Instance: Once your VM is up, you can access it directly from the Exosphere dashboard.</li> </ul> <p>Step 4: Dive deeper into the Jetstream2 documentation to explore advanced features, troubleshooting guides, and more.</p> <p>Note: This tutorial provides a quick overview of getting started with NSF ACCESS-CI and Jetstream2. For comprehensive details, always refer to the official documentation provided by both platforms. </p>"},{"location":"cloud/openstack/","title":"OpenStack","text":""},{"location":"cloud/openstack/#connecting-to-an-existing-openstack-cloud","title":"Connecting to an Existing OpenStack Cloud","text":""},{"location":"cloud/openstack/#option-1-installing-openstack-cli-using-python","title":"Option 1: Installing OpenStack CLI using Python","text":"<p>Step 1: Prerequisites Ensure you have Python installed on your system. You can download Python from the official website: https://www.python.org/downloads/</p> <p>Step 2: Install OpenStack CLI Open your terminal or command prompt and execute the following command to install the OpenStack CLI using <code>pip</code>:</p> <pre><code>pip install python-openstackclient\n</code></pre> <p>Step 3: Verify the Installation To verify that the OpenStack CLI has been successfully installed, run the following command:</p> <pre><code>openstack --version\n</code></pre> <p>You should see the version number of the installed OpenStack CLI if the installation was successful.</p>"},{"location":"cloud/openstack/#option-2-installing-openstack-cli-using-homebrew-macos","title":"Option 2: Installing OpenStack CLI using Homebrew (MacOS)","text":"<p>Step 1: Prerequisites Ensure you have Homebrew installed on your MacOS. If you don't have Homebrew, you can install it using the following command in your terminal:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Step 2: Install OpenStack CLI Open your terminal and use Homebrew to install the OpenStack CLI:</p> <pre><code>brew install openstackclient\n</code></pre> <p>Step 3: Verify the Installation To verify that the OpenStack CLI has been successfully installed, run the following command:</p> <pre><code>openstack --version\n</code></pre> <p>You should see the version number of the installed OpenStack CLI if the installation was successful.</p>"},{"location":"cloud/openstack/#conclusion","title":"Conclusion","text":"<p>In this installation section, we provided two options for installing the OpenStack CLI. The first option uses Python's <code>pip</code> package manager, which is a cross-platform method for installation. The second option is specific to MacOS users and uses Homebrew as the package manager.</p> <p>Choose the option that best suits your system, and once the OpenStack CLI is installed, you can proceed with the main tutorial to connect to an existing OpenStack Cloud.</p> <p>If you already have the OpenStack CLI installed, you can skip this section and move on to the authentication and connection steps in the main tutorial. Happy cloud computing!</p>"},{"location":"docker/actions/","title":"GitHub Actions and Docker","text":"<p>GitHub Actions is a feature that allows automation and execution of workflows invoved in the development of your software and code. Read more on GitHub Actions at the offical GitHub Docs page.</p> <p>Actions can also be used to create and manage your Docker Images. </p> <p>Here you will learn how to can create Actions which build and then push images from GitHub to a Docker Registry of your choice. </p>"},{"location":"docker/actions/#prerequisites","title":"Prerequisites","text":"<ul> <li>A GitHub account</li> <li>A Docker account</li> </ul>"},{"location":"docker/actions/#setting-up-a-git-repository-with-actions-for-docker","title":"Setting up a Git Repository with Actions for Docker","text":"CI/CD Terminology <ul> <li> continuous integration builds, tests, and automatically accepts changes to code in a repository </li> <li> continuous delivery delivers code changes to production-ready environments</li> <li> continuous deployment does just that, it deploys changes to the code</li> <li> <p> CI pipeline compiles code, tests it, and makes sure all of your changes work. It should run whenever there is a change (push) to the repository</p> </li> <li> <p> CD pipeline goes one step further and deploys new code into production.</p> </li> </ul> <p>We are focusing on GitHub, but there are other platforms which you can explore for building and pushing containers to registries</p> <p>These include GitLab Runners </p> <p>Other types of Continuous Integration are used on code repositories to ensure that code stays functional. </p>"},{"location":"docker/actions/#create-a-github-repository","title":"Create a GitHub Repository","text":"<p>Navigate to your GitHub Account and select New;</p> <p></p>  Creating README, LICENSE, &amp; .gitignore <p>When you create your new git repository you are asked to create a <code>README</code>, a <code>LICENCE</code>, and a <code>.gitignore</code> file.</p> <p>Go ahead and create all three of these, as they are useful and fundamental to making your repository reproducible. </p> <p>README -- we want to use the README to help future-us when we revisit this repository. Make sure to include detailed instructions </p> <p>LICENSE -- pick a license which is useful for your specific software use case.</p> <p><code>.gitignore</code> -- use a file which will keep docker project files isolated</p> <code>.gitignore</code> example <pre><code>    # Docker project generated files to ignore\n    #  if you want to ignore files created by your editor/tools,\n    #  please consider a global .gitignore https://help.github.com/articles/ignoring-files\n    .vagrant*\n    bin\n    docker/docker\n    .*.swp\n    a.out\n    *.orig\n    build_src\n    .flymake*\n    .idea\n    .DS_Store\n    docs/_build\n    docs/_static\n    docs/_templates\n    .gopath/\n    .dotcloud\n    *.test\n    bundles/\n    .hg/\n    .git/\n    vendor/pkg/\n    pyenv\n    Vagrantfile\n</code></pre> <p>In the repository, create two nested folders required for a Docker Actions project:</p> <ul> <li>A <code>.github/workflows</code> folder: containing required necessary <code>yml</code> files that build the containers through GitHub;</li> </ul> <p>In the workflows folder, we're going to add a <code>.yml</code> </p> <p>In the main repository, along with the <code>README</code> and <code>LICENCE</code> file, create another folder called <code>/docker</code></p> <p>In the <code>/docker</code> folder we're going to put the <code>Dockerfile</code> file necessary to build the image.</p>"},{"location":"docker/actions/#link-your-github-and-docker-accounts","title":"Link your GitHub and Docker accounts","text":"<p>Ensure you can access Docker Hub from any workflows you create:</p> <ol> <li>Add your Docker ID as a secret to GitHub. Navigate to your GitHub repository and click Settings &gt; Secrets &gt; New secret.</li> </ol> <p></p> <p>2. Create a new secret with the name <code>DOCKER_HUB_USERNAME</code> and your Docker ID as value. 3. On DockerHub, create a new Personal Access Token (PAT). To create a new token, go to Docker Hub Settings and then click New Access Token. Name it, and copy the newly generated PAT.</p> Tip <p>Name your Docker Hub Access token the same name as your GitHub repository, it will help with keeping track which GitHub repository is related to which Docker image.</p> <p></p> <ol> <li>On GitHub, return to your repository secrets, and add the PAT as a secret with the name <code>DOCKER_HUB_ACCESS_TOKEN</code>.</li> </ol> <p></p>"},{"location":"docker/actions/#setting-up-a-github-action-workflow","title":"Setting up a GitHub Action Workflow","text":"<p>Now that you have connected your GitHub repository with your Docker account, you are ready to add the necessary files to your repo.</p> <p>Note</p> <pre><code>In this example, we will use the existing Docker Image Alpine.\n</code></pre> <p>1. In your GitHub repository, create a file and name in <code>Dockerfile</code>; In the first line of your <code>Dockerfile</code> paste:</p> <pre><code>FROM alpine:3.14\n</code></pre> <p>2. Click the <code>Actions</code> tab and in the search bar, search for <code>docker</code>. Select the <code>docker image workflow</code> (as shown in the image below)</p> <p>Note</p> <pre><code>This will create the `.github/workflows` repository and necessary `yml` file required for the GitHub actions.\n</code></pre> <p></p> <p>3. You will be prompted to the <code>docker-image.yml</code> file; paste the following code, and <code>commit</code> your changes.</p> <p><pre><code>name: Docker Image Small Alpine\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n      -\n        name: Checkout \n        uses: actions/checkout@v2\n      -\n        name: Login to Docker Hub\n        uses: docker/login-action@v1\n        with:\n          username: ${{ secrets.DOCKER_HUB_USERNAME }}\n          password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v2\n        with:\n          context: .\n          file: ./Dockerfile\n          push: true\n          tags: ${{ secrets.DOCKER_HUB_USERNAME }}/simplewhale\n</code></pre> 4. Upon committing and pushing your changes, you can check your Workflows under the Actions tab on GitHub.</p> <p>Note</p> <pre><code>Github will show you when a workflow is building:\n- An orange dot next to your commit count means that the workflow is running;\n- A crossed red circle means that your workflow has failed;\n- A green check means your workflow ran successfully.\n</code></pre> <p></p> <p>5. Navigate to your Docker Hub to see your GitHub Actions generated Docker image.</p> <p></p> <p></p>"},{"location":"docker/actions/#tagging-a-pushing-an-image","title":"Tagging a Pushing an Image","text":"<p>When updating your own Docker Images, you may want to tag the images with a version number.</p> <p>In this example, we update our Alpine Docker Image to a newer version and adding a package.</p> <p>Navigate to your <code>Dockerfile</code> and modify it by adding the following lines:</p> <pre><code>FROM alpine:3.15\n\nLABEL author=\"&lt;name&gt;\" \nLABEL email=\"&lt;email&gt;\"\nLABEL date_created=\"&lt;date&gt;\"\n\nRUN apk update &amp;&amp; apk upgrade --available\nRUN apk add vim fortune\n\nENV PATH=/usr/games:${PATH}\n\nENV LC_ALL=C\n\nENTRYPOINT fortune\n</code></pre> <p>Save, and navigate to the github action (<code>.github/workflows/docker-image.yml</code>). The last line should be <code>tags</code>, edit the line to reflect the version you would like to add.</p> <pre><code>tags: ${{ secrets.DOCKER_HUB_USERNAME }}/simplewhale:v0.2\n</code></pre> <p>Warning</p> <ul> <li>To correctoy add a tag, add a colon followed by a version number such as <code>&lt;name of container&gt;:&lt;version number&gt;</code>.</li> <li>If there is no tag, the version pushed will default to <code>latest</code>.</li> <li>Remember to update/change the tag line in future pushes. </li> </ul> <p>Add, commit, push your changes; You will see your Docker Image being built with the added tagged version.</p> <p></p>"},{"location":"docker/actions/#setting-up-your-own","title":"Setting up your own","text":"<p> GitHub Actions Runner</p> <p> Self-hosted Actions Runner Docs</p> <p> GitLab Runner</p>"},{"location":"docker/advanced/","title":"Advanced Docker Techniques","text":""},{"location":"docker/advanced/#volumes","title":"Volumes","text":"<p>When you run a container, you can bring a directory from the host system into the container, and give it a new name and location using the <code>-v</code> or <code>--volume</code> flag.</p> <pre><code>$ mkdir -p ~/local-data-folder\n$ echo \"some data\" &gt;&gt; ~/local-data-folder/data.txt\n$ docker run -v ${HOME}/local-data-folder:/data $YOUR_DOCKERHUB_USERNAME/mynotebook:latest cat /data/data.txt\n</code></pre> <p>In the example above, you can mount a folder from your localhost, in your home user directory into the container as a new directory named <code>/data</code>.</p>"},{"location":"docker/advanced/#create-and-manage-volumes","title":"Create and manage volumes","text":"<p>Unlike a bind mount, you can create and manage volumes outside the scope of any container.</p> <p>A given volume can be mounted into multiple containers simultaneously. When no running container is using a volume, the volume is still available to Docker and is not removed automatically. You can remove unused volumes using <code>docker volume prune</code> command.</p> <p>When you create a Docker volume, it is stored within a directory on the Docker Linux host (<code>/var/lib/docker/</code>).</p> <p>Note</p> <pre><code>File location on Mac OS X is a bit different:\n&lt;https://timonweb.com/posts/getting-path-and-accessing-persistent-volumes-in-docker-for-mac/&gt;\n</code></pre> <p>Let's create a volume</p> <pre><code>$ docker volume create my-vol\n</code></pre> <p>List volumes:</p> <pre><code>$ docker volume ls\n\nlocal               my-vol\n</code></pre> <p>Inspect a volume by looking at the Mount section in the <code>docker volume inspect</code></p> <pre><code>$ docker volume inspect my-vol\n[\n    {\n        \"Driver\": \"local\",\n        \"Labels\": {},\n        \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\",\n        \"Name\": \"my-vol\",\n        \"Options\": {},\n        \"Scope\": \"local\"\n    }\n]\n</code></pre> <p>Remove a volume</p> <pre><code>$ docker volume rm my-vol\n$ docker volume ls\n</code></pre>"},{"location":"docker/advanced/#populate-a-volume-using-a-container","title":"Populate a volume using a container","text":"<p>This example starts an <code>alpine</code> container and populates the new volume <code>output-vol</code> with the some output created by the container.</p> <pre><code>docker volume create output-vol\ndocker run --name=data-app --mount source=output-vol,target=/data alpine sh -c 'env &gt;&gt; /data/container-env.txt'\n</code></pre> <p>Use <code>docker inspect output-vol</code> to see where the volume data lives on your host, and then use <code>cat</code> to confirm that it contains the output created by the container.</p> <pre><code>docker volume inspect output-vol\nsudo cat /var/lib/docker/volumes/output-vol/_data/container-env.txt\n</code></pre> <p>You should see something like:</p> <pre><code>HOSTNAME=790e13bba28a\nSHLVL=1\nHOME=/root\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPWD=/\n</code></pre> <p>After running either of these examples, run the following commands to clean up the container and volume.</p> <pre><code>docker rm data-app\ndocker volume rm output-vol\n</code></pre>"},{"location":"docker/advanced/#bind-mounts","title":"Bind mounts","text":"<p>Bind mounts: When you use a bind mount, a file or directory on the host machine is mounted into a container.</p> Tip <p>If you are developing new Docker applications, consider using named volumes instead. You can't use Docker CLI commands to directly manage bind mounts.</p> <p></p> <p>Warning</p> <pre><code>One side effect of using bind mounts, for better or for worse, is that\nyou can change the host filesystem via processes running in a container,\nincluding creating, modifying, or deleting important system files or\ndirectories. This is a powerful ability which can have security\nimplications, including impacting non-Docker processes on the host\nsystem.\n\nIf you use `--mount` to bind-mount a file or directory that does not yet\nexist on the Docker host, Docker does not automatically create it for\nyou, but generates an error.\n</code></pre>"},{"location":"docker/advanced/#start-a-container-with-a-bind-mount","title":"Start a container with a bind mount","text":"<p>Create a <code>bind-data</code> directory in your home directory.</p> <pre><code>cd ~\nmkdir -p ~/bind-data\n</code></pre> <p>Run a container, mounting this directory inside the container, and the container should create some data in there.</p> <pre><code>docker run --mount type=bind,source=\"$(pwd)\"/bind-data,target=/data alpine sh -c 'env &gt;&gt; /data/container-env.txt'\n</code></pre> <p>Check that the output looks right.</p> <pre><code>cat ~/bind-data/container-env.txt\n</code></pre>"},{"location":"docker/advanced/#use-a-read-only-bind-mount","title":"Use a read-only bind mount","text":"<p>For some development applications, the container needs to write into the bind mount, so changes are propagated back to the Docker host. At other times, the container only needs read access.</p> <p>This example modifies the one above but mounts the directory as a read-only bind mount, by adding <code>ro</code> to the (empty by default) list of options, after the mount point within the container. Where multiple options are present, separate them by commas.</p> <pre><code>docker run --mount type=bind,source=\"$(pwd)\"/bind-data,target=/data,readonly alpine sh -c 'ls -al /data/ &amp;&amp; env &gt;&gt; /data/container-env.txt'\n</code></pre> <p>You should see an error message about not being able to write to a read-only file system.</p> <pre><code>sh: can't create /data/container-env.txt: Read-only file system\n</code></pre>"},{"location":"docker/build/","title":"Building Docker Images","text":"<p>Now that we are relatively comfortable with running Docker, we can look at some advanced Docker topics, such as:</p> <ul> <li>Building our own Docker images from the <code>Dockerfile</code></li> <li>Modify an existing Dockerfile and create a new image</li> <li>Push an image to a Registry</li> </ul>"},{"location":"docker/build/#requirements","title":"Requirements","text":"<p>Clone our example repository with pre-written Dockerfiles From your CodeSpace, we are going to copy a second GitHub repository onto our VM. If you are working locally, make sure that you change directories away from any other Git repository that you may have been working in.</p> <pre><code>$ cd /workspaces\n\n$ git clone https://github.com/cyverse-education/intro2docker\n\n$ cd intro2docker/\n</code></pre>"},{"location":"docker/build/#writing-a-dockerfile","title":"Writing a Dockerfile","text":"<p>Important</p> <p><code>Dockerfile</code> must be capitalized. It does not have a file extension.</p> <p>Create a file called <code>Dockerfile</code>, and add content to it as described below, e.g.</p> <pre><code>$ touch Dockerfile\n</code></pre> <p>Formatting in the <code>Dockerfile</code></p> <p>We use a code line escape character <code>\\</code> to allow single line scripts to be written on multiple lines in the Dockerfile.</p> <p>We also use the double characters <code>&amp;&amp;</code> which essentially mean \u201cif true, then do this\u201d while executing the code. The <code>&amp;&amp;</code> can come at the beginning of a line or the end when used with <code>\\</code>.</p> <p>The <code>Dockerfile</code> contains Instructions: a series of commands that Docker executes during the creation and execution of a container.</p>"},{"location":"docker/build/#arg","title":"ARG","text":"<p>The only command that can come before a <code>FROM</code> statement is <code>ARG</code></p> <p><code>ARG</code> can be used to set arguments for later in the build, e.g.,</p> <pre><code>ARG VERSION=latest\n\nFROM ubuntu:$VERSION\n</code></pre>"},{"location":"docker/build/#from","title":"FROM","text":"<p>A valid <code>Dockerfile</code> must start with a <code>FROM</code> statement which initializes a new build stage and sets the base image for subsequent layers.</p> <p>We\u2019ll start by specifying our base image, using the FROM statement</p> <pre><code>FROM ubuntu:latest\n</code></pre> <p>If you are building on an <code>arm64</code> or Windows system you can also give the optional <code>--platform</code> flag, e.g.,</p> <pre><code>FROM --platform=linux/amd64 ubuntu:latest\n</code></pre> When to use a multi-stage build pattern? <p>Docker has the ability to build container images from one image, and run that \"builder\" image from a second \"base\" image, in what is called a \"builder pattern\".</p> <p>Build patterns are useful if you're compiling code from (proprietary) source code and only want to feature the binary code as an executed function in the container at run time. </p> <p>Build patterns can greatly reduce the size of your container.</p> <p>You can use multiple <code>FROM</code> commands as build stages. The <code>AS</code> statement follows the <code>image:tag</code> as a psuedo argument. </p> <pre><code># build stage\nFROM golang:latest AS build-env\nWORKDIR /go/src/app\nADD . /go/src/app\nRUN go mod init\nRUN cd /go/src/app &amp;&amp; go build -o hello\n\n# final stage\nFROM alpine:latest\nWORKDIR /app\nCOPY --from=build-env /go/src/app /app/\nENTRYPOINT ./hello\n</code></pre>"},{"location":"docker/build/#label","title":"LABEL","text":"<p>You can create labels which are then tagged as  JSON metadata to the image</p> <pre><code>LABEL author=\"your-name\" \nLABEL email=\"your@email-address\"\nLABEL version=\"v1.0\"\nLABEL description=\"This is your first Dockerfile\"\nLABEL date_created=\"2022-05-13\"\n</code></pre> <p>You can also add labels to a container when it is run:</p> <pre><code>$ docker run --label description=\"this label came later\" ubuntu:latest\n\n$ docker ps -a\n\n$ docker inspect ###\n</code></pre>"},{"location":"docker/build/#run","title":"RUN","text":"<p>Different than the <code>docker run</code> command is the <code>RUN</code> build function. <code>RUN</code> is used to create new layers atop the \"base image\"</p> <p>Here, we are going to install some games and programs into our base image:</p> <pre><code>RUN apt-get update &amp;&amp; apt-get install -y fortune cowsay lolcat\n</code></pre> <p>Here we've installed <code>fortune</code> <code>cowsay</code> and <code>lolcat</code> as new programs into our base image.</p> <p>Best practices for building new layers</p> <pre><code>Ever time you use the `RUN` command it is a good idea to use the `apt-get update` or `apt update` command to make sure your layer is up-to-date. This can become a problem though if you have a very large container with a large number of `RUN` layers.\n</code></pre>"},{"location":"docker/build/#env","title":"ENV","text":"<p>In our new container, we need to change and update some of the environment flags. We can do this using the <code>ENV</code> command</p> <pre><code>ENV PATH=/usr/games:${PATH}\n\nENV LC_ALL=C\n</code></pre> <p>Here we are adding the <code>/usr/games</code> directory to the <code>PATH</code> so that when we run the new container it will find our newly installed game commands</p> <p>We are also updating the \"locales\" to set the language of the container.</p>"},{"location":"docker/build/#copy","title":"COPY","text":"<p>The <code>COPY</code> command will copy files from the directory where <code>Dockerfile</code> is kept into the new image. You must specify where to copy the files or directories</p> <pre><code>COPY . /app\n</code></pre> When to use <code>COPY</code> vs <code>ADD</code> <p><code>COPY</code> is more basic and is good for files</p> <p><code>ADD</code> has some extra features like <code>.tar</code> extraction and URL support</p>"},{"location":"docker/build/#cmd","title":"CMD","text":"<p>The <code>CMD</code> command is used to run software in your image. In general use the [\"command\"] syntax:</p> <pre><code>CMD [\"executable\", \"parameter1\", \"parameter2\"]\n</code></pre>"},{"location":"docker/build/#entrypoint","title":"ENTRYPOINT","text":"<p>ENTRYPOINT works similarly to <code>CMD</code> but is designed to allow you to run your container as an executable.</p> <pre><code>ENTRYPOINT fortune | cowsay | lolcat\n</code></pre> <p>The default <code>ENTRYPOINT</code> of most images is <code>/bin/sh -c</code> which executes a <code>shell</code> command.</p> <p><code>ENTRYPOINT</code> supports both the <code>ENTRYPOINT [\"command\"]</code> syntax and the <code>ENTRYPOINT command</code> syntax</p> What is the difference in the <code>ENTRYPOINT</code> and <code>CMD</code> <p>The CMD instruction is used to define what is execute when the container is run.</p> <p>The ENTRYPOINT instruction cannot be overridden, instead it is appended to when a new command is given to the <code>docker run container:tag new-cmd</code> statement </p> <p>the executable is defined with ENTRYPOINT, while CMD specifies the default parameter</p>"},{"location":"docker/build/#user","title":"USER","text":"<p>Most containers are run as <code>root</code> meaning that they have super-user privileges within themselves</p> <p>Typically, a new user is necessary in a container that is used interactively or may be run on a remote system.</p> <p>During the build of the container, you can create a new user with the <code>adduser</code> command and set up a <code>/home/</code> directory for them. This new user would have something like 1000:1000 <code>uid:gid</code> permissions without <code>sudo</code> privileges.</p> <p>As a last step, the container is run as the new <code>USER</code>, e.g., </p> <pre><code>ARG VERSION=18.04\n\nFROM ubuntu:$VERSION\n\nRUN useradd ubuntu &amp;&amp; \\\n    chown -R ubuntu:ubuntu /home/ubuntu\n\nUSER ubuntu\n</code></pre>"},{"location":"docker/build/#expose","title":"EXPOSE","text":"<p>You can open ports using the <code>EXPOSE</code> command.</p> <pre><code>EXPOSE 8888\n</code></pre> <p>The above command will expose port 8888.</p> <p>Note</p> <pre><code>Running multiple containers using the same port is not trivial and would require the usage of a web server such as [NGINX](https://www.nginx.com/). However, you can have multiple containers interact with each other using [Docker Compose](compose.md).\n</code></pre>"},{"location":"docker/build/#summary-of-instructions","title":"Summary of Instructions","text":"Instruction Command Description <code>ARG</code> Sets environmental variables during image building <code>FROM</code> Instructs to use a specific Docker image <code>LABEL</code> Adds metadata to the image <code>RUN</code> Executes a specific command <code>ENV</code> Sets environmental variables <code>COPY</code> Copies a file from a specified location to the image <code>CMD</code> Sets a command to be executed when running a container <code>ENTRYPOINT</code> Configures and run a container as an executable <code>USER</code> Used to set User specific information <code>EXPOSE</code> exposes a specific port"},{"location":"docker/compose/","title":"Running multi-container Applications with Docker Compose","text":"<p> Docker Compose is an extension of Docker which allows you to run multiple containers synchronously and in communication with one another. </p> <p>Compose allows you to define and run a multi-container service using a <code>Dockerfile</code> and a <code>docker-compose.yml</code>. </p> <p>Note</p> <pre><code>Docker for Mac and Docker Toolbox already include Compose along with\nother Docker apps, so Mac users do not need to install Compose\nseparately. Docker for Windows and Docker Toolbox already include\nCompose along with other Docker apps, so most Windows users do not need\nto install Compose separately.\n\nFor Linux users\n\n``` bash\nsudo curl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n\nsudo chmod +x /usr/local/bin/docker-compose\n```\n</code></pre> <p>Main advantages of Docker compose include:</p> <ul> <li>Your applications can be defined in a YAML file where all the same     options required in <code>docker run</code> are now defined (reproducibility).</li> <li>It allows you to manage your application(s) as a single entity     rather than dealing with starting individual containers     (simplicity).</li> </ul> <p>Note</p> <pre><code>For the sake of this example, either create your own `Dockerfile` or use the same Jupyter SciPy Notebook as in the [Advanced Section](advanced.md)\n</code></pre>"},{"location":"docker/compose/#creating-a-docker-composeyml","title":"Creating a <code>docker-compose.yml</code>","text":"<p>Let's now create a Docker Compose <code>.yml</code> that calls Jupyter Notebook and RStudio</p> <p>1. Create a folder <code>shared_data</code> in your current directory:</p> <pre><code>mkdir shared_data\n</code></pre> <p>2. Create an empty <code>docker-compose.yml</code> file (e.g., touch docker-compose.yml) and paste the following lines</p> <pre><code>version: \"3\"\n\n# All available services\nservices:\n\n  # Computation\n  jupyter:\n    container_name: \"jupyter_notebook\"\n    image: \"jupyter/minimal-notebook\"\n    restart: \"always\"\n    environment:\n      - JUPYTER_TOKEN=mytoken\n    user: root\n    volumes:\n      - ./data:/home/jovyan/work/\n    ports:\n      - 8888:8888\n\n  rstudio:\n    container_name: \"rstudio\"\n    image: \"rocker/rstudio\"\n    restart: \"always\"\n    environment:\n      - DISABLE_AUTH=true\n    volumes:\n      - ./data:/home/rstudio\n    ports:\n      - 8787:8787\n</code></pre> <p>3. Run both Jupyter Lab and RStudio using <code>docker-compose up</code> instead of <code>docker run</code>.</p> <p>Note</p> <pre><code>Handling containers with Docker Compose is fairly simple\n\n``` bash\ndocker-compose up\n```\n\nattaches the volumes, opens ports, and starts the container\n\n``` bash\ndocker-compose down\n```\n\ndestroys the container\n</code></pre> <p>A brief explanation of <code>docker-compose.yml</code> is as below:</p> <ul> <li>The web service builds from the Dockerfile in the current directory.</li> <li>Forwards the container's exposed port to port 8888 on the host.</li> <li>Mounts the project directory on the host to <code>/work</code> or <code>/rstudio</code> inside the     container (allowing you to modify code without having to rebuild the     image).</li> <li><code>restart: always</code> means that it will restart whenever it fails.</li> </ul>"},{"location":"docker/compose/#running-shutting-down-restarting-docker-compose","title":"Running, shutting down, restarting <code>docker-compose</code>","text":"<p>Run the containers with</p> <pre><code>$ docker-compose up -d\n</code></pre> <p>To stop running a running <code>docker-compose</code> session, either press <code>CTRL + C</code> or use the command:</p> <pre><code>docker-compose down\n</code></pre> <p>The above command removes containers, networks, volumes and images created by <code>docker-container up</code>.</p> <p>To restart a container, use the command </p> <pre><code>docker-compose restart\n</code></pre> <p><code>restart</code> will restart the docker-compose service without taking into account changes one may have made to the <code>yml</code> or environment.</p>"},{"location":"docker/compose/#example-using-docker-compose-webodm","title":"Example using Docker-Compose: WebODM","text":"<p>Warning</p> <p>For the purpose of these following examples it is not suggested to use GitHub Codespaces.</p> <p> Web Open Drone Map (WebODM)</p> <p>OpenDroneMap is an open source photogrammetry toolkit to process aerial imagery into maps and 3D models running on command line. WebODM (Web OpenDroneMap) is an extension of ODM running on multiple Docker Containers provinding a user friendly web interface for easy visualization.</p> <p>To use WebODM:</p> <p>Prerequisites</p> <p>WebODM requires <code>docker</code>, <code>docker-compose</code> to function. Additionally, if you are on Windows, users will be required to have the Docker Windows Application installed as well as having the WSL2 (Windows Subsystem for Linux) operational.</p> <ol> <li>Ensure your machine is up to date: <code>sudo apt-get update</code></li> <li>Clone the WebODM repository: <code>git clone https://github.com/OpenDroneMap/WebODM --config core.autocrlf=input --depth 1</code></li> <li>Move into the WebODM folder: <code>cd WebODM</code></li> <li>Run WebODM: <code>sudo ./webodm.sh start</code></li> <li>The necessary docker images will be downloaded (~2 minutes) and WebODM will be accessible through http://localhost:8000/</li> </ol> <p>Note</p> <p>You will be asked to create an account as a formality. Add any username and a password and select Create Account.</p> <p>6. Download example data: <code>clone https://github.com/OpenDroneMap/odm_data_aukerman.git</code>. This git repository contains 77 HD images usable for WebODM. For other examples refer to ODMData.</p> <p>7. In the WebODM portal, click on Select Images and GCP, navigate to <code>odm_data_aukerman/images</code> and select between 20-50 images (16 is the absolute minimum, whilst 32 is the suggested minimum).</p> <p></p> <p>8. WebODM will process the uploaded images (~5-10 minutes); upon completion, click View Map.</p> <p></p> <p>9. A map will open; you can click on 3D (bottom right) to see the 3D rendered model generated.</p> <p></p>"},{"location":"docker/intro/","title":"Introduction to Docker","text":""},{"location":"docker/intro/#prerequisites","title":"Prerequisites","text":"<p>In order to complete these exercises we STRONGLY recommend that you set up a personal  GitHub and  DockerHub account (account creation for both services is free). </p> <p>There are no specific skills needed for this tutorial beyond elementary command line ability and using a text editor. </p> <p>We are going to be using  GitHub CodeSpaces for the hands on portion of the workshop, which features  VS Code as a fully enabled development environment with Docker already installed. </p> <p>CodeSpaces is a featured product from GitHub and requires a paid subscription or Academic account for access. Your account will temporarily be integrated with the course GitHub Organization for the next steps in the workshop.</p> <p>Our instructions on starting a new CodeSpace are here. </p> Installing Docker on your personal computer <p>We are going to be using virtual machines on the cloud for this course, and we will explain why this is a good thing, but there may be a time when you want to run Docker on your own computer.</p> <p>Installing Docker takes a little time but it is reasonably straight forward and it is a one-time setup.</p> <p>Installation instructions from Docker Official Docs for common OS and chip architectures:</p> <ul> <li> Mac OS X</li> <li> Windows</li> <li> Ubuntu Linux</li> </ul> Never used a terminal before? <p>That is    OK! (This person never used a terminal until after their terminal degree, and now they actually PREFER to work in it for writing code) </p> <p>Don't be afraid or ashamed, but be ready to learn some new skills -- we promise it will be worth your while and even FUN! </p> <p>Before venturing much further, you should review the Software Carpentry lessons on \"The Unix Shell\" and \"Version Control with Git\" -- these are great introductory lessons related to the skills we're teaching here.</p> <p>You've given up on ever using a terminal? No problem, Docker can be used from graphic interfaces, like Docker Desktop, or platforms like Portainer. We suggest you read through their documentation on how to use Docker.</p>"},{"location":"docker/intro/#fundamental-docker-commands","title":"Fundamental Docker Commands","text":"<p>Docker commands in the terminal use the prefix <code>docker</code>.</p> <p>Note</p> <pre><code>For every command listed, the correct execution of the commands through the command line is by using `docker` in front of the command: for example `docker help` or `docker search`. Thus, every :material-docker: = `docker`.\n</code></pre>"},{"location":"docker/intro/#help","title":"help","text":"<p>Like many other command line applications the most helpful flag is the <code>help</code> command which can be used with the Management Commands:</p> <pre><code>$ docker \n$ docker --help\n</code></pre>"},{"location":"docker/intro/#search","title":"search","text":"<p>We talk about the concept of Docker Registries in the next section, but you can search the public list of registeries by using the <code>docker search</code> command to find public containers on the Official Docker Hub Registry:</p> <pre><code>$ docker search  \n</code></pre>"},{"location":"docker/intro/#pull","title":"pull","text":"<p>Go to the Docker Hub and type <code>hello-world</code> in the search bar at the top of the page. </p> <p>Click on the 'tag' tab to see all the available 'hello-world' images. </p> <p>Click the 'copy' icon at the right to copy the <code>docker pull</code> command, or type it into your terminal:</p> <pre><code>$ docker pull hello-world\n</code></pre> <p>Note</p> <pre><code>If you leave off the `:` and the tag name, it will by default pull the `latest` image\n</code></pre> <pre><code>$ docker pull hello-world\nUsing default tag: latest\nlatest: Pulling from library/hello-world\n2db29710123e: Pull complete \nDigest: sha256:bfea6278a0a267fad2634554f4f0c6f31981eea41c553fdf5a83e95a41d40c38\nStatus: Downloaded newer image for hello-world:latest\ndocker.io/library/hello-world:latest\n</code></pre> <p>Now try to list the files in your current working directory:</p> <pre><code>$ ls -l\n</code></pre> Where is the image you just pulled? <p>Docker saves container images to the Docker directory (where Docker is installed). </p> <p>You won't ever see them in your working directory.</p> <p>Use 'docker images' to see all the images on your computer:</p> <pre><code>$ docker images\n</code></pre> adding yourself to the Docker group on Linux <p>Depending on how and where you've installed Docker, you may see a <code>permission denied</code> error after running <code>$ docker run helo-world</code> command.</p> <p>If you're on Linux, you may need to prefix your Docker commands with <code>sudo</code>. </p> <p>Alternatively to run docker command without <code>sudo</code>, you need to add your user name (who has root privileges) to the docker \"group\".</p> <p>Create the docker group:</p> <pre><code>$ sudo groupadd docker\n</code></pre> <p>Add your user to the docker group::</p> <pre><code>$ sudo usermod -aG docker $USER\n</code></pre> <p>Log out or close terminal and log back in and your group membership will be initiated</p>"},{"location":"docker/intro/#run","title":"run","text":"<p>The single most common command that you'll use with Docker is <code>docker run</code> (see official help manual for more details).</p> <p><code>docker run</code> starts a container and executes the default \"entrypoint\", or any other \"command\" that follows <code>run</code> and any optional flags.</p> What is an entrypoint? <p>An entrypoint is the initial command(s) executed upon starting the Docker container. It is listed in the <code>Dockerfile</code> as <code>ENTRYPOINT</code> and can take 2 forms: as commands followed by parameters (<code>ENTRYPOINT command param1 param2</code>)  or as an executable (<code>ENTRYPOINT [\u201cexecutable\u201d, \u201cparam1\u201d, \u201cparam2\u201d]</code>)</p> <pre><code>$ docker run hello-world:latest\n</code></pre> <p>In the demo above, you used the <code>docker pull</code> command to download the <code>hello-world:latest</code> image.</p> <p>What about if you run a container that you haven't downloaded?</p> <pre><code>$ docker run alpine:latest ls -l\n</code></pre> <p>When you executed the command <code>docker run alpine:latest</code>, Docker first looked for the cached image locally, but did not find it, it then ran a <code>docker pull</code> behind the scenes to download the <code>alpine:latest</code> image and then execute your command.</p> <p>When you ran <code>docker run alpine:latest</code>, you provided a command <code>ls -l</code>, so Docker started the command specified and you saw the listing of the Alpine file system (not your host system, this was insice the container!).</p>"},{"location":"docker/intro/#images","title":"images","text":"<p>You can now use the <code>docker images</code> command to see a list of all the cached images on your system:</p> <pre><code>$ docker images \nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nalpine                  latest              c51f86c28340        4 weeks ago         1.109 MB\nhello-world             latest              690ed74de00f        5 months ago        960 B\n</code></pre> Inspecting your containers <p>To find out more about a Docker images, run <code>docker inspect hello-world:latest</code></p>"},{"location":"docker/intro/#ps","title":"ps","text":"<p>Now it's time to see the <code>docker ps</code> command which shows you all containers that are currently running on your machine. </p> <pre><code>docker ps\n</code></pre> <p>Since no containers are running, you see a blank line.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n</code></pre> <p>Let's try a more useful variant: <code>docker ps --all</code></p> <pre><code>$ docker ps --all\nCONTAINER ID   IMAGE                                            COMMAND                  CREATED          STATUS                      PORTS     NAMES\na5eab9243a15   hello-world                                      \"/hello\"                 5 seconds ago    Exited (0) 3 seconds ago              loving_mcnulty\n3bb4e26d2e0c   alpine:latest                                    \"/bin/sh\"                17 seconds ago   Exited (0) 16 seconds ago             objective_meninsky\n192ffdf0cbae   opensearchproject/opensearch-dashboards:latest   \"./opensearch-dashbo\u2026\"   3 days ago       Exited (0) 3 days ago                 opensearch-dashboards\na10d47d3b6de   opensearchproject/opensearch:latest              \"./opensearch-docker\u2026\"   3 days ago       Exited (0) 3 days ago                 opensearch-node1\n</code></pre> <p>What you see above is a list of all containers that you have run. </p> <p>Notice that the <code>STATUS</code> column shows the current condition of the container: running, or as shown in the example, when the container was exited.</p>"},{"location":"docker/intro/#stop","title":"stop","text":"<p>The <code>stop</code> command is used for containers that are actively running, either as a foreground process or as a detached background one.</p> <p>You can find a running container using the <code>docker ps</code> command.</p>"},{"location":"docker/intro/#rm","title":"rm","text":"<p>You can remove individual stopped containers by using the <code>rm</code> command. Use the <code>ps</code> command to see all your stopped contiainers:</p> <pre><code>@user \u279c /workspaces $ docker ps -a\nCONTAINER ID   IMAGE                        COMMAND                  CREATED              STATUS                          PORTS     NAMES\n03542eaac9dc   hello-world                  \"/hello\"                 About a minute ago   Exited (0) About a minute ago             unruffled_nobel\n</code></pre> <p>Use the first few unique alphanumerics in the CONTAINER ID to remove the stopped container:</p> <pre><code>@user \u279c /workspaces (mkdocs \u2717) $ docker rm 0354\n0354\n</code></pre> <p>Check to see that the container is gone using <code>ps -a</code> a second time (<code>-a</code> is shorthand for <code>--all</code>; the full command is <code>docker ps -a</code> or <code>docker ps --all</code>).</p>"},{"location":"docker/intro/#rmi","title":"rmi","text":"<p>The <code>rmi</code> command is similar to <code>rm</code> but it will remove the cached images. Used in combination with <code>docker images</code> or <code>docker system df</code> you can clean up a full cache</p> <pre><code>docker rmi\n</code></pre> <pre><code>@user \u279c /workspaces/ (mkdocs \u2717) $ docker images\nREPOSITORY                   TAG       IMAGE ID       CREATED        SIZE\nopendronemap/webodm_webapp   latest    e075d13aaf35   21 hours ago   1.62GB\nredis                        latest    a10f849e1540   5 days ago     117MB\nopendronemap/nodeodm         latest    b4c50165f838   6 days ago     1.77GB\nhello-world                  latest    feb5d9fea6a5   7 months ago   13.3kB\nopendronemap/webodm_db       latest    e40c0f274bba   8 months ago   695MB\n@user \u279c /workspaces (mkdocs \u2717) $ docker rmi hello-world\nUntagged: hello-world:latest\nUntagged: hello-world@sha256:10d7d58d5ebd2a652f4d93fdd86da8f265f5318c6a73cc5b6a9798ff6d2b2e67\nDeleted: sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412\nDeleted: sha256:e07ee1baac5fae6a26f30cabfe54a36d3402f96afda318fe0a96cec4ca393359\n@user \u279c /workspaces (mkdocs \u2717) $ \n</code></pre>"},{"location":"docker/intro/#system","title":"system","text":"<p>The <code>system</code> command can be used to view information about containers on your cache, you can view your total disk usage, view events or info.</p> <p>You can also use it to <code>prune</code> unused data and image layers.</p> <p>To remove all cached layers, images, and data you can use the <code>-af</code> flag for <code>all</code> and <code>force</code></p> <pre><code>docker system prune -af\n</code></pre>"},{"location":"docker/intro/#tag","title":"tag","text":"<p>By default an image will recieve the tag <code>latest</code> when it is not specified during the <code>docker build</code> </p> <p>Image names and tags can be created or changed using the <code>docker tag</code> command. </p> <pre><code>docker tag imagename:oldtag imagename:newtag\n</code></pre> <p>You can also change the registry name used in the tag:</p> <pre><code>docker tag docker.io/username/imagename:oldtag harbor.cyverse.org/project/imagename:newtag\n</code></pre> <p>The cached image laters will not change their <code>sha256</code> and both image tags will still be present after the new tag name is generated. </p>"},{"location":"docker/intro/#push","title":"push","text":"<p>By default <code>docker push</code> will upload your local container image to the Docker Hub</p> <p>We will cover <code>push</code> in more detail at the end of Day 2, but the essential functionality is the same as pull.</p> <p>Also, make sure that your container has the appropriate tag</p> <p>First, make sure to log into the Docker Hub, this will allow you to download private limages, to upload private/public images:</p> <pre><code>docker login\n</code></pre> <p>Alternately, you can link GitHub / GitLab accounts to the Docker Hub.</p> <p>To push the image to the Docker Hub:</p> <pre><code>docker push username/imagename:tag \n</code></pre> <p>or</p> <pre><code>docker push docker.io/username/imagename:tag\n</code></pre> <p>or, to a private registry, here we push to CyVerse private <code>harbor.cyverse.org</code> registry which uses \"project\" sub folders:</p> <pre><code>docker push harbor.cyverse.org/project/imagename:newtag \n</code></pre>"},{"location":"docker/intro/#commands-entrypoints","title":"Commands &amp; Entrypoints","text":"<p>We will cover the differences in <code>CMD</code> and <code>ENTRYPOINT</code> on Day 2 when we build our own images, but it is important to understand that a container can have a command appended to the <code>docker run</code> function.</p> <p>When a image has no commands or entrypoints specified in its Dockerfile, it will default to running a <code>/bin/sh</code> syntax. In those cases, you can add a command when the congtainer is run:</p> <pre><code>$ docker run alpine echo \"Hello world\"\n</code></pre> <p>the Docker client dutifully ran the <code>echo</code> command in our <code>alpine</code> container and then exited. </p> <p>If you've noticed, all of that happened pretty quickly.  Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast!</p>"},{"location":"docker/intro/#interactive-commands-with-containers","title":"Interactive Commands with Containers","text":"<p>Lets try another command, this time to access the container as a shell:</p> <pre><code>$ docker run alpine:latest sh\n</code></pre> <p>Wait, nothing happened, right? </p> <p>Is that a bug? </p> <p>Well, no.</p> <p>The container will exit after running any scripted commands such as <code>sh</code>, unless they are run in an \"interactive\" terminal (TTY) - so for this example to not exit, you need to add the <code>-i</code> for interactive and <code>-t</code> for TTY. </p> <p>You can run them both in a single flag as <code>-it</code>, which is the more common way of adding the flag:</p> <pre><code>$ docker run -it alpine:latest sh\n</code></pre> <p>The prompt should change to something more like <code>/ #</code>.</p> <p>You are now running a shell inside the container! </p> <p>Try out a few commands like <code>ls -l</code>, <code>uname -a</code> and others.</p> <p>Exit out of the container by giving the <code>exit</code> command.</p> <pre><code>/ # exit\n</code></pre> Making sure you've exited the container <p>If you type <code>exit</code> your container will exit and is no longer active. To check that, try the following:</p> <pre><code>$ docker ps --latest\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                          PORTS                    NAMES\nde4bbc3eeaec        alpine                \"/bin/sh\"                3 minutes ago       Exited (0) About a minute ago                            pensive_leavitt\n</code></pre> <p>If you want to keep the container active, then you can use keys <code>ctrl +p</code> <code>ctrl +q</code>. To make sure that it is not exited run the same <code>docker ps --latest</code> command again:</p> <pre><code>$ docker ps --latest\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                         PORTS                    NAMES\n0db38ea51a48        alpine                \"sh\"                     3 minutes ago       Up 3 minutes                                            elastic_lewin\n</code></pre> <p>Now if you want to get back into that container, then you can type <code>docker attach &lt;container id&gt;</code>. This way you can save your container:</p> <pre><code>$ docker attach 0db38ea51a48\n</code></pre>"},{"location":"docker/intro/#house-keeping-and-cleaning-up-exited-containers","title":"House Keeping and  Cleaning Up Exited Containers","text":""},{"location":"docker/intro/#managing-docker-images","title":"Managing Docker Images","text":"<p>In the previous example, you pulled the <code>alpine</code> image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally on your system, run the <code>docker images</code> command.</p> <pre><code>    $ docker images\n    REPOSITORY                 TAG                 IMAGE ID            CREATED             SIZE\n    ubuntu                     bionic              47b19964fb50        4 weeks ago         88.1MB\n    alpine                     latest              caf27325b298        4 weeks ago         5.53MB\n    hello-world                latest              fce289e99eb9        2 months ago        1.84kB\n</code></pre> <p>Above is a list of images that I've pulled from the registry and those I've created myself (we'll shortly see how). You will have a different list of images on your machine. The TAG refers to a particular snapshot of the image and the ID is the corresponding unique identifier for that image.</p> <p>For simplicity, you can think of an image akin to a Git repository - images can be committed with changes and have multiple versions. When you do not provide a specific version number, the client defaults to latest.</p>"},{"location":"docker/intro/#clutter-and-cache","title":"Clutter and Cache","text":"<p>Docker images are cached on your machine in the location where Docker was installed. These image files are not visible in the same directory where you might have used <code>docker pull &lt;imagename&gt;</code>.</p> <p>Some Docker images can be large. Especially data science images with many scientific programming libraries and packages pre-installed.</p> <p>Checking your system cache</p> <pre><code>Pulling many images from the Docker Registries may fill up your hard disk!\n\nTo inspect your system and disk use:\n\n```\n$ docker system info\n$ docker system df\n```\n\nTo find out how many images are on your machine, type:\n\n```\n$ docker images\n```\n\nTo remove images that you no longer need, type:\n\n```\n$ docker system prune\n```\n\nThis is where it becomes important to differentiate between *images*, *containers*, and *volumes* (which we'll get to more in a bit).\n\nYou can take care of all of the dangling images and containers on your system.\n\nNote, that `prune` will not remove your cached *images*\n\n```\n$ docker system prune\n    WARNING! This will remove:\n    - all stopped containers\n    - all networks not used by at least one container\n    - all dangling images\n    - all dangling build cache\n\nAre you sure you want to continue? [y/N]\n```\n\nIf you added the `-af` flag it will remove \"all\" `-a` dangling images, empty containers, AND ALL CACHED IMAGES with \"force\" `-f`.\n</code></pre>"},{"location":"docker/intro/#managing-data-in-docker","title":"Managing Data in Docker","text":"<p>It is possible to store data within the writable layer of a container, but there are some limitations:</p> <ul> <li>The data doesn\u2019t persist when that container is no longer running, and it can be difficult to get the data out of the container if another process needs it.</li> <li>A container\u2019s writable layer is tightly coupled to the host machine where the container is running. You can\u2019t easily move the data somewhere else.</li> <li>Its better to put your data into the container AFTER it is built - this keeps the container size smaller and easier to move across networks.</li> </ul> <p>Docker offers three different ways to mount data into a container from the Docker host:</p> <ul> <li>Volumes</li> <li>tmpfs mounts</li> <li>Bind mounts</li> </ul> <p></p> <p>When in doubt, volumes are almost always the right choice.</p>"},{"location":"docker/intro/#volumes","title":"Volumes","text":"<p>Volumes are often a better choice than persisting data in a container\u2019s writable layer, because using a volume does not increase the size of containers using it, and the volume\u2019s contents exist outside the lifecycle of a given container. While bind mounts (which we will see in the Advanced portion of the Camp) are dependent on the directory structure of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts:</p> <ul> <li>Volumes are easier to back up or migrate than bind mounts.</li> <li>You can manage volumes using Docker CLI commands or the Docker API.</li> <li>Volumes work on both UNIX and Windows containers.</li> <li>Volumes can be more safely shared among multiple containers.</li> <li>A new volume\u2019s contents can be pre-populated by a container.</li> </ul> When Should I Use the Temporary File System mount? <p>If your container generates non-persistent state data, consider using a <code>tmpfs</code> mount to avoid storing the data anywhere permanently, and to increase the container\u2019s performance by avoiding writing into the container\u2019s writable layer. The data is written to the host's memory instead of a volume; When the container stops, the <code>tmpfs</code> mount is removed, and files written there will not be kept.</p> <p>Choose the <code>-v</code> flag for mounting volumes</p> <p><code>-v</code> or <code>--volume</code>: Consists of three fields, separated by colon characters (:). </p> <p>The fields must be in the correct order, and the meaning of each field is not immediately obvious.</p> <ul> <li>The first field is the path on your local machine that where the data are.</li> <li>The second field is the path where the file or directory are mounted in the container.</li> <li>The third field is optional, and is a comma-separated list of options, such as <code>ro</code> (read only).</li> </ul> <pre><code>-v /home/username/your_data_folder:/container_folder\n</code></pre> <pre><code>$ docker run -v /home/$USER/read_cleanup:/work alpine:latest ls -l /work\n</code></pre> <p>So what if we wanted to work interactively inside the container?</p> <pre><code>$ docker run -it -v /home/$USER/read_cleanup:/work alpine:latest sh\n</code></pre> <pre><code>$ ls -l \n$ ls -l work\n</code></pre> <p>Once you're in the container, you will see that the <code>/work</code> directory is mounted in the working directory.</p> <p>Any data that you add to that folder outside the container will appear INSIDE the container. And any work you do inside the container saved in that folder will be saved OUTSIDE the container as well.</p>"},{"location":"docker/intro/#working-with-interactive-containers","title":"Working with Interactive Containers","text":"<p>Let's go ahead and run some Integrated Development Environment images from \"trusted\" organizations on the Docker Hub Registry.</p>"},{"location":"docker/intro/#jupyter-lab-or-rstudio-server-ide","title":"Jupyter Lab  or RStudio-Server  IDE","text":"<p>In this section, let's find a Docker image which can run a Jupyter Notebook</p> <p>Search for official images on Docker Hub which contain the string 'jupyter'</p> <pre><code>$ docker search jupyter\n</code></pre> <p>It should return something like:</p> <pre><code>NAME                                   DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\njupyter/datascience-notebook           Jupyter Notebook Data Science Stack from htt\u2026   912                  \njupyter/all-spark-notebook             Jupyter Notebook Python, Scala, R, Spark, Me\u2026   374                  \njupyter/scipy-notebook                 Jupyter Notebook Scientific Python Stack fro\u2026   337                  \njupyterhub/jupyterhub                  JupyterHub: multi-user Jupyter notebook serv\u2026   307                  [OK]\njupyter/tensorflow-notebook            Jupyter Notebook Scientific Python Stack w/ \u2026   298                  \njupyter/pyspark-notebook               Jupyter Notebook Python, Spark, Mesos Stack \u2026   224                  \njupyter/base-notebook                  Small base image for Jupyter Notebook stacks\u2026   168                  \njupyter/minimal-notebook               Minimal Jupyter Notebook Stack from https://\u2026   150                  \njupyter/r-notebook                     Jupyter Notebook R Stack from https://github\u2026   44                   \njupyterhub/singleuser                  single-user docker images for use with Jupyt\u2026   43                   [OK]\njupyter/nbviewer                       Jupyter Notebook Viewer                         27                   [OK]\n</code></pre> <p>Search for images on Docker Hub which contain the string 'rstudio'</p> <pre><code>$ docker search rstudio\n</code></pre> <pre><code>NAME                                           DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nrocker/rstudio                                 RStudio Server image                            389                  [OK]\nrstudio/r-base                                 Docker Images for R                             24                   \nrocker/rstudio-stable                          Build RStudio based on a debian:stable (debi\u2026   16                   [OK]\nrstudio/rstudio-server-pro                     Deprecated Docker images for RStudio Server \u2026   10                   \nrstudio/r-session-complete                     Images for sessions and jobs in RStudio Serv\u2026   10                   \nrstudio/plumber                                                                                6                    \nrstudio/rstudio-connect                        Default Docker image for RStudio Connect        4                    \nrstudio/r-builder-images-win                                                                   3                    \nrstudio/rstudio-workbench                      Docker Image for RStudio Workbench (formerly\u2026   2                    \nsaagie/rstudio                                 RStudio with sparklyr, Saagie's addin and ab\u2026   2                    [OK]\nibmcom/rstudio-ppc64le                         Integrated development environment (IDE) for\u2026   2                    \nrstudio/checkrs-tew                            Test Environment: Web                           1                    [OK]\nrstudio/rstudio-package-manager                Default Docker image for RStudio Package Man\u2026   1                    \nrstudio/shinyapps-package-dependencies         Docker images used to test the install scrip\u2026   1                    \nrstudio/rstudio-workbench-preview                                                              1                    \n</code></pre> Untrusted community images <p>An important thing to note: None of these Jupyter or RStudio images are 'official' Docker images, meaning they could be trojans for spyware, malware, or other nasty warez.</p>"},{"location":"docker/intro/#understanding-ports","title":"Understanding PORTS","text":"<p>When we want to run a container that runs on the open internet, we need to add a TCP or UDP port number from which we can access the application in a browser using the machine's IP (Internet Protocol) address or DNS (Domain Name Service) location.</p> <p>To do this, we need to access the container over a separate port address on the machine we're working on.</p> <p>Docker uses the flag <code>--port</code> or <code>-p</code> for short followed by two sets of port numbers. </p> Exposing Ports <p>Docker can in fact expose all ports to a container using the capital <code>-P</code> flag</p> <p>For security purposes, it is generally NEVER a good idea to expose all ports.</p> <p>Typically these numbers can be the same, but in some cases your machine may already be running another program (or container) on that open port.</p> <p>The port has two sides <code>left:right</code> separated by a colon. The left side port number is the INTERNAL port that the container software thinks its using. The right side number is the EXTERNAL port that you can access on your computer (or virtual machine).</p> <p>Here are some examples to run basic RStudio and Jupyter Lab:</p> <pre><code>$ docker run --rm -p 8787:8787 -e PASSWORD=cc2022 rocker/rstudio\n</code></pre> <p>note: on CodeSpaces, the reverse proxy for the DNS requires you to turn off authentication</p> <pre><code>$ docker run --rm -p 8787:8787 -e DISABLE_AUTH=true rocker/rstudio\n</code></pre> <pre><code>$ docker run --rm -p 8888:8888 jupyter/base-notebook\n</code></pre> Preempting stale containers from your cache <p>We've added the <code>--rm</code> flag, which means the container will automatically removed from the cache when the container is exited.</p> <p>When you start an IDE in a terminal, the terminal connection must stay active to keep the container alive.</p>"},{"location":"docker/intro/#detaching-your-container-while-it-is-running","title":"Detaching your container while it is running","text":"<p>If we want to keep our window in the foreground  we can use the <code>-d</code> - the detached flag will run the container as a background process, rather than in the foreground. </p> <p>When you run a container with this flag, it will start, run, telling you the container ID:</p> <p><pre><code>$ docker run --rm -d -p 8888:8888 jupyter/base-notebook\n</code></pre> Note, that your terminal is still active and you can use it to launch more containers. </p> <p>To view the running container, use the <code>docker ps</code> command.</p>"},{"location":"docker/intro/#docker-commands","title":"Docker Commands","text":"<p>Here is a compiled list of fundamental Docker Commands:</p> Command Usage Example <code>pull</code> Downloads an image from Docker Hub <code>docker pull hello-world:latest</code> <code>run</code> runs a container with entrypoint <code>docker run -it user/image:tag</code> <code>build</code> Builds a docker image from a Dockerfile in current working directory <code>docker build -t user/image:tag .</code> <code>images</code> List all images on the local machine <code>docker images list</code> <code>tag</code> Adds a different tag name to an image <code>docker tag hello-world:latest hello-world:new-tag-name</code> <code>login</code> Authenticate to the Docker Hub (requires username and password) <code>docker login</code> <code>push</code> Upload your new image to the Docker Hub <code>docker push user/image:tag</code> <code>inspect</code> Provide detailed information on constructs controlled by Docker <code>docker inspect containerID</code> <code>ps</code> List all containers on your system <code>docker ps -a</code> <code>rm</code> Delete a stopped or running container <code>docker rm -f &lt;container ID&gt;</code> <code>rmi</code> Delete an image from your cache <code>docker rmi hello-world:latest</code> <code>stop</code> Stop a running container <code>docker stop alpine:latest</code> <code>system</code> View system details, remove old images and containers with <code>prune</code> <code>docker system prune</code> <code>push</code> Uploads an image to the Docker Hub (or other private registry) <code>docker push username/image:tag</code>"},{"location":"docker/registry/","title":"Finding the right container","text":"<p>There is a high likelihood that the Docker image you need already exists for the application you use in your research. </p> <p>Rather than starting from scratch and creating your own image from a <code>Dockerfile</code>, you should first go searching for one that already exists. This will save you time writing and compiling your own image.</p> <p>First, you will need to know where to look for existing images. Docker images are hosted across the internet on libraries that are called Registries.</p> <p>Container Terminology</p> <ul> <li> Container: when an image is run it becomes the container; A container shares its kernel with other containers using the same image and each runs as an isolated process on the host. </li> <li> Registry: an online library of container images</li> <li> Name: the name of the image</li> <li> Tag: identifies exact version of the image, following a <code>:</code> in the name. If no tag name is given, by default Docker will assign the <code>latest</code> tag name to an image.</li> <li> Layer: an intermediate image, the result of a single set of build commands. An image is built upon layers starting with a base operating system.</li> <li> Dockerfile: a text file that contains a list of commands that the Docker daemon calls while creating the layers of an image. </li> <li> Image: images are compressed files in a cache on a host and can be built locally or downloaded from a registry</li> <li> Base image: has no parent layers, usually base images are a basic Linux operating system like <code>alpine</code>, <code>debian</code>, or <code>ubuntu</code>.</li> <li> Child image: any image built from a base image that has added layers.</li> <li> Official images: verified () images hosted on a public container registry. Safe to use, built by the professionals who know them best.</li> <li> Publisher image: certified images that also include support and guarantee compatibility.</li> <li> User image: images created and shared by users like you. Their contents may be a mystery and therefore are not to be trusted. </li> </ul>"},{"location":"docker/registry/#docker-registries","title":"Docker Registries","text":"<p>Docker uses the concept of Registries, online libraries where container images are cached for public utilization.</p> What EXACTLY is a Container Registry? <p>A Registry is a storage and distribution system for named Docker images</p> <p>Organized by owners into repositories with compiled images that users can download and run </p> <p>Things you can do with registries:</p> <ul> <li>Search for public images;</li> <li>Pull official images;</li> <li>Host and share private images;</li> <li>Push your images.</li> </ul> <p>You must have an account on each registry in order to create repositories and to host your own images.</p> <ul> <li>You can create multiple repositories; </li> <li>You can create multiple tagged images in a repository;</li> <li>You can set up your own private registry using a Docker Trusted Registry.</li> </ul>"},{"location":"docker/registry/#search-image-registries","title":"Search Image Registries","text":"<p>Warning</p> <p>Only use images from trusted sources or images for which you can see the <code>Dockerfile</code>. Any image from an untrusted source could contain something other than what's indicated. If you can see the Dockerfile you can see exactly what is in the image.</p> <p>Docker looks into the Docker Hub public registry by default. </p> <p>Examples of public/private registries to consider for your research needs:</p> Registry Name Container Types  Docker Hub Official Images for Docker  Amazon Elastic Container Registry run containers on AWS  Google Container Registry run containers on Google Cloud  Azure Container Registry run containers on Azure  NVIDIA GPU Cloud containers for GPU computing  GitHub Container Registry managed containers on GitHub  GitLab Container Registry managed containers on GitLab  RedHat Quay.io containers managed by RedHat  BioContainers Registry bioinformatics containers"},{"location":"docker/registry/#dockerhub","title":"DockerHub","text":"<p>Docker Hub is a service provided by Docker for finding and sharing container images with your team. Docker Hub is the most well-known and popular image registry for Docker containers.</p> <p>Info</p> <ul> <li>Registry: a storage and distribution system for named Docker images</li> <li>Repository: collection of \"images\" with individual \"tags\".</li> <li>Teams &amp; Organizations: Manages access to private repositories.</li> <li>Builds: Automatically build container images from GitHub or Bitbucket on the Docker Hub.</li> <li>Webhooks: Trigger actions after a successful push to a repository to integrate Docker Hub with other services.</li> </ul>"},{"location":"docker/registry/#biocontainers","title":"BioContainers","text":"<p>BioContainers is a community-driven project that provides the infrastructure and basic guidelines to create, manage and distribute bioinformatics containers with special focus in proteomics, genomics, transcriptomics and metabolomics. BioContainers is based on the popular frameworks of Docker.</p> <p>Although anyone can create a BioContainer, the majority of BioContainers are created by the Bioconda project. Every Bioconda package has a corresponding BioContainer available at Quay.io.</p>"},{"location":"docker/registry/#redhat-quayio","title":"RedHat Quay.io","text":"<p>Quay is another general image registry. It works the same way as Docker Hub. However, Quay is home to all BioContainers made by the Bioconda project. Now we will find a BioContainer image at Quay, pull that image and run it on cloud virtual machine.</p>"},{"location":"docker/registry/#nvidia-gpu-cloud","title":"NVIDIA GPU Cloud","text":"<p>NVIDIA is one of the leading makers of graphic processing units (GPU). GPU were established as a means of handling graphics processing operations for video cards, but have been greatly expanded for use in generalized computing applications, Machine Learning, image processing, and matrix-based linear algebras.</p> <p>NVIDIA have created their own set of Docker containers and Registries for running on CPU-GPU enabled systems.</p> <ul> <li>NVIDIA-Docker runs atop the NVIDIA graphics drivers on the host system, the NVIDIA drivers are imported to the container at runtime.</li> <li>NVIDIA Docker Hub hosts numerous NVIDIA Docker containers, from which you can build your own images.</li> <li>NVIDIA GPU Cloud hosts numerous containers for HPC and Cloud applications. You must register an account with them (free) to access these. </li> </ul> <p>NVIDIA GPU Cloud hosts three registry spaces</p> <ul> <li><code>nvcr.io/nvidia</code> - catalog of fully integrated and optimized deep learning framework containers.</li> <li><code>nvcr.io/nvidia-hpcvis</code> - catalog of HPC visualization containers (beta).</li> <li><code>nvcr.io/hpc</code> -  popular third-party GPU ready HPC application containers.</li> </ul> <p>NVIDIA Docker can be used as a base-image to create containers running graphical applications remotely. High resolution 3D screens are piped to a remote desktop platform.</p>"},{"location":"getting_started/code_conduct/","title":"Code of Conduct","text":"<p>All attendees, speakers, staff and volunteers at Container Camp are required to follow our code of conduct. </p> <p>CyVerse expects and appreciates cooperation from all participants to help ensure a safe, collaborative environment for everyone. Harrassment by any individual will not be tolerated and may result in the individual being removed from the Camp.</p> <p>Harassment includes: offensive verbal comments related to gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion, technology choices, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention.</p> <p>Participants who are asked to stop any harassing behavior are expected to comply immediately.</p> <p>Workshop staff are also subject to the anti-harassment policy. In particular, staff should not use sexualised images, activities, or other material. </p> <p>If a participant engages in harassing behavior, the workshop organisers may take any action they deem appropriate, including warning the offender or expulsion from the workshop with no refund.</p> <p>If you are being harassed, or notice that someone else is being harassed, or have any other concerns, please contact a member of the workshop staff immediately. Staff can be identified as they'll be wearing badges or nametags.</p> <p>Workshop staff will be happy to help participants contact local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the workshop. We value your attendance.</p> <p>We expect participants to follow these rules at conference and workshop venues and conference-related social events.</p> <p>See http://www.ashedryden.com/blog/codes-of-conduct-101-faq for more information on codes of conduct.</p>"},{"location":"getting_started/logistics/","title":"Logistics","text":""},{"location":"getting_started/logistics/#location","title":"Location","text":"<p>Basics Camp will be held virtually, a Zoom URL will be sent to registered participants.</p> <p>Advanced Camp will be hosted in the Health Sciences Innovation Building (HSIB) on the University of Arizona Campus:</p> <p>Room # 444</p>"},{"location":"getting_started/logistics/#transportation","title":"Transportation","text":"<p> Visitor Parking Interactive Map -- Parking is available around campus in designated lots: https://parking.arizona.edu/parking/ </p> <p> Cat Tran Shuttle service drops off near the HSIB.</p> <p> Sun Link StreetCar service drops off near HSIB and goes through Downtown. </p> <p> Tucson International Airport is located eight miles south of campus and downtown. Local ride-sharing services and public transit are available between campus and local hotels.</p>"},{"location":"getting_started/logistics/#hotels","title":"Hotels","text":"<p>Campus hotels with transportation:</p> <p>Aloft Tucson University</p> <p>The Graduate Tucson</p> <p>Tucson Marriot University Park</p>"},{"location":"getting_started/logistics/#meals","title":"Meals","text":"<p>Tucson is a UNESCO World City of Gastronomy. There is no shortage of amazing local restaurants to explore near the University. </p> <p>Advanced Container Campers will be able to access local food trucks and cafes in the immediate vicinity of the workshop classroom.</p> <p>Coffee, water, and snacks will be provided. </p>"},{"location":"getting_started/logistics/#emergency-medicine","title":"Emergency Medicine","text":"<p>HSIB is adjacent the Banner University Medical Center. In the event of a medical emergency, attendees may be transported to Banner, or to the nearest urgent care facility.</p>"},{"location":"getting_started/schedule/","title":"Schedule","text":""},{"location":"getting_started/schedule/#day-1-container-orchestration","title":"Day 1 - Container Orchestration","text":"<p>Content</p> <ul> <li>Introduction to Kubernetes</li> <li>K8s CLI</li> <li>Deploying K8s clusters</li> </ul> <p>Learning Objectives</p> <ul> <li>Comfort working in the terminal with a remote K8s cluster</li> <li>Understanding of K8s as it is used to manage containers</li> <li>Start your own Zero2JupyterHub</li> </ul> <p>Agenda</p> Time (PDT) Activity Instructor Outcome 09:00 Welcome All 09:05 Overview of Website Tyson Code of Conduct 09:10 What is Kubernetes? Tyson overview of K8s ecosystem 09:30  K8s CLI Tyson Basic command line use of K8s 09:55 Break 10:00  K8s CLI cont. Tyson &amp; Michele Connecting to a K8s cluster 10:55 Break 11:00  Deploying K8s clusters Michele Zero2JupyterHub 11:55 Break 12:00  Managing K8s clusters Michele &amp; Tyson 12:55 Conclude for the day All push changes to GitHub Homework <ul> <li>Consider what type of hardware &amp; container orchestration you're most interested in and come ready with your own ideas for deploying something with Terraform on Day 2.</li> </ul>"},{"location":"getting_started/schedule/#day-2-infrastructure-as-code","title":"Day 2 - Infrastructure as Code","text":"<p>Content</p> <ul> <li>Use Terraform to provision hardware and deploy applications &amp; containers</li> <li>Build Terraform templates</li> </ul> <p>Learning Objectives</p> <ul> <li>Understanding of what IaC is and how it fits along with the orchestration of containers and cloud</li> <li>Ability to launch your own cloud instances on at least one cloud platform (OpenStack)</li> <li>Ability to provision your instances with Terraform and access them</li> </ul> <p>Agenda</p> Time (PDT) Activity Instructor Outcome 09:00 Welcome back All 09:05 Discuss previous day, answer questions 09:20  What is Terraform Edwin &amp; Tyson 09:55 Break 10:00  Terraform CLI Tyson 10:55 Break 11:00  Using a Terraform to simply manage Docker Edwin &amp; Michele 11:55 Break 12:00  Terraform management continued Edwin &amp; Tyson 12:55 Conclude"},{"location":"getting_started/schedule/#day-3-cacao","title":"Day 3 - CACAO","text":"<p>Content</p> <pre><code>- Introduction to CyVerse CACAO\n- Writing advanced Terraform templates for CACAO\n</code></pre> <p>Learning Objectives</p> <pre><code>- Being able to execute containers on the HPC\n- Create a small Kubernetes cluster\n</code></pre> <p>Agenda</p> Time (PDT) Activity Instructor Outcome 09:00 Welcome back All 09:05 Discuss previous day, answer questions 09:20 Introduction to CACAO Edwin 09:30 Using the CACAO UI Edwin 09:55 Break 10:00 Overview of CACAO templates Edwin 10:55 Break 11:00 Creating a CACAO template Edwin 11:55 Break 12:00 Show and Tell - present your use cases everyone 12:55 Conclude"},{"location":"getting_started/schedule_basics/","title":"Schedule basics","text":""},{"location":"getting_started/schedule_basics/#day-1-introduction-to-docker","title":"Day 1 - Introduction to Docker","text":"Content <pre><code>- Introduction to Docker and its uses in reproducible science \n- Launching development environments on CodeSpaces for container testing\n- Using Docker on the commandline.\n</code></pre> Goals <pre><code>- Introduction to containers &amp; [where to find them](../docker/registry.md)\n- Command line containers with CodeSpaces (optional: run locally)\n- Find and use official Docker images\n</code></pre> Activities Time (MST/AZ) Activity Instructor Outcome 09:00 Welcome All 09:05 Overview of Website Michele Code of Conduct 09:10 Why use Containers? Carlos What containers are used for in science 09:25 Start CodeSpace  Tyson Using Dev Environments to create containers 09:30  Docker Commands and Execution Carlos Basic command line use of Docker 09:55 Break 10:00  Docker Commands and Execution Carlos &amp; Tyson Basic command line use of Docker 10:55 Break 11:00  Managing Docker and Data Michele Volumes and Interactive Use inside containers 11:55 Break 12:00  Managing Docker and Data Michele &amp; Tyson 12:55 Conclude for the day All push changes to your GitHub Optional Homework <pre><code>- Test other Docker container images on CodeSpaces or locally\n</code></pre>"},{"location":"getting_started/schedule_basics/#day-2-building-docker-containers","title":"Day 2 - Building Docker Containers","text":"Content <pre><code>- Use GitHub to browse for public Dockerfiles\n- Build Dockerfiles and push them to public registry\n- Use Version Control to set up automated container builds with GitHub Actions\n</code></pre> Goals <pre><code>- Introduction to what Dockerfiles are and what you use them for\n- Start thinking about how to modify them for your own applications\n</code></pre> Activities Time (MST/AZ) Activity Instructor Notes 09:00 Welcome back All 09:05 Discuss previous day, answer questions 09:15 (re)start Dev Environment  Tyson 09:20 Finding the right container  Tyson 09:55 Break 10:00 Building Docker Images  Carlos 10:55 Break 11:00 Using Docker Compose  Michele 11:55 Break 12:00 Integrating your Containers into CyVerse Tyson 12:55 Conclude"},{"location":"getting_started/schedule_basics/#day-3-singularity-orchestration-and-containers-on-the-hpc","title":"Day 3 - Singularity, Orchestration, and Containers on the HPC","text":"Content <pre><code>- Introduction to Singularity\n- Introduction to Kubernetes\n</code></pre> Goals <pre><code>- Being able to execute containers on the HPC\n- Create a small Kubernetes cluster\n</code></pre> Activities Time (MST/AZ) Activity Instructor Notes 09:00 Welcome back All 09:05 Discuss previous day, answer questions 09:15 (re(re))start Dev Environment  09:20 Introduction to Singularity 09:30 Obtaining Singularity and the Singularity CLI 09:55 Break 10:00 Singularity Commands &amp; Interacting with Singularity Images 10:55 Break 11:00 Singularity Images Continued* 11:55 Break 12:00 Integrating your Containers into CyVerse, questions, inquiries 12:55 Conclude** <ul> <li>If we still have time, we will also discuss using Singularity on the HPC. ** For additional questions and inquiries, we will make time for one-on-ones on either Thursday or Friday (flexible schedule).</li> </ul>"},{"location":"getting_started/setup/","title":"Pre-Camp Setup","text":""},{"location":"getting_started/setup/#prerequisites","title":"Prerequisites","text":"<p>Before attending Container Camp (Basics) please do the following:</p> <p>Create a GitHub Account</p> <p>Create a Docker Hub Account</p> <p>Optional</p> <p>Create a CyVerse Account</p> <p>Before attending Cloud Native Camp please do the following:</p> <p> (all of the above)</p> <p>Create an ACCESS-CI profile</p>"},{"location":"orchestration/advk8s/","title":"Advanced use of Kubernetes","text":""},{"location":"orchestration/advk8s/#deploying-your-own-kubernetes-clusters","title":"Deploying your own Kubernetes clusters","text":"<p>We are going to be using Jetstream-2 for this section and will rely on their documentation for Kubernetes deployment</p>"},{"location":"orchestration/advk8s/#helm","title":"Helm","text":"<p>Helm is the package manager that allows users to define, install, and manage K8s applications using Helm Charts, which are packages of pre-configured Kubernetes resources.</p> <p>Similarly to how Docker users can create their own containers and share them collaborators through DockerHub, Helm Charts are built by users and shared through repositories such as artifacthub. This allows for streamlining deployment and management for complex K8s orchestrations.</p>"},{"location":"orchestration/advk8s/#helm-example-zero2jupyterhub","title":"Helm example: Zero2JupyterHub","text":"<ol> <li> <p>Install Helm</p> <ul> <li>Users can install Helm on their own machines by following the official documentation. For this example, we are going to assume that students are using a Unix OS. In that case, the following commands should work: <pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></li> </ul> <p>Info</p> <p>If you are using the Virtual Machines provided by the CyVerse team, Helm is already installed.</p> </li> <li> <p>Add the JupyterHub Helm repository, which contains the charts needed to deploy JupyterLab.     <pre><code>helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/\nhelm repo update\n</code></pre>     You can check if the help repository list by doing <code>help repo list</code>. You should see something like      <pre><code>NAME            URL\njupyterhub      https://jupyterhub.github.io/helm-chart/\n</code></pre></p> </li> <li>Obtain the <code>config.yaml</code><ul> <li>One can write their own config file, but these take extensive work and deep understanding of charts. What we can do instead, is to pull the values that we need from Jupyterhub and save its output to a file. We can do that with the following command: <pre><code>helm show values jupyterhub/jupyterhub &gt; jupyter.yaml\n</code></pre></li> <li>If you were to look inside this yaml file, you will see a long list of configurable lines. These values need not to be changed to execute the jupyterhub, apart from <code>secretToken</code>.</li> <li>Generate the secret token with <code>openssl rand -hex 32</code>, copy the hex string and replace the <code>secretToken</code> line from the <code>jupyter.yml</code> file using a text editor (e.g., nano or vim). </li> </ul> </li> <li>From within the folder, deploy the jupyterhub using the following command (installation should only take a minute):     <pre><code>helm install jupyterhub jupyterhub/jupyterhub --values jupyter.yaml\n</code></pre></li> <li>Access the JupyterHub through your VM IP address!</li> </ol> Uninstalling the chart <p>To uninstall the chart simply do <pre><code>helm uninstall jupyterhub\n</code></pre> If you also want to remove the repository do <pre><code>helm repo remove jupyterhub\n</code></pre> The names can get pretty confusing, please understand that the first is uninstalling the chart named <code>jupyterhub</code> and the second the repository named <code>jupyterhub</code> (and this is why you want to keep names unique :) ).</p>"},{"location":"orchestration/advk8s/#miniaturized-versions-of-kubernetes","title":"Miniaturized versions of Kubernetes","text":"Why use mini versions of Kubernetes? <p>There are multiple projects developing \"light-weight\" Kubernetes.</p> <p>The justification for these projects being that full Kubernetes deployments with all of its functionality increases the cognitive load and the number of configurations and parameters that you must work with when running containers and clusters, particularly at the edge.</p> <p>Projects that are working on miniaturized versions of K8s:</p> name functionality use cases minikube microK8s runs fast, self-healing, and highly available Kubernetes clusters K3s runs production-level Kubernetes workloads on low resourced and remotely located IoT and Edge devices K3d lightweight wrapper that runs K3s in a docker container"},{"location":"orchestration/advk8s/#install-k3s","title":"Install <code>K3s</code>","text":"<p>K3s</p>"},{"location":"orchestration/advk8s/#install-minikube","title":"Install <code>minikube</code>","text":"<p>Minikube is useful for running K8s on a single node or locally -- its primary use is to teach you how to use K8s.</p> <pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\nsudo minikube config set vm-driver none\n</code></pre>"},{"location":"orchestration/advk8s/#installing-microk8s","title":"Installing <code>microK8s</code>","text":""},{"location":"orchestration/advterra/","title":"Creating Terraform Templates on OpenStack","text":""},{"location":"orchestration/advterra/#overview","title":"Overview","text":"<p>This advanced tutorial will guide you through setting up a Terraform project with Docker.</p> <p>Goals</p> <p>material-play: Understand how to use registry.terraform.io</p> <p> Understand practical terraform advanced language concepts by orchestrating Docker containers</p> <p> Understand how to install software and provision multiple VMs</p> <p> Understand how Terraform is used to orchestrate VMs together</p> <p> Undertand how Terraform is used to create a multi-node JupyterHub</p> Things we won't cover <p> Basic management of Kubernetes clusters (that is a different lesson)</p> <p> All of Terraform's features</p>"},{"location":"orchestration/advterra/#using-registryterraformio","title":"Using registry.terraform.io","text":"<p>Terraform maintains an active registry site located at https://registry.terraform.io.</p> <p>This is the Terraform's defacto catalog for - providers - modules - policies (enforcement rules via Terraform Cloud) - runtasks (integrations with other services via Terraform Cloud)</p> <p>Examples of providers: - https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest - https://registry.terraform.io/providers/kreuzwerker/docker/latest - https://registry.terraform.io/providers/hashicorp/kubernetes/latest</p>"},{"location":"orchestration/advterra/#prerequisites-in-doing-terraform-exercises","title":"Prerequisites in doing Terraform exercises","text":"<ul> <li> <p>Basic understanding of  OpenStack and  Terraform as was covered in the prior lesson</p> </li> <li> <p>Access to an OpenStack cloud (we will use Jetstream2)</p> </li> <li> <p> Terraform installed on your workstation or local machine</p> </li> </ul> <p>Outcomes</p> <p>By the end of this tutorial, you will </p> <p> have used Terraform to manage Docker containers while learning advanced Terraform concepts</p>"},{"location":"orchestration/advterra/#using-a-terraform-to-simply-manage-docker","title":"Using a Terraform to simply manage Docker","text":"<p>We will use VMs with Docker installed and learn how to launch containers in a declarative way, rather than using the <code>docker</code> command. Once you gain access to your VM, perform the following steps</p> <ol> <li>ssh into your VM (alternatively, you can declare use the Docker Terraform provider using ssh access)</li> <li><code>git clone https://gitlab.com/stack0/terraform-docker-helloworld.git</code></li> <li><code>cd terraform-docker-hellow-world</code></li> <li>Review <code>input.tf</code>, <code>main.tf</code>, and <code>terraform.tfvars.example</code> to get a sense of how to manage a Docker container using Terraform<ol> <li>Review the concept of a Terraform <code>resource</code></li> <li>How many resources are created?</li> <li>What is the relationship between the resources?</li> <li>Is there any question that comes to mind about the port property of <code>docker_container</code> resource</li> </ol> </li> <li><code>cp terraform.tfvars.example terraform.tfvars</code></li> <li>edit <code>terraform.tfvars</code><ol> <li>feel free to edit the <code>image</code> or <code>container_name</code> with a container you prefer, but if you do, please select a container with a port that you can access</li> <li>if you are using local docker access, use the default value; if you are accessing the docker host remotely, use the string \"ssh://myuser@1.2.3.4\", where <code>myuser</code> is replaced with your vm username and <code>1.2.3.4</code> is replaced with your vm's ip address</li> </ol> </li> <li><code>terraform apply -auto-approve</code></li> </ol> Expected Response <pre><code>Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following\nsymbols:\n+ create\n\nTerraform will perform the following actions:\n\n# docker_container.mycontainer will be created\n+ resource \"docker_container\" \"mycontainer\" {\n    + attach                                      = false\n    + bridge                                      = (known after apply)\n    + command                                     = (known after apply)\n    + container_logs                              = (known after apply)\n    + container_read_refresh_timeout_milliseconds = 15000\n    + entrypoint                                  = (known after apply)\n    + env                                         = (known after apply)\n    + exit_code                                   = (known after apply)\n    + hostname                                    = (known after apply)\n    + id                                          = (known after apply)\n    + image                                       = (known after apply)\n    + init                                        = (known after apply)\n    + ipc_mode                                    = (known after apply)\n    + log_driver                                  = (known after apply)\n    + logs                                        = false\n    + must_run                                    = true\n    + name                                        = \"edwins-container\"\n    + network_data                                = (known after apply)\n    + read_only                                   = false\n    + remove_volumes                              = true\n    + restart                                     = \"no\"\n    + rm                                          = false\n    + runtime                                     = (known after apply)\n    + security_opts                               = (known after apply)\n    + shm_size                                    = (known after apply)\n    + start                                       = true\n    + stdin_open                                  = false\n    + stop_signal                                 = (known after apply)\n    + stop_timeout                                = (known after apply)\n    + tty                                         = false\n    + wait                                        = false\n    + wait_timeout                                = 60\n    }\n\n# docker_image.mydocker will be created\n+ resource \"docker_image\" \"mydocker\" {\n    + id          = (known after apply)\n    + image_id    = (known after apply)\n    + name        = \"nginx:latest\"\n    + repo_digest = (known after apply)\n    }\n\nPlan: 2 to add, 0 to change, 0 to destroy.\ndocker_image.mydocker: Creating...\ndocker_image.mydocker: Creation complete after 3s [id=sha256:89da1fb6dcb964dd35c3f41b7b93ffc35eaf20bc61f2e1335fea710a18424287nginx:latest]\ndocker_container.mycontainer: Creating...\ndocker_container.mycontainer: Creation complete after 1s [id=54f9987cde7fe5b474348ca3e89955c24eb076d6d4ea49aee7e44928f3f8e711]\n\nApply complete! Resources: 2 added, 0 changed, 0 destroyed.\n</code></pre> <ol> <li><code>terraform destroy -auto-approve</code></li> </ol>"},{"location":"orchestration/advterra/#increasing-the-number-of-containers-using-count","title":"Increasing the number of containers using count","text":"<p>We will next update Terraform to create multiple containers of the same image.</p> <ol> <li>Modify the <code>input.tf</code> with a new variable     <pre><code>variable \"num_containers\" {\n    type = number\n    description = \"number, number of containers to launch\"\n    default = 1\n}\n</code></pre></li> <li>Modify the <code>main.tf</code> with the following changes in red:     <pre><code>\n    resource \"docker_container\" \"mycontainer\" {\n        count = var.num_containers\n        image = docker_image.mydocker.image_id\n        name = \"${format(\"%s%02d\", var.container_name, count.index)}\"\n        ports {\n            internal = 80\n            external = \"${8080 + count.index}\" # illustrates using count.index to indicate which port to use\n        }\n    }\n    output \"docker_containers\" {\n        value = keys({\n            for index, d in docker_container.mycontainer.* : \"${format(\"%s,%s,%s\", index, d.name,d.id)}\" =&gt; d\n        })\n    }\n    </code></pre></li> <li>Edit your <code>terraform.tfvars</code> to include a new <code>num_containers</code> input variable with a value <code>5</code></li> <li><code>terraform apply -auto-approve</code></li> </ol> Expected Response <pre><code>$ terraform apply -auto-approve\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n+ create\n\nTerraform will perform the following actions:\n\n# docker_container.mycontainer[0] will be created\n+ resource \"docker_container\" \"mycontainer\" {\n    + attach                                      = false\n    + bridge                                      = (known after apply)\n    + command                                     = (known after apply)\n    + container_logs                              = (known after apply)\n    + container_read_refresh_timeout_milliseconds = 15000\n    + entrypoint                                  = (known after apply)\n    + env                                         = (known after apply)\n    + exit_code                                   = (known after apply)\n    + hostname                                    = (known after apply)\n    + id                                          = (known after apply)\n    + image                                       = (known after apply)\n    + init                                        = (known after apply)\n    + ipc_mode                                    = (known after apply)\n    + log_driver                                  = (known after apply)\n    + logs                                        = false\n    + must_run                                    = true\n    + name                                        = \"mycontainer00\"\n    + network_data                                = (known after apply)\n    + read_only                                   = false\n    + remove_volumes                              = true\n    + restart                                     = \"no\"\n    + rm                                          = false\n    + runtime                                     = (known after apply)\n    + security_opts                               = (known after apply)\n    + shm_size                                    = (known after apply)\n    + start                                       = true\n    + stdin_open                                  = false\n    + stop_signal                                 = (known after apply)\n    + stop_timeout                                = (known after apply)\n    + tty                                         = false\n    + wait                                        = false\n    + wait_timeout                                = 60\n\n    + ports {\n        + external = 8080\n        + internal = 80\n        + ip       = \"0.0.0.0\"\n        + protocol = \"tcp\"\n        }\n    }\n\n# docker_container.mycontainer[1] will be created\n+ resource \"docker_container\" \"mycontainer\" {\n    + attach                                      = false\n    + bridge                                      = (known after apply)\n    + command                                     = (known after apply)\n    + container_logs                              = (known after apply)\n    + container_read_refresh_timeout_milliseconds = 15000\n    + entrypoint                                  = (known after apply)\n    + env                                         = (known after apply)\n    + exit_code                                   = (known after apply)\n    + hostname                                    = (known after apply)\n    + id                                          = (known after apply)\n    + image                                       = (known after apply)\n    + init                                        = (known after apply)\n    + ipc_mode                                    = (known after apply)\n    + log_driver                                  = (known after apply)\n    + logs                                        = false\n    + must_run                                    = true\n    + name                                        = \"mycontainer01\"\n    + network_data                                = (known after apply)\n    + read_only                                   = false\n    + remove_volumes                              = true\n    + restart                                     = \"no\"\n    + rm                                          = false\n    + runtime                                     = (known after apply)\n    + security_opts                               = (known after apply)\n    + shm_size                                    = (known after apply)\n    + start                                       = true\n    + stdin_open                                  = false\n    + stop_signal                                 = (known after apply)\n    + stop_timeout                                = (known after apply)\n    + tty                                         = false\n    + wait                                        = false\n    + wait_timeout                                = 60\n\n    + ports {\n        + external = 8081\n        + internal = 80\n        + ip       = \"0.0.0.0\"\n        + protocol = \"tcp\"\n        }\n    }\n\n# docker_container.mycontainer[2] will be created\n+ resource \"docker_container\" \"mycontainer\" {\n    + attach                                      = false\n    + bridge                                      = (known after apply)\n    + command                                     = (known after apply)\n    + container_logs                              = (known after apply)\n    + container_read_refresh_timeout_milliseconds = 15000\n    + entrypoint                                  = (known after apply)\n    + env                                         = (known after apply)\n    + exit_code                                   = (known after apply)\n    + hostname                                    = (known after apply)\n    + id                                          = (known after apply)\n    + image                                       = (known after apply)\n    + init                                        = (known after apply)\n    + ipc_mode                                    = (known after apply)\n    + log_driver                                  = (known after apply)\n    + logs                                        = false\n    + must_run                                    = true\n    + name                                        = \"mycontainer02\"\n    + network_data                                = (known after apply)\n    + read_only                                   = false\n    + remove_volumes                              = true\n    + restart                                     = \"no\"\n    + rm                                          = false\n    + runtime                                     = (known after apply)\n    + security_opts                               = (known after apply)\n    + shm_size                                    = (known after apply)\n    + start                                       = true\n    + stdin_open                                  = false\n    + stop_signal                                 = (known after apply)\n    + stop_timeout                                = (known after apply)\n    + tty                                         = false\n    + wait                                        = false\n    + wait_timeout                                = 60\n\n    + ports {\n        + external = 8082\n        + internal = 80\n        + ip       = \"0.0.0.0\"\n        + protocol = \"tcp\"\n        }\n    }\n\n# docker_container.mycontainer[3] will be created\n+ resource \"docker_container\" \"mycontainer\" {\n    + attach                                      = false\n    + bridge                                      = (known after apply)\n    + command                                     = (known after apply)\n    + container_logs                              = (known after apply)\n    + container_read_refresh_timeout_milliseconds = 15000\n    + entrypoint                                  = (known after apply)\n    + env                                         = (known after apply)\n    + exit_code                                   = (known after apply)\n    + hostname                                    = (known after apply)\n    + id                                          = (known after apply)\n    + image                                       = (known after apply)\n    + init                                        = (known after apply)\n    + ipc_mode                                    = (known after apply)\n    + log_driver                                  = (known after apply)\n    + logs                                        = false\n    + must_run                                    = true\n    + name                                        = \"mycontainer03\"\n    + network_data                                = (known after apply)\n    + read_only                                   = false\n    + remove_volumes                              = true\n    + restart                                     = \"no\"\n    + rm                                          = false\n    + runtime                                     = (known after apply)\n    + security_opts                               = (known after apply)\n    + shm_size                                    = (known after apply)\n    + start                                       = true\n    + stdin_open                                  = false\n    + stop_signal                                 = (known after apply)\n    + stop_timeout                                = (known after apply)\n    + tty                                         = false\n    + wait                                        = false\n    + wait_timeout                                = 60\n\n    + ports {\n        + external = 8083\n        + internal = 80\n        + ip       = \"0.0.0.0\"\n        + protocol = \"tcp\"\n        }\n    }\n\n# docker_container.mycontainer[4] will be created\n+ resource \"docker_container\" \"mycontainer\" {\n    + attach                                      = false\n    + bridge                                      = (known after apply)\n    + command                                     = (known after apply)\n    + container_logs                              = (known after apply)\n    + container_read_refresh_timeout_milliseconds = 15000\n    + entrypoint                                  = (known after apply)\n    + env                                         = (known after apply)\n    + exit_code                                   = (known after apply)\n    + hostname                                    = (known after apply)\n    + id                                          = (known after apply)\n    + image                                       = (known after apply)\n    + init                                        = (known after apply)\n    + ipc_mode                                    = (known after apply)\n    + log_driver                                  = (known after apply)\n    + logs                                        = false\n    + must_run                                    = true\n    + name                                        = \"mycontainer04\"\n    + network_data                                = (known after apply)\n    + read_only                                   = false\n    + remove_volumes                              = true\n    + restart                                     = \"no\"\n    + rm                                          = false\n    + runtime                                     = (known after apply)\n    + security_opts                               = (known after apply)\n    + shm_size                                    = (known after apply)\n    + start                                       = true\n    + stdin_open                                  = false\n    + stop_signal                                 = (known after apply)\n    + stop_timeout                                = (known after apply)\n    + tty                                         = false\n    + wait                                        = false\n    + wait_timeout                                = 60\n\n    + ports {\n        + external = 8084\n        + internal = 80\n        + ip       = \"0.0.0.0\"\n        + protocol = \"tcp\"\n        }\n    }\n\n# docker_image.mydocker will be created\n+ resource \"docker_image\" \"mydocker\" {\n    + id          = (known after apply)\n    + image_id    = (known after apply)\n    + name        = \"nginx:latest\"\n    + repo_digest = (known after apply)\n    }\n\nPlan: 6 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n+ docker_containers = (known after apply)\ndocker_image.mydocker: Creating...\ndocker_image.mydocker: Creation complete after 3s [id=sha256:89da1fb6dcb964dd35c3f41b7b93ffc35eaf20bc61f2e1335fea710a18424287nginx:latest]\ndocker_container.mycontainer[1]: Creating...\ndocker_container.mycontainer[4]: Creating...\ndocker_container.mycontainer[2]: Creating...\ndocker_container.mycontainer[0]: Creating...\ndocker_container.mycontainer[3]: Creating...\ndocker_container.mycontainer[1]: Creation complete after 1s [id=8c80b558394f62fe004ff156fa1fee06ce723f2b0721cfdfd0f4e64fb6fb28f8]\ndocker_container.mycontainer[3]: Creation complete after 1s [id=c0bc456f847a4e403ee4bcd133f79a21f22b4ec8f94db8793160b28807789e06]\ndocker_container.mycontainer[0]: Creation complete after 1s [id=4ce1d4e0617994ba225701e5e0ef243daa8762968c8d7202be2612ecaeaca064]\ndocker_container.mycontainer[2]: Creation complete after 1s [id=36cf1496a418395408a9d5630c8e1cf8b12fa948ab73fe95a3b1a3d8ce767e99]\ndocker_container.mycontainer[4]: Creation complete after 1s [id=46ab03a8cd2616f20d34058d20b298df8ecc84bf5e8cfaf8013086f21ba1f501]\n\nApply complete! Resources: 6 added, 0 changed, 0 destroyed.\n\nOutputs:\n\ndocker_containers = [\n\"0,mycontainer00,4ce1d4e0617994ba225701e5e0ef243daa8762968c8d7202be2612ecaeaca064\",\n\"1,mycontainer01,8c80b558394f62fe004ff156fa1fee06ce723f2b0721cfdfd0f4e64fb6fb28f8\",\n\"2,mycontainer02,36cf1496a418395408a9d5630c8e1cf8b12fa948ab73fe95a3b1a3d8ce767e99\",\n\"3,mycontainer03,c0bc456f847a4e403ee4bcd133f79a21f22b4ec8f94db8793160b28807789e06\",\n\"4,mycontainer04,46ab03a8cd2616f20d34058d20b298df8ecc84bf5e8cfaf8013086f21ba1f501\",\n]\n</code></pre> <ol> <li>Notice some pieces of the codes that were introduced<ol> <li>What is the <code>count.index</code>?</li> <li>What is the format function and why use it?</li> <li>What will index start at?</li> <li>Terraform supports math operations</li> <li>An example of using the keys() method and ad hoc dictionary construction</li> </ol> </li> <li><code>terraform destroy -auto-approve</code></li> </ol>"},{"location":"orchestration/advterra/#when-resources-change-outside-of-terraform","title":"When resources change outside of Terraform","text":"<p>In this exercise we'll discover how to use Terraform to handle change.</p> <ol> <li>Use <code>docker stop</code> and <code>docker rm</code> to stop and delete docker containers 2 and 3.</li> </ol> Expected Response <pre><code>$ docker stop 36cf1496a418395408a9d5630c8e1cf8b12fa948ab73fe95a3b1a3d8ce767e99 c0bc456f847a4e403ee4bcd133f79a21f22b4ec8f94db8793160b28807789e06\n36cf1496a418395408a9d5630c8e1cf8b12fa948ab73fe95a3b1a3d8ce767e99\nc0bc456f847a4e403ee4bcd133f79a21f22b4ec8f94db8793160b28807789e06\n$ docker rm 36cf1496a418395408a9d5630c8e1cf8b12fa948ab73fe95a3b1a3d8ce767e99 c0bc456f847a4e403ee4bcd133f79a21f22b4ec8f94db8793160b28807789e06\n36cf1496a418395408a9d5630c8e1cf8b12fa948ab73fe95a3b1a3d8ce767e99\nc0bc456f847a4e403ee4bcd133f79a21f22b4ec8f94db8793160b28807789e06\n</code></pre> <ol> <li>Execute a <code>terraform show</code> and count the number of instances in the state</li> <li>Execute a <code>terraform refresh</code> </li> </ol> Expected Response <pre><code>$ terraform refresh\ndocker_image.mydocker: Refreshing state... [id=sha256:89da1fb6dcb964dd35c3f41b7b93ffc35eaf20bc61f2e1335fea710a18424287nginx:latest]\ndocker_container.mycontainer[3]: Refreshing state... [id=c0bc456f847a4e403ee4bcd133f79a21f22b4ec8f94db8793160b28807789e06]\ndocker_container.mycontainer[2]: Refreshing state... [id=36cf1496a418395408a9d5630c8e1cf8b12fa948ab73fe95a3b1a3d8ce767e99]\ndocker_container.mycontainer[1]: Refreshing state... [id=8c80b558394f62fe004ff156fa1fee06ce723f2b0721cfdfd0f4e64fb6fb28f8]\ndocker_container.mycontainer[4]: Refreshing state... [id=46ab03a8cd2616f20d34058d20b298df8ecc84bf5e8cfaf8013086f21ba1f501]\ndocker_container.mycontainer[0]: Refreshing state... [id=4ce1d4e0617994ba225701e5e0ef243daa8762968c8d7202be2612ecaeaca064]\n\nOutputs:\n\ndocker_containers = [\n\"0,mycontainer00,4ce1d4e0617994ba225701e5e0ef243daa8762968c8d7202be2612ecaeaca064\",\n\"1,mycontainer01,8c80b558394f62fe004ff156fa1fee06ce723f2b0721cfdfd0f4e64fb6fb28f8\",\n\"2,mycontainer02,36cf1496a418395408a9d5630c8e1cf8b12fa948ab73fe95a3b1a3d8ce767e99\",\n\"3,mycontainer03,c0bc456f847a4e403ee4bcd133f79a21f22b4ec8f94db8793160b28807789e06\",\n\"4,mycontainer04,46ab03a8cd2616f20d34058d20b298df8ecc84bf5e8cfaf8013086f21ba1f501\",\n]\n</code></pre> <ol> <li>Execute a <code>terraform show</code> again and recount the instance in the state. Why doesn't the output variable change?</li> <li>Execute a <code>terraform apply</code> (without the <code>-auto-approve</code>) and review what will be updated. Once you are satisfied with the changes that will happen, enter <code>yes</code> at the prompt.</li> </ol> Expected Response <pre><code>$ terraform apply \ndocker_image.mydocker: Refreshing state... [id=sha256:89da1fb6dcb964dd35c3f41b7b93ffc35eaf20bc61f2e1335fea710a18424287nginx:latest]\ndocker_container.mycontainer[0]: Refreshing state... [id=4ce1d4e0617994ba225701e5e0ef243daa8762968c8d7202be2612ecaeaca064]\ndocker_container.mycontainer[1]: Refreshing state... [id=8c80b558394f62fe004ff156fa1fee06ce723f2b0721cfdfd0f4e64fb6fb28f8]\ndocker_container.mycontainer[4]: Refreshing state... [id=46ab03a8cd2616f20d34058d20b298df8ecc84bf5e8cfaf8013086f21ba1f501]\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n+ create\n\nTerraform will perform the following actions:\n\n# docker_container.mycontainer[2] will be created\n+ resource \"docker_container\" \"mycontainer\" {\n    + attach                                      = false\n    + bridge                                      = (known after apply)\n    + command                                     = (known after apply)\n    + container_logs                              = (known after apply)\n    + container_read_refresh_timeout_milliseconds = 15000\n    + entrypoint                                  = (known after apply)\n    + env                                         = (known after apply)\n    + exit_code                                   = (known after apply)\n    + hostname                                    = (known after apply)\n    + id                                          = (known after apply)\n    + image                                       = \"sha256:89da1fb6dcb964dd35c3f41b7b93ffc35eaf20bc61f2e1335fea710a18424287\"\n    + init                                        = (known after apply)\n    + ipc_mode                                    = (known after apply)\n    + log_driver                                  = (known after apply)\n    + logs                                        = false\n    + must_run                                    = true\n    + name                                        = \"mycontainer02\"\n    + network_data                                = (known after apply)\n    + read_only                                   = false\n    + remove_volumes                              = true\n    + restart                                     = \"no\"\n    + rm                                          = false\n    + runtime                                     = (known after apply)\n    + security_opts                               = (known after apply)\n    + shm_size                                    = (known after apply)\n    + start                                       = true\n    + stdin_open                                  = false\n    + stop_signal                                 = (known after apply)\n    + stop_timeout                                = (known after apply)\n    + tty                                         = false\n    + wait                                        = false\n    + wait_timeout                                = 60\n\n    + ports {\n        + external = 8082\n        + internal = 80\n        + ip       = \"0.0.0.0\"\n        + protocol = \"tcp\"\n        }\n    }\n\n# docker_container.mycontainer[3] will be created\n+ resource \"docker_container\" \"mycontainer\" {\n    + attach                                      = false\n    + bridge                                      = (known after apply)\n    + command                                     = (known after apply)\n    + container_logs                              = (known after apply)\n    + container_read_refresh_timeout_milliseconds = 15000\n    + entrypoint                                  = (known after apply)\n    + env                                         = (known after apply)\n    + exit_code                                   = (known after apply)\n    + hostname                                    = (known after apply)\n    + id                                          = (known after apply)\n    + image                                       = \"sha256:89da1fb6dcb964dd35c3f41b7b93ffc35eaf20bc61f2e1335fea710a18424287\"\n    + init                                        = (known after apply)\n    + ipc_mode                                    = (known after apply)\n    + log_driver                                  = (known after apply)\n    + logs                                        = false\n    + must_run                                    = true\n    + name                                        = \"mycontainer03\"\n    + network_data                                = (known after apply)\n    + read_only                                   = false\n    + remove_volumes                              = true\n    + restart                                     = \"no\"\n    + rm                                          = false\n    + runtime                                     = (known after apply)\n    + security_opts                               = (known after apply)\n    + shm_size                                    = (known after apply)\n    + start                                       = true\n    + stdin_open                                  = false\n    + stop_signal                                 = (known after apply)\n    + stop_timeout                                = (known after apply)\n    + tty                                         = false\n    + wait                                        = false\n    + wait_timeout                                = 60\n\n    + ports {\n        + external = 8083\n        + internal = 80\n        + ip       = \"0.0.0.0\"\n        + protocol = \"tcp\"\n        }\n    }\n\nPlan: 2 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n~ docker_containers = [\n    - \"0,mycontainer00,4ce1d4e0617994ba225701e5e0ef243daa8762968c8d7202be2612ecaeaca064\",\n    - \"1,mycontainer01,8c80b558394f62fe004ff156fa1fee06ce723f2b0721cfdfd0f4e64fb6fb28f8\",\n    - \"2,mycontainer02,36cf1496a418395408a9d5630c8e1cf8b12fa948ab73fe95a3b1a3d8ce767e99\",\n    - \"3,mycontainer03,c0bc456f847a4e403ee4bcd133f79a21f22b4ec8f94db8793160b28807789e06\",\n    - \"4,mycontainer04,46ab03a8cd2616f20d34058d20b298df8ecc84bf5e8cfaf8013086f21ba1f501\",\n    ] -&gt; (known after apply)\n\nDo you want to perform these actions?\nTerraform will perform the actions described above.\nOnly 'yes' will be accepted to approve.\n\nEnter a value: \n</code></pre> <ol> <li><code>terraform destroy -auto-approve</code></li> </ol>"},{"location":"orchestration/advterra/#validating-your-input-data","title":"Validating your input data","text":"<p>Next we will show how to add validation to your inputs 1. In your <code>input.tf</code>, add the following variable <pre><code>variable \"port_assignment_list\" {\n  type = list(number)\n  description = \"list of port assignments, size should = num_containers\"\n  default = []\n  validation {\n    condition     = length(var.port_assignment_list) &gt; 0\n    error_message = \"Port assignment not &gt; 0\"\n  }\n}\n</code></pre> 2. In your <code>main.tf</code>, replace your docker_container resource to the following: <pre><code>resource \"docker_container\" \"mycontainer\" {\n  count = var.num_containers\n  image = docker_image.mydocker.image_id\n  name = \"${format(\"%s%02d\", var.container_name, count.index)}\"\n  ports {\n    internal = 80\n    external = var.port_assignment_list[count.index] # illustrates using count.index to indicate which port to use\n  }\n}\n</code></pre> 3. Add/update the following variables in your <code>terraform.tfvars</code> <pre><code>num_containers=5\nport_assignment_list=[]\n</code></pre> 4. <code>terraform apply -auto-approve</code> 5. What do you expect?     - Try updating the condition such that length must equal <code>num_containers</code></p> <p>Now, let's figure out how to make our validation slightly more useful 1. If necessary, restore your <code>input.tf</code> to include the original validation block <pre><code>variable \"port_assignment_list\" {\n  type = list(number)\n  description = \"list of port assignments, size should = num_containers\"\n  default = []\n  validation {\n    condition     = length(var.port_assignment_list) &gt; 0\n    error_message = \"Port assignment not &gt; 0\"\n  }\n}\n</code></pre> 2. Update your <code>main.tf</code> to include a validation block like the following: <pre><code>resource \"docker_container\" \"mycontainer\" {\n  count = var.num_containers\n  image = docker_image.mydocker.image_id\n  name = \"${format(\"%s%02d\", var.container_name, count.index)}\"\n  ports {\n    internal = 80\n    external = var.port_assignment_list[count.index] # illustrates using count.index to indicate which port to use\n  }\n\n  lifecycle {\n    precondition {\n      condition = length(var.port_assignment_list) == num_containers\n      error_message = \"length != num_containers\"\n    }\n  }\n}\n</code></pre> 3. Update your <code>terraform.tfvars</code> with the following settings <pre><code>num_containers=5\nport_assignment_list=[4040,5050,6060,7070]\n</code></pre> 4. <code>terraform apply -auto-approve</code> 5. What do you expect?     - Try adding another port (or remove a port) so that <code>num_containers</code> and the length of the list is equal 6. <code>terraform destroy -auto-approve</code></p>"},{"location":"orchestration/advterra/#increasing-the-number-of-containers-using-for_each","title":"Increasing the number of containers using for_each","text":"<p>Next we will create multiple docker containers, but using a different method, <code>for_each</code>.</p> <ol> <li>copy <code>input.tf</code>, <code>main.tf</code> from <code>01c-multiple-containers</code> (overwrite your existing files)<ol> <li>review the differences in <code>input.tf</code></li> <li>review the differences in <code>main.tf</code></li> </ol> </li> <li>remove the <code>num_containers</code> from your <code>terraform.tfvars</code> and add a new variable called <code>containers_map</code>, something like <pre><code>containers_map={\n    cat = 8080,\n    dog = 8090,\n    bird = 9090,\n    cow = 9080,\n    horse = 9070\n}\n</code></pre></li> <li><code>terraform apply -auto-approve</code></li> <li>Notice some pieces of the codes that were introduced<ol> <li>Notice the declaration of a <code>map</code>, the default value, and how to set it in <code>terraform.tfvars</code></li> <li>Map keys do not need quoting. What about values?</li> <li>How would you compare the how ports are configured in between the use of <code>count</code> and <code>for_each</code></li> <li>Why might you use <code>count</code> and <code>for_each</code></li> </ol> </li> <li><code>terraform destroy -auto-approve</code></li> </ol>"},{"location":"orchestration/advterra/#using-dynamic-blocks-create-repeatable-nestable-blocks-in-resources","title":"Using dynamic blocks create repeatable nestable blocks in resources","text":"<p>Next we will see an example of resource properties that can be repeated</p> <ol> <li>Copy <code>input.tf</code>, <code>main.tf</code> from <code>02a-using-ports</code> (overwrite your existing files)<ol> <li>Review the differences in <code>main.tf</code></li> </ol> </li> <li>Edit <code>input.tf</code> with a new variable <pre><code>variable \"container_ports\" {\n    type = list(object({\n      internal=string,\n      external=string\n    }))\n    description = \"map(object), port object as defined by https://registry.terraform.io/providers/kreuzwerker/docker/latest/docs/resources/container#nestedblock--ports\"\n    default = []\n}\n</code></pre></li> <li>edit the main.tf with the following <code>docker_container</code> resource definition <pre><code>resource \"docker_container\" \"mycontainer\" {\n  image = docker_image.mydocker.image_id\n  name  = var.container_name\n\n  dynamic \"ports\" {\n    for_each = var.container_ports\n    content {\n      internal = ports.value.internal\n      external = ports.value.external\n    }\n  }\n}\n</code></pre></li> <li>Add the following variable in your <code>terraform.tfvars</code> <pre><code>ports = [\n    {internal=\"80\", external=\"8080\"},\n    {internal=\"90\", external=\"9090\"}\n]\n</code></pre></li> <li><code>terraform apply -auto-approve</code></li> <li>Verify the ports were added when you use <code>docker ps</code></li> <li><code>terraform destroy -auto-approve</code></li> </ol>"},{"location":"orchestration/cacao/","title":"CyVerse CACAO Interface","text":""},{"location":"orchestration/cacao/#log-into-cacao-js2","title":"Log into CACAO (JS2)","text":"<p>To start, you will need an ACCESS user account and have access to a project.</p> <p>Go to https://cacao.jetstream-cloud.org to log in to CACAO. You will be prompted to authenticate with ACCESS.</p> <p>Info</p> <p>You should select \"ACCESS CI (XSEDE)\" as your identity provider unless you linked your other identities through CILogon. If in doubt, select \"ACCESS CI (XSEDE)\".</p> <p></p>"},{"location":"orchestration/cacao/#add-your-cloud-credential","title":"Add your Cloud Credential","text":"<p>Next, you will need to add cloud computing credentials. For now, the only option is Jetstream 2, but in the future, other providers like Google and AWS will be supported.</p> <p>Click on Credentials on the lefthand menu bar. From the Credentials page, click the \"+ Add Credential\" button and select \"Cloud Credential\". Select Jetstream 2 and the project you would like to add. These correspond to Jetstream 2 projects you have access to.</p> <p></p> <p>Once your credential is added, it should show up on the Credentials page.</p>"},{"location":"orchestration/cacao/#add-your-ssh-key","title":"Add your SSH Key","text":"<p>Next, you will need to add a public SSH key. This will allow you to access your VMs via SSH. If you already have an SSH key, you can use that. Otherwise, you can generate a new one.</p> <p>Again, click on the \"+ Add Credential\" button and this time select \"Public SSH Key\". Enter a name and paste in your public key.</p> <p></p>"},{"location":"orchestration/cacao/#start-a-deployment","title":"Start a Deployment","text":"<p>Next, we will start a deployment onto Jetstream 2.</p> <p>Click on the  Deployments tab on the lefthand menu bar. You will see your cloud providers and projects, and if you have multiple providers or projects, you can select them here. You should have Jetstream 2 and a project selected.</p> <p>Next, click the \"+ Add Deployment\" button. You will see several options for default templates to launch VMs, containers, or whole clusters. We will use the first template to launch a single VM.</p> <p></p> <p>You will then name the deployment, select the number of instances and size of instances. For now, stick with the default Featured-AlmaLinux8 image and 1 instance of m3.tiny. You can name it whatever you want.</p>"},{"location":"orchestration/cacao/#access-deployment","title":"Access Deployment","text":"<p>Once you have submitted your deployment, you can see it on the Deployments page. It will be in the \"Starting\" status for a few minutes. Once it is running, you can click on it to see details about it and to access it.</p> <p></p> <p>From here, you can click on the icons on the right to access a Web Shell or a Web Desktop (for certain images). You can also pause, shelve, or delete the deployment from here. Try opening the Web Shell, which will bring you to a command line inside your running VM.</p>"},{"location":"orchestration/cacao_terra/","title":"CACAO Terraform templates on the CLI","text":""},{"location":"orchestration/cacao_terra/#overview","title":"Overview","text":"<p>This tutorial will guide you through the process importing a basic terraform template into CACAO.</p> <p>A more detailed version of this tutorial can be found here https://docs.jetstream-cloud.org/ui/cacao/import_terraform_template/</p> <p>By the end of this tutorial, you will have imported a terraform template into CACAO and deployed it using the UI to confirm that it works.</p>"},{"location":"orchestration/cacao_terra/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of Terraform, git, and the command line.</li> <li>An OpenStack environment with access to the API (if you need to validate any terraform changes).</li> <li>Terraform installed on your local machine or VM server (if you need to validate any terraform changes).</li> <li>An SSH key pair to access the VM server running Terraform (if you need to validate any terraform changes).</li> </ul>"},{"location":"orchestration/cacao_terra/#cyverse-cacao-browser-ui","title":"CyVerse CACAO Browser UI","text":"<p>One of CyVerse's test deployments for CACAO will be used for this exercise so that we can import a template using the newer metadata schemas, which will be deployed to production in the very near future.</p> <p>The url for the CACAO site that we will use today is https://cacao.jetstream-cloud.org</p> <p>Please login to verify that your ACCESS credentials work with the CACAO test site. !!!+ warn     For the workshop, use the \"ACCESS CI (XSEDE)\" identity provider when you login.     </p>"},{"location":"orchestration/cacao_terra/#installation-of-the-cyverse-cacao-cli","title":"Installation of the CyVerse CACAO CLI","text":"<p>Info</p> <p>If you are using a VM provided by the workshop, the CACAO CLI is already installed. You can skip this installation section.</p> <p>The CyVerse CACAO CLI is a command line tool interact with the CyVerse CACAO API. The first step is the install the cli on your local machine or to a vm.</p> <ul> <li>Linux download for CACAO CLI</li> <li>Windows download for CACAO CLI</li> <li>MacOS (Intel) download for CACAO CLI</li> <li>MacOS (ARM) download for CACAO CLI</li> </ul> <p>If you're using a VM provided by the workshop or a Linux system, you can use these instructions to install the CACAO CLI.</p> <ol> <li>If necessary, obtain a shell or terminal on the Linux system you wish to install the CACAO CLI.</li> <li>Copy link for \"Linux download for CACAO CLI\" from above.</li> <li><code>curl https://gitlab.com/cyverse/cacao/-/package_files/88735155/download --output cacao.zip</code></li> <li><code>unzip cacao.zip # this will create a file cacao_linux_amd64</code></li> <li><code>sudo mv cacao_linux_amd64 /usr/local/bin/cacao # optional; otherwise, add it into your path</code></li> <li><code>chmod +x /usr/local/bin/cacao</code></li> </ol>"},{"location":"orchestration/cacao_terra/#reviewing-the-terraform-template-we-will-use-to-import","title":"Reviewing the Terraform template we will use to import","text":"<p>We will be using https://github.com/cyverse/cacao-terraform-hello-world for this tutorial.</p> <p>You will find a directory called <code>.cacao</code> at the root of the template and two files:</p> <ul> <li><code>metadata.json</code>, which contains the CACAO-specific metadata for the template</li> <li><code>ui.json</code>, which contains the UI-specific hints for the template</li> </ul> <p>The <code>ui.json</code> file is optional, and if not present, then the CACAO UI will present a default naive rendering of the template.</p> <p>Details about the metadata and ui schemas can be found here: https://docs.jetstream-cloud.org/ui/cacao/import_terraform_template/</p> <p>Warn</p> <p>The UI metadata specification is currently in flux and will likely change in the near future, especially as we get feedback from the community.</p>"},{"location":"orchestration/cacao_terra/#stop-if-you-want-to-fork-the-template-lets-do-it-now","title":"Stop! If you want to fork the template, let's do it now.","text":"<p>!!!+ info     This is an optional step. You do not need to fork a template for this part of the tutorial, but you may if you wish.</p>"},{"location":"orchestration/cacao_terra/#using-the-cacao-cli","title":"Using the CACAO CLI","text":""},{"location":"orchestration/cacao_terra/#login-to-cacao-cli","title":"Login to CACAO CLI","text":"<ol> <li><code>cacao login --browser</code></li> </ol> What to do if you encounter a login issue <p>Sometimes login using the command line will fail -- a typo happens, a copy-n-paste of a token happens, using the wrong api url happens, etc -- and you need to reset your login. To reset your login, you can use the following command:</p> <p><code>cacao logout</code></p> Expected Response <pre><code>Please provide address of Cacao API.\nFormat Should be: http://&lt;hostname&gt;:&lt;port&gt;        or    &lt;hostname&gt;:&lt;port&gt; \n(Developers: this should match the value of API_DOMAIN in install/config.yaml followed by \"/api\", e.g. ca.cyverse.local/api)\nCacao API address (http://ca.cyverse.local/api): https://cacao.jetstream-cloud.org/api\n</code></pre> <ol> <li>Enter the CACAO API url: <code>https://cacao.jetstream-cloud.org/api</code></li> </ol> Expected Response <pre><code>Cacao API address (http://ca.cyverse.local/api): https://cacao.jetstream-cloud.org/api\nPlease go to this URL in the browser: https://cacao.jetstream-cloud.org/api/user/login\n\nAfter login, you should get a JSON response, the auth token could be the value of following properties:\n- \"IDToken\" or \"id_token\" if keycloak\n- \"access_token\" if other auth provider\n\nEnter the auth token you get from browser: \n</code></pre> <ol> <li>In your browser open the following URL (this is also echoed in the terminal, line #5): https://cacao.jetstream-cloud.org/api/user/login !!!+ warn     For the workshop, use the \"ACCESS CI (XSEDE)\" identity provider when you login.     </li> <li>You will need to grab the the \"access_token\" that is returned from the browser and paste the following text into the terminal: \"Bearer\" followed by a space and then the access token. For example, if the access_token were GOOSE, the the string you enter will be: <code>Bearer GOOSE</code></li> </ol> <p>Tip</p> <ul> <li>Do not include the quotes around the token. </li> <li><code>Bearer</code> is case sensitive.</li> <li>There is one space between Bearer and the token.</li> </ul> <p>Below are screenshots of what you might expect to see from Chrome and Firefox.</p> <p>Chrome:  Firefox: </p> <p>After pasting the \"Bearer \" + access_token, you should return to the shell prompt.</p> Expected Response <pre><code>Enter the auth token you get from browser: Bearer NB2HI4DTHIXASDFUINASDKLFASDHFKASDFJASDFA23FNYTHI4Z5GE3DSMRTGMYDGNJWHE3TIJTWMVZHG2LPNY6XMMROGATGY2LGMV2GS3LFHU4TAMBQGAYA\n$ \n</code></pre> <ol> <li>To test a successful login, you can execute a cacao command, such as: <code>cacao provider get</code></li> </ol>"},{"location":"orchestration/cacao_terra/#importing-a-terraform-template-into-cacao","title":"Importing a Terraform template into CACAO","text":"<p><code>cacao template create git &lt;source url&gt;  &lt;template name&gt; --branch &lt;git branch&gt; --path &lt;path to template&gt;</code></p> <p>Here are the values to enter for each:</p> <ul> <li><code>&lt;source url&gt;</code>: <code>https://github.com/cyverse/cacao-terraform-hello-world.git</code></li> <li><code>&lt;template name&gt;</code>: \"-hello-world\" (e.g. student0001-hello-world) <li><code>&lt;git branch&gt;</code>: <code>main</code></li> <li><code>&lt;path to template&gt;</code>: <code>.</code></li> <p>!!!+ warn     If you forked the template, you will need to use your forked url.</p> <p>Putting this all together, your command will look similar to this: </p> <pre><code>cacao template create git https://github.com/cyverse/cacao-terraform-hello-world.git  student0001-hello-world --branch main --path .\n</code></pre> Expected Response <pre><code>{\n    \"tid\": \"template-cjfepm8795ktgsth2750\",\n    \"timestamp\": \"2023-08-18T04:00:28.179794071Z\"\n}\n</code></pre> <p>!!!+ note     Save the <code>tid</code> value for later use.</p>"},{"location":"orchestration/cacao_terra/#launching-a-template-using-the-cacao-ui","title":"Launching a template using the CACAO UI","text":"<ol> <li>Open the CACAO UI in your browser: https://cacao.jetstream-cloud.org</li> <li>Click on the \"Deployments\" tab on the left-hand side.</li> <li>Click on the \"+ Add Deployment\" button.</li> <li>Select the \"edwin's hello world example\"/\"student0001-hello-world\" template. (Note: the subname of the template will be different for you.)</li> <li>Click the \"Next\" button.</li> <li>Fill out the Parameters<ol> <li><code>Choose Region</code>: leave as default</li> <li><code>Instance name</code>: enter your student name (e.g. student0001) + date e.g. <code>student0001-2021-08-18</code></li> <li><code>Image</code>: select \"Featured-Ubuntu22\"</li> <li><code>Size</code>: leave as default </li> </ol> </li> <li>Click the \"Next\" button.</li> <li>After reviewing the summary, click the \"Submit\" button.</li> <li>You will be listed on the \"Deployments\" page. You can click on the deployment to see the status of the deployment.</li> </ol> <p>While you are waiting, questions or we can begin the next section, \"What if you want to customize a template?\"</p>"},{"location":"orchestration/cacao_terra/#cleaning-up-if-you-didnt-fork-your-template","title":"Cleaning up (if you didn't fork your template)","text":"<p>!!!+ warn     You will not be able to delete a template if there are any active deployments using it. 1. Delete the deployment in the UI or cli (<code>cacao deployment delete &lt;deployment id</code>). 2. <code>cacao template delete &lt;template id&gt;</code></p> Expected Response <pre><code>{\n    \"id\": \"template-cjfepmuf95ktgsth2750\",\n    \"tid\": \"template-cjfepmuf95ktgsth2750\",\n    \"timestamp\": \"2023-08-18T04:36:07.420852435Z\"\n}\n</code></pre>"},{"location":"orchestration/cacao_terra/#what-if-you-make-changes-to-a-template-and-want-to-use-it-for-new-deployments","title":"What if you make changes to a template and want to use it for new deployments?","text":"<p>Answer: You do not need to do anything. The CACAO UI will automatically detect changes to the template on the configure branch and will use the latest branch code new deployments.</p>"},{"location":"orchestration/cacao_terra/#what-if-you-make-metadata-changes-to-a-template-eg-cacaometadatajson-or-cacaouijson-and-want-to-use-it-for-new-deployments","title":"What if you make metadata changes  to a template (e.g. <code>.cacao/metadata.json</code> or <code>.cacao/ui.json</code>) and want to use it for new deployments?","text":"<p>Answer: <code>cacao template sync &lt;tid&gt;</code></p>"},{"location":"orchestration/cacao_terra/#for-those-of-you-who-forked-the-repo-we-can-experiment-with-template-modifications","title":"For those of you who forked the repo, we can experiment with template modifications","text":"<p>Here are a couple suggestions * change the name of the template * remove image (and/or size) from the template and set it</p>"},{"location":"orchestration/cacao_terra/#what-if-you-want-to-customize-a-template-you-dont-own-but-is-public","title":"What if you want to customize a template you don't own but is public?","text":"<p>If there is time, we can go through this section. Otherwise, you can try this on your own.</p> <ol> <li>Fork the template into your own github or gitlab account</li> <li>Add a <code>.cacao/metadata.json</code> and <code>.cacao/ui.json</code> to your forked template</li> <li>Import the template using the <code>cacao template create git</code> command.</li> </ol>"},{"location":"orchestration/k8s/","title":"Lesson 1: Introduction to Kubernetes","text":"<p>Learning Objectives</p> <ul> <li>Understand what Kubernetes is and its role in managing containerized applications</li> <li>Recognize the importance and benefits of using Kubernetes in cloud computing</li> <li>Identify reasons why you may need to use Kubernetes for your own research or projects</li> <li>Learn about the existence of Kubernetes clusters and how to leverage them using <code>kubectl</code></li> </ul> <p> Kubernetes, or <code>K8s</code> for short, is an open-source system designed to automate the deployment, scaling, and management of containerized applications. </p> <p>K8s is the most prevalent platform for managing containerized applications at scale in the realm of cloud computing. If you're considering incorporating containers into your research or project portfolio, it's highly likely you have already interacted with or will interact with a K8s cluster in the future.</p> <p>In this introductory lesson, we'll focus on how to leverage existing Kubernetes Clusters using <code>kubectl</code>.</p>  K8s vs  K3s <p>We are going to be using a platform developed by a project named  Rancher, called  K3s.</p> <p>K3s is a lightweight certified Kubernetes distribution designed for edge and production workloads.  </p> <p>K3s uses the same <code>kubectl</code> and <code>helm</code> commands as full K8s. </p> <p>We prefer K3s to K8s for our virtual machines and for workshops for the same reason you might prefer to use Alpine Linux to Ubuntu Linux in a simple container deployment.</p> What is container orchestration? <p>Container orchestration is the process of automating the deployment, scaling, management, and coordination of containerized applications. In the context of Kubernetes, container orchestration involves managing the lifecycle of containers within a cluster. This includes tasks such as deploying containers, ensuring high availability, distributing network traffic, scaling applications up or down based on demand, and handling updates seamlessly.</p>  Why would you want to build your own K8s Cluster?  <p>You're here to learn about containerized applications and container orchestration. We aim to familiarize you with these technologies and help you understand their place in the roadmap of Cloud Native Computing. However, it's crucial to note,</p> <p> Designing, deploying, and maintaining your own K8s cluster can be a complex and challenging task </p> <ul> <li>This is partly because there are numerous existing platforms out there, such as managed K8s services. Additionally, managing K8s requires a dedicated DevOps or software engineering team to ensure your platform operates smoothly.</li> </ul> <p>Still interested in building your own cluster? Here are some reasons why you might need to use K8s for your research or project:</p> <p>(1) Your applications consist of multiple services. The K8s API automates the tasks of managing numerous containers and allocating resources. </p> <p>(2) Your work scales dynamically - if your computing needs fluctuate based on workloads, K8s can be useful. Using containers to scale your applications is more efficient than manually launching VMs.</p> <p>(3) You have too many containers to manage - K8s excels at its primary function: managing and maintaining containers. </p> <p>(4) Your domain is transitioning to the cloud. If your field is moving towards being cloud-native, it's crucial to develop workflows in anticipation of this shift.</p> <p>(5) Consistency is key. K8s' declarative state provides a clear description of how everything is managed.</p> <p>Kubernetes Terminology</p> <p>Kubernetes Cluster: A collection of nodes that run containerized applications. This is the primary unit of organization in Kubernetes.</p> <p>kubectl: The command-line interface used for interacting with Kubernetes clusters. It enables users to manage various aspects of the cluster.</p> <p>Pod: The smallest deployable unit in Kubernetes, which can consist of one or more containers that communicate with each other. Pods are run on nodes.</p> <p>Node: Typically a virtual machine (VM) that runs Kubernetes components, including the kubelet, kube-proxy, and container runtime.</p> <p>Kubelet: The agent that runs on each node, responsible for managing the pods on its respective VM.</p> <p>Kube-proxy: A network proxy that runs on each node, used by a Service to handle network communication.</p> <p>Container runtime: The software that runs containers within pods. Examples include <code>Docker</code>, <code>containerd</code> (native to Kubernetes), and <code>CRI-O</code>.</p> <p>Service: Assigns a network address to an application, including a persistent IP address and a DNS entry within the cluster. A Service also manages load balancing across pods and can dynamically add or remove them.</p> <p>ReplicaSet: Allows for the creation of multiple pods simultaneously, ensuring that a specified number of identical pods are running at any given time.</p> <p>Deployment: Provides declarative updates for Pods and ReplicaSets, allowing for rolling updates and rollbacks.</p> <p>Control Plane: A node that manages worker nodes. It consists of the API server, cluster store, controller manager, and scheduler.</p> <p>Namespace: A way to divide cluster resources between multiple users.</p> <p>Ingress: An API object that manages external access to the services in a cluster, typically HTTP.</p> <p>Persistent Volume (PV): A piece of storage in the cluster that has been provisioned by an administrator.</p> <p>Persistent Volume Claim (PVC): A request for storage by a user.</p> <p>ConfigMap: An API object used to store non-confidential data in key-value pairs.</p> <p>Secret: An API object used to store sensitive data, like passwords and keys.</p> <p>So, what does a K8s Cluster look like?</p> <p>In short, this:</p> <p></p> <p>The image above is taken from the official K8s documentation and depics the relation between each component.</p> <p>A K8s cluster comprises of multiple elemets essentially groupable in 2 subsets:</p> <ul> <li>The Control Plane components:<ul> <li>api (K8s API): the front end of the control plane, it exposes the Kubernentes API. Command line tool: <code>kube-apiserver</code>.</li> <li>etcd: a store used for backing cluster data (e.g., cluster cofiguration, state information). Controlled by the <code>kube-apiserver</code> tool.</li> <li>sched (Scheduler): the component that watches for newly created Pods with no assigned node. Once a new Pod is detected, a node is assigned. Controlled by <code>kube-scheduler</code>.</li> <li>c-m (Controller Manager): controls the controllers processes. Accessible through <code>kube-controller-manager</code></li> <li>c-c-m (Cloud Controller Manager): lets you to link your cluster to your provider's API, allowing you to choose what components interact with the external platform and which components interact with the internal cluster. Read more on the cloud controller manager here.</li> </ul> </li> <li>The Node components:<ul> <li>kubelet: An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod. <code>kubelet</code></li> <li>k-proxy (kube-proxy): maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. <code>kube-proxy</code></li> </ul> </li> </ul> <p>We keep talking about Pods and Nodes, what are Pods and Nodes?</p> <p>Pods</p> <p></p> <p>A Pod is the smallest deployable unit in Kubernetes. It represents a single instance of a running process in a cluster and encapsulates one or more closely related containers. Containers within the same Pod share the same network namespace, IP address, and storage volumes, making them suitable for co-located and tightly coupled applications.</p> <p>In the image above, we see 4 different Pods, each having at least one containerized app. Notice how each Pod has its own IP address, and apps within the same Pods share volumes for storage and IP address.</p> <p>Nodes</p> <p></p> <p>A Node is a physical or virtual machine that runs containers. Nodes are the worker machines in a Kubernetes cluster where Pods are scheduled and executed. Nodes collectively form the computational resources of the cluster, where containers are scheduled and executed. The interaction between Pods and Nodes forms the core of how Kubernetes manages and distributes workloads within a cluster.</p> <p>In the image, the Node contains different Pods. Notice <code>kubelet</code> and Docker: </p> <ul> <li><code>kubelet</code>  is a component that runs on each Node in a Kubernetes cluster and manages its life cycle, ensuring that the Node is healthy.</li> <li>Docker provides the runtime environment for containers, whilst K8s manages the orchestration.</li> </ul>"},{"location":"orchestration/k8s/#k8s-cli-kubectl","title":"K8s CLI <code>kubectl</code>","text":"<p>The Kubernetes API uses a command-line tool called <code>kubectl</code>.</p> <p>Using K8s does not require you to own or maintain your own cluster. You can use the <code>kubectl</code> tool to connect to running clusters and start your containers.</p>"},{"location":"orchestration/k8s/#install-kubectl","title":"Install <code>kubectl</code>","text":"<p>Official Documentation</p> <p><code>kubectl</code> is already installed in CodeSpaces.</p> <p>To install on Linux:</p> <pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\nchmod +x kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"orchestration/k8s/#kubeconfig","title":"kubeconfig","text":"<p>K8s uses YAML files for configuring a cluster. </p> <p>The <code>config</code> file is required to make the handshake between the K8s Cluster and external requests, like the one you're making from your local computer or CodeSpace.</p>"},{"location":"orchestration/k8s/#config","title":"<code>config</code>","text":"<p>To connect to a running K8s cluster, you need to create a <code>config</code> (.yaml) and place it in the <code>~/.kube/</code> folder.</p> <p>In our demo K3s cluster, the <code>config</code> file is maintained in the <code>/etc/rancher/k3s/k3s.yaml</code>. You can copy the file to  <code>~/.kube/config</code> or create a symlink. </p>  example <code>config</code> file <p>Example of the <code>~/.kube/config</code> file which is used for K3s</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: &lt;REDACTED&gt;\n    server: https://127.0.0.1:6443\n  name: default\ncontexts:\n- context:\n    cluster: default\n    user: default\n  name: default\ncurrent-context: default\nkind: Config\npreferences: {}\nusers:\n- name: default\n  user:\n    client-certificate-data: &lt;REDACTED&gt;\n    client-key-data: &lt;REDACTED&gt;\n</code></pre> <ol> <li> <p>Copy the <code>~/.kube/config</code> file from your cluster or use our example here over to your localhost or VM and put it into a temporary directory.</p> </li> <li> <p>Make a second copy and put it into your own <code>~/.kube/config</code> folder</p> </li> </ol> <p>By default, the <code>config</code> file is in the <code>~/.kube/</code> directory, but it can be put anywhere or given any name, use the <code>--kubeconfig</code> flag to tell <code>kubectl</code> where to get the config:</p> <pre><code>kubectl --kubeconfig /custom/path/kube.config get pods\n</code></pre>"},{"location":"orchestration/k8s/#setting-up-the-namespace","title":"Setting up the <code>namespace</code>","text":"<p><code>kubectl</code> needs to know the <code>namespace</code> of the cluster <code>config</code> you're working with. </p> <p>In this first example today, we're going to be working locally on a K3s with a new <code>namespace</code> for each of the students (so we can differentiate whose pods are whose)</p> <p>To set the context and namespace of the cluster:</p> <pre><code>kubectl config set-context student##-cc23 --namespace=student##-cloudcamp23\n</code></pre> <p>where ## is your assigned student ID. If you're doing this lesson on your own time, you can choose your own unique context and namespace.</p> <p>Once the <code>config</code> namespace is set, you should be ready to launch \"pods\" i.e. containers on the cluster</p>"},{"location":"orchestration/k8s/#pods","title":"pods","text":"<p>Each pod is managed using its own <code>.yaml</code> file. This way K8s / K3s can declaratively set the resources, container, networking, volumes, and permissions for each running pod.</p> <p>Create a new directory called <code>examples</code> on the JS2 workshop instance you have connected to.</p> <p>Create a new file called <code>alpine.yaml</code> </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: alpine-&lt;change-this-to-your-username&gt;\nspec:\n  containers:\n  - name: mypod\n    image: alpine:3.14\n    resources:\n      limits:\n        memory: 100Mi\n        cpu: 100m\n      requests:\n        memory: 100Mi\n        cpu: 100m\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\n</code></pre>"},{"location":"orchestration/k8s/#launching-a-pod","title":"Launching a Pod","text":"<p>Try launching a pod on the cluster</p> <pre><code>kubectl create -f alpine.yaml\n</code></pre> <p>Check to see that it is running</p> <pre><code>kubectl get pods\n</code></pre> <p>Do you see any other pods?</p> <p>If the pod hasn't started yet, you can check the timestamp to see if it was created:</p> <pre><code>kubectl get events --sort-by=.metadata.creationTimestamp\n</code></pre> <p>Try to connect to your running pod (container)</p> <pre><code>kubectl exec -it alpine-&lt;yourusername&gt; -- /bin/sh\n</code></pre>"},{"location":"orchestration/k8s/#pod-networking","title":"Pod networking","text":"<p>Let's check the networking inside the pod</p> <pre><code># uncomment if ifconfig is not installed\n# apk add net-tools\nifconfig -a\n</code></pre> <p>Exit the pod (<code>ctrl</code>^<code>D</code>)</p> <p>Check the IP with <code>kubectl</code></p> <pre><code>kubectl get pod -o wide pod-&lt;yourname&gt;\n</code></pre>"},{"location":"orchestration/k8s/#taking-down-a-pod","title":"Taking down a Pod","text":"<p>Once you've exited the pod, delete it</p> <pre><code>kubectl delete -f pod1.yaml\n</code></pre> <p>double check that its gone</p> <pre><code>kubectl get pods\n</code></pre>"},{"location":"orchestration/k8s/#create-a-deployment","title":"Create a Deployment","text":"<p>While we can create and delete pods on our own, what we realy want is to make our containers have \"high availability\". </p> <p>High availiability means that when a node dies or is restarted, the pod will \"come back up\" on its own.</p> <p>First, we can create a <code>deployment.yaml</code></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: alpine-instructor-deployment\nspec:\n  replicas: 3 # Number of Pod replicas\n  selector:\n    matchLabels:\n      app: alpine-instructor\n  template:\n    metadata:\n      labels:\n        app: alpine-instructor\n    spec:\n      containers:\n      - name: mypod\n        image: alpine:3.14\n        resources:\n          limits:\n            memory: 100Mi\n            cpu: 100m\n          requests:\n            memory: 100Mi\n            cpu: 100m\n        command: [\"sh\", \"-c\", \"sleep infinity\"]\n</code></pre> <p>This <code>deplyoment.yaml</code> will create three replicas of the given Pod, each with the specified `alpine`` image, resource limits, and command. You can adjust the number of replicas or other parameters as needed for your workshop demonstration.</p> <p>To create the deployment using the provided YAML file, you can save the file to your local system (e.g., deployment.yaml) and then run the following kubectl command:</p> <pre><code>kubectl apply -f deployment.yaml\n</code></pre> Why bother with Deployments? <p>High Availability: By running three replicas of the Pod, the application becomes more resilient to failures. If one Pod fails, the other two can continue to handle requests, ensuring that the application remains available.</p> <p>Load Balancing: Kubernetes automatically distributes incoming traffic across the replicas. This helps in evenly spreading the load, leading to better utilization of resources and potentially improved response times.</p> <p>Fault Tolerance: If a node in the cluster fails, the Pods running on that node will be lost. Having multiple replicas ensures that the application continues to run on the remaining nodes. Kubernetes will also work to reschedule the lost Pods on other available nodes.</p> <p>Scalability: Having multiple replicas allows the application to handle more simultaneous requests. If the load increases further, the number of replicas can be easily adjusted, either manually or through autoscaling.</p> <p>Rolling Updates and Rollbacks: Deployments in Kubernetes facilitate rolling updates, allowing you to update the application with zero downtime. If something goes wrong, you can also easily roll back to a previous version. The multiple replicas ensure that some instances of the application are always available during this process.</p> <p>Observability and Monitoring: With multiple replicas, you can monitor the behavior of the application across different instances and nodes, providing insights into the system's performance and potential issues.</p> <p>Consistent Environment: Each replica runs in an identical environment, ensuring consistency in the application's behavior and facilitating testing and debugging.</p> <p>In summary, a deployment with three replicas enhances the availability, reliability, scalability, and manageability of the application within a Kubernetes cluster. It aligns well with best practices for running production-grade, distributed systems.</p>"},{"location":"orchestration/k8s/#run-a-dashboard","title":"Run a Dashboard","text":"<p>UpCloud instructions</p> <p>Once the cluster is up and running, you may choose to create a Dashboard for it</p> <p>The Dashboard can only run from a <code>localhost</code> so we have to do an ssh tunnel to connect to it</p> <pre><code>ssh -L localhost:8001:127.0.0.1:8001 &lt;user&gt;@&lt;master_public_IP&gt;\n</code></pre> <p>start the dashboard</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n</code></pre> <p>check to see the dashboard pods are running</p> <pre><code>kubectl get pods -A\n</code></pre>"},{"location":"orchestration/k8s/#create-an-admin-dashboard-user","title":"create an <code>admin</code> dashboard user","text":"<pre><code>mkdir ~/dashboard &amp;&amp; cd ~/dashboard\n</code></pre> <p>create a <code>dashboard-admin.yaml</code></p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n</code></pre> <p>deploy</p> <pre><code>kubectl apply -f dashboard-admin.yaml\n</code></pre> <p>print the <code>admin</code> token so you can log into the dashboard</p> <pre><code>kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode\n</code></pre>"},{"location":"orchestration/k8s/#create-a-read-only-dashboard-read-onlyyaml-user","title":"create a read-only <code>dashboard-read-only.yaml</code> user","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: read-only-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n  name: read-only-clusterrole\n  namespace: default\nrules:\n- apiGroups:\n  - \"\"\n  resources: [\"*\"]\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - extensions\n  resources: [\"*\"]\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources: [\"*\"]\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: read-only-binding\nroleRef:\n  kind: ClusterRole\n  name: read-only-clusterrole\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: read-only-user\n  namespace: kubernetes-dashboard\n</code></pre> <p>deploy it</p> <pre><code>kubectl apply -f dashboard-read-only.yaml\n</code></pre> <p>print read-only token</p> <pre><code>kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount read-only-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode\n</code></pre>"},{"location":"orchestration/k8s/#start-the-dashboard","title":"Start the dashboard","text":"<pre><code>kubectl proxy\n</code></pre> <pre><code>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n</code></pre> <p>Copy/Paste your admin or read-only token</p> <pre><code>kubectl -n kubernetes-dashboard create token admin-user\n</code></pre>"},{"location":"orchestration/k8s/#delete-the-dashboard","title":"delete the dashboard","text":"<pre><code>kubectl delete -f dashboard-admin.yaml\nkubectl delete -f dashboard-read-only.yaml\nkubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n</code></pre>"},{"location":"orchestration/registry/","title":"Manage private container registry","text":"<p>If you maintain your own cyberinfrastructure and are pulling many containers per day, you may consider hosting your own registry.</p>"},{"location":"orchestration/registry/#docker-registry-server","title":"Docker Registry Server","text":"<p>Setting up your own Docker Registry Server </p> <p>Registry deployment documentation</p>"},{"location":"orchestration/registry/#harbor","title":"Harbor","text":"<p>Harbor is for managing container registries with Kubernetes</p> <p>CyVerse manages their own public/private Harbor server for their Discovery Environment. You can authenticate to it using your CyVerse user account.</p> <p>https://harbor.cyverse.org</p> <p>CyVerse featured Docker containers in the Discovery Environment are cached on our Harbor, using a combination of GitHub Actions.</p> <p>https://github.com/cyverse-vice </p>"},{"location":"orchestration/terra/","title":"Introduction to Terraform","text":"<p> Terraform is an open-source infrastructure-as-code (IaC) software tool created by HashiCorp.</p> What is Infrastructure-as-Code (IaC)? <p>\"Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.\" -  Wikipedia</p> <p>IaC tools allow you to manage infrastructure with configuration files rather than through a graphical user interface. IaC allows you to build, change, and manage your infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share. --  Terraform Documentation</p>"},{"location":"orchestration/terra/#overview","title":"Overview","text":"<p>This basic tutorial will guide you through setting up a Terraform project and deploying virtual machines (VMs) as infrastructure on OpenStack Cloud.  </p> <p>Goals</p> <p> Understand orchestration for deployment to OpenStack cloud (Jetstream2)</p> <p> Understand the benefits of Terraform</p> <p> Ability to perform basic deployments on OpenStack using Terraform</p> <p> Ability to perform provisioning of deployed OpenStack resources through Terraform</p> Things we won't cover <p> OpenStack API</p> <p> All of Terraform's features</p>"},{"location":"orchestration/terra/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Basic understanding of  OpenStack and VMs</p> </li> <li> <p>Access to an OpenStack cloud (we will use Jetstream2)</p> </li> <li> <p> Terraform installed on your local machine</p> </li> <li> <p>Optional: request your own allocation to Jetstream2 on NSF ACCESS-CI</p> </li> </ul> <p>Outcomes</p> <p>By the end of this tutorial, you will </p> <p> have created SSH keypair</p> <p> generated an <code>*-openrc.sh</code> file from OpenStack</p> <p> started, stopped, and destroyed a Terraform deployment on an OpenStack Cloud</p> Terminology <p> Ansible - is a suite of software tools that enables infrastructure as code</p> <p> Deploy - to create a cloud resource or software</p> <p> Infrastructure - is the collection of hardware and software elements such as computing power, networking, storage, and virtualization resources needed to enable cloud computing</p> <p> Orchestration - is the automated configuring, coordinating, and managing of computer systems and software</p> <p> Playbook - are a list of tasks that automatically execute against a host</p> <p> Provision - making changes to a VM including updating the operating system, installing software, adding configurations</p> <p> Terraform - is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently</p>"},{"location":"orchestration/terra/#getting-onto-openstack-cloud","title":"Getting onto  OpenStack Cloud","text":"What is OpenStack? <p> OpenStack is an open source cloud computing infrastructure software project and is one of the three most active open source projects in the world.</p> <p>OpenStack clouds are managed by individuals and institutions on their own bare-metal hardware. </p> <p> </p> <p>If you do not have an account, go to https://allocations.access-ci.org/ and begin the process by requesting an \"Explore\" start-up allocation. </p> <p>ACCESS is the NSF's management layer for their research computing network (formerly called TerraGrid and XSEDE) which includes high performance computing, high throughput computing, and research clouds like Jetstream2. </p> <p> </p> <p>Jetstream2 is a public research cloud which uses OpenStack as its management layer. </p> <p>CyVerse is developing a User Interface for Jetstream2 called CACAO (Cloud Automation &amp; Continuous Analysis Orchestration). Beneath its hood is Terraform. CACAO can also be used from the CLI (which we will show in a later lesson).</p>"},{"location":"orchestration/terra/#terraform-installation","title":"Terraform installation","text":"<p> Official Documentation</p> Windows Installation <p>Download Terraform using the appropriate distribution for your OS</p> Mac OS X Installation <p>Instructions for Mac OS X installation</p> <p>If you're on OS X, you can use brew to install with the following commands:</p> <pre><code># install terraform -- taken from https://learn.hashicorp.com/tutorials/terraform/install-cli\nbrew tap hashicorp/tap &amp;&amp; brew install hashicorp/tap/terraform\n\n# install ansible and jq (for processing terraform's statefile into an ansible inventory)\nbrew install ansible jq\n</code></pre> Linux Installation <p>Instructions for Ubuntu 22.04 installation</p> <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install terraform\n</code></pre> <p>Install Ansible &amp; J Query</p> <pre><code>sudo apt-add-repository ppa:ansible/ansible\nsudo apt update &amp; sudo apt install -y ansible jq\n</code></pre> <p>Confirm installation:</p> <pre><code>terraform\n</code></pre> <p>Should output:</p> <pre><code>Usage: terraform [global options] &lt;subcommand&gt; [args]\n\nThe available commands for execution are listed below.\nThe primary workflow commands are given first, followed by\nless common or more advanced commands.\n\nMain commands:\n  init          Prepare your working directory for other commands\n  validate      Check whether the configuration is valid\n  plan          Show changes required by the current configuration\n  apply         Create or update infrastructure\n  destroy       Destroy previously-created infrastructure\n\nAll other commands:\n  console       Try Terraform expressions at an interactive command prompt\n  fmt           Reformat your configuration in the standard style\n  force-unlock  Release a stuck lock on the current workspace\n  get           Install or upgrade remote Terraform modules\n  graph         Generate a Graphviz graph of the steps in an operation\n  import        Associate existing infrastructure with a Terraform resource\n  login         Obtain and save credentials for a remote host\n  logout        Remove locally-stored credentials for a remote host\n  metadata      Metadata related commands\n  output        Show output values from your root module\n  providers     Show the providers required for this configuration\n  refresh       Update the state to match remote systems\n  show          Show the current state or a saved plan\n  state         Advanced state management\n  taint         Mark a resource instance as not fully functional\n  test          Experimental support for module integration testing\n  untaint       Remove the 'tainted' state from a resource instance\n  version       Show the current Terraform version\n  workspace     Workspace management\n\nGlobal options (use these before the subcommand, if any):\n  -chdir=DIR    Switch to a different working directory before executing the\n                given subcommand.\n  -help         Show this help output, or the help for a specified subcommand.\n  -version      An alias for the \"version\" subcommand.\n</code></pre>"},{"location":"orchestration/terra/#generate-an-openstack-credential-for-terraform","title":"Generate an OpenStack Credential for Terraform","text":"<p>Log into OpenStack's Horizon Interface</p> <p>Step 1 Log into OpenStack's Horizon Interface and create application credentials</p> <p>Generate an <code>openrc.sh</code> file in Jetstream2 Horizon Interface (https://js2.jetstream-cloud.org), </p> <p>Select the \"Identity\" then \"Application Credentials\" option in the menu (left side)</p> <p>Select \"+ Create Application Credential\" button on right</p> <p></p> <p>Give your new credentials a name and description, leave most of the fields blank</p> <p></p> <p>Download the new crededential <code>openrc.sh</code> file to your local</p> <p>Important</p> <p>Do not close the Application Credentials window without copying the <code>secret</code> or downloading the <code>openrc.sh</code> file.</p> <p></p>"},{"location":"orchestration/terra/#create-an-ssh-keypair-with-openstack","title":"Create an SSH keypair with OpenStack","text":"Creating a SSH key <p>To create an SSH key on an Ubuntu 22.04 terminal, you can follow these steps:</p> <p>Step 1: Open your terminal and type the following command to generate a new SSH key pair:</p> <pre><code>ssh-keygen -t rsa -b 4096\n</code></pre> <p>Step 2: When prompted, press \"Enter\" to save the key in the default location, or enter a different filename and location to save the key.</p> <p>Enter a passphrase to secure your key. This passphrase will be required to use the key later.</p> <p>Once the key is generated, you can add it to your SSH agent by running the following command:</p> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n</code></pre> <p>Step 3: Copy the public key to your remote server by running the following command, replacing \"user\" and \"server\" with your username and server address:</p> <pre><code>ssh-copy-id &lt;user&gt;@&lt;server-ip&gt;\n</code></pre> <p>create_ssh_script.sh:</p> <pre><code>#!/bin/bash\n\necho \"Generating SSH key pair...\"\nssh-keygen -t rsa -b 4096\n\necho \"Adding key to SSH agent...\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n\nread -p \"Enter your remote server username: \" username\nread -p \"Enter your remote server IP address: \" server\n\necho \"Ready to copy your new public key to a remote server:\"\n\nssh-copy-id username@server\n\necho \"SSH key setup complete!\"\n</code></pre> <p>Save the script to a file, make it executable with the following command:</p> <pre><code>chmod +x create_ssh_script.sh\n</code></pre> <p>run it with the following command:</p> <pre><code>./create_ssh_script.sh\n</code></pre> <p>Check to make sure you have a public key in the <code>~/.ssh/</code> directory, it should have the extension <code>.pub</code></p> <pre><code>ls ~/.ssh/\n</code></pre> <p>Create the keypair to OpenStack</p> <pre><code>openstack keypair create --public-key ~/.ssh/id_rsa.pub tswetnam-terraform-key\n</code></pre> <p>You can now check in OpenStack for the new keypair here: https://js2.jetstream-cloud.org/project/key_pairs</p>"},{"location":"orchestration/terra/#initialize-your-terraform-project","title":"Initialize your Terraform project","text":"<p>Create a new project folder for our configuration files.</p> <pre><code>mkdir ~/terraform\n</code></pre> <p>Copy the <code>openrc.sh</code> file you downloaded from OpenStack into the new folder.</p> <pre><code>cp *-openrc.sh ~/terraform/\n</code></pre> <p>Change Directory into your new <code>terraform/</code> folder and <code>source</code> the <code>openrc.sh</code> file to create its environmental variables locally.</p> <p>By sourcing this file, you avoid placing sensitive information about yourself into your code. </p> <pre><code>source *-openrc.sh\n</code></pre>"},{"location":"orchestration/terra/#configuration-files","title":"Configuration files","text":"<p>Terraform code is written in HCL (Hashicorp Configuration Language), and its configuration files typically end in the <code>.tf</code> file extension. </p> <p>Configuration <code>.tf</code> files can either be split into multiple files or maintained in a single file. </p> <p>When using multiple files, it is up to your discretion what the file names are, or how many you decide to split it into.</p>"},{"location":"orchestration/terra/#file-organization","title":"File Organization","text":"<p>An example for file organization of a terraform project might involve:</p> <pre><code>terraform-project/\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 variables.tf\n\u251c\u2500\u2500 outputs.tf\n\u251c\u2500\u2500 instances.tf\n\u251c\u2500\u2500 security.tf\n\u2514\u2500\u2500 modules/\n    \u251c\u2500\u2500 network/\n    \u2502   \u251c\u2500\u2500 main.tf\n    \u2502   \u251c\u2500\u2500 variables.tf\n    \u2502   \u2514\u2500\u2500 outputs.tf\n    \u2514\u2500\u2500 compute/\n        \u251c\u2500\u2500 main.tf\n        \u251c\u2500\u2500 variables.tf\n        \u2514\u2500\u2500 outputs.tf\n</code></pre> <p> Main Configuration File (<code>main.tf</code>): - contains the primary infrastructure resources and configurations for virtual machines, networks, and storage.</p> <p> Security File (<code>security.tf</code>): - contains optional security group details for instances.</p> <p> Variables File (<code>variables.tf</code>): - defines all the input variables used in the configuration. Declare variables with default values or leave them empty for required user input. Include descriptions for each variable to provide context.</p> <p> Instances File (<code>instances.tf</code>): - provisions the Instance flavor and IP networking of the VMs</p> <p> Outputs File (<code>outputs.tf</code>): - defines the outputs Terraform displays after applying the Main and Variables configuration. Includes: IP addresses, DNS names, or information for resources.</p>"},{"location":"orchestration/terra/#other-optional-files","title":"Other optional files","text":"<p> Provider Configuration File (<code>provider.tf</code>): - includes the provider(s) used in the configuration, such as OpenStack (on commerical cloud: Amazon Web Services (AWS), Azure, or Google Cloud Platform(GCP)) along with their authentication and regional settings.</p> <p> Modules and Reusable Configurations: - create separate <code>.tf</code> files for reusable modules and configurations. Reuse across multiple projects or within the same project on multiple VMs.</p>"},{"location":"orchestration/terra/#terraform-configuration-files","title":"Terraform configuration files.","text":"<p>Create the <code>main.tf</code> file in the <code>~/terraform/</code> directory</p>"},{"location":"orchestration/terra/#maintf","title":"<code>main.tf</code>","text":"<pre><code>terraform {\n  required_version = \"&gt;= 0.14.0\"\n  required_providers {\n    openstack = {\n      source = \"terraform-provider-openstack/openstack\"\n      version = \"&gt;=1.47.0\"\n    }\n  }\n}\n\nprovider \"openstack\" {\n  auth_url = \"https://js2.jetstream-cloud.org:5000/v3/\"\n  region = \"IU\"\n}\n</code></pre> <p>The <code>main.tf</code> file has just the basics for calling out to an OpenStack provider - we created the necessary configurations for this in the prior steps by sourcing the <code>*-openrc.sh</code> file and running <code>terraform init</code>. </p>"},{"location":"orchestration/terra/#variablestf","title":"<code>variables.tf</code>","text":"<p>Create <code>variables.tf</code>, it can also be called <code>inputs.tf</code> </p> <p>Here you need to go back to OpenStack and get a couple of additional variables:</p> <p><code>vm_number</code> - defines the number of VMs you wish to launch</p> <p><code>public_key</code> - you need the name of your paired SSH key that you generated in the prior step</p> <p><code>image_name</code> - select the name of a featured or unique image you wish to launch.</p> <pre><code>variable \"vm_number\" {\n  # creates a single VM\n  # replace with a larger number to launch more than one VM\n  default = \"1\"\n}\n\nvariable \"public_key\" {\n  # replace this with the name of the public ssh key you uploaded to Jetstream 2\n  # https://docs.jetstream-cloud.org/ui/cli/managing-ssh-keys/\n  default = \"tswetnam-terraform-key\"\n}\n\nvariable \"image_name\" {\n  # replace this with the image name of the ubuntu iso you want to use\n  # https://js2.jetstream-cloud.org/project/images \n  default = \"Featured-Ubuntu20\"\n}\n\nvariable \"network_name\" {\n  # replace this with the id of the public interface on JS2 in Project / Network / Networks / public\n  # https://js2.jetstream-cloud.org/project/networks/ \n  default = \"auto_allocated_network\"\n}\n</code></pre>"},{"location":"orchestration/terra/#securitytf","title":"<code>security.tf</code>","text":"<p>This file produces two distinct security groups, <code>terraform_ssh_ping</code> creates a group with <code>ssh</code> access and <code>ping</code>, <code>terraform_tcp_1</code> creates a group which opens the HTTP (80, 8080), HTTPS (443) ports for connecting a browser based service.</p> <pre><code>################\n#Security section\n################\n\n# Creating Compute Security group\nresource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" {\n  name = \"terraform_ssh_ping\"\n  description = \"Security group with SSH and PING open to 0.0.0.0/0\"\n\n  #ssh rule\n  rule{\n    ip_protocol = \"tcp\"\n    from_port  =  \"22\"\n    to_port    =  \"22\"\n    cidr       = \"0.0.0.0/0\"\n  }\n  rule {\n    from_port   = -1\n    to_port     = -1\n    ip_protocol = \"icmp\"\n    cidr        = \"0.0.0.0/0\"\n  }\n\n}\n\n# Create a Netowrking Security group\nresource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" {\n  name        = \"terraform_tcp_1\"\n  description = \"Security group with TCP open to 0.0.0.0/0\"\n}\n\n# Allow HTTP (port 80) traffic\nresource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" {\n  direction         = \"ingress\"\n  ethertype         = \"IPv4\"\n  protocol          = \"tcp\"\n  port_range_min    = 80\n  port_range_max    = 80\n  remote_ip_prefix  = \"0.0.0.0/0\"\n  security_group_id = \"${openstack_networking_secgroup_v2.terraform_tcp_1.id}\"\n}\n\n# Allow HTTPS (port 443) traffic\nresource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" {\n  direction         = \"ingress\"\n  ethertype         = \"IPv4\"\n  protocol          = \"tcp\"\n  port_range_min    = 443\n  port_range_max    = 443\n  remote_ip_prefix  = \"0.0.0.0/0\"\n  security_group_id = \"${openstack_networking_secgroup_v2.terraform_tcp_1.id}\"\n}\n\n# Allow Service (port 8080) traffic\nresource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" {\n  direction         = \"ingress\"\n  ethertype         = \"IPv4\"\n  protocol          = \"tcp\"\n  port_range_min    = 8080\n  port_range_max    = 8080\n  remote_ip_prefix  = \"0.0.0.0/0\"\n  security_group_id = \"${openstack_networking_secgroup_v2.terraform_tcp_1.id}\"\n}\n</code></pre>"},{"location":"orchestration/terra/#instancestf","title":"<code>instances.tf</code>","text":"<pre><code>################\n# Instance OS\n################\n\n# create each Ubuntu20 instance\nresource \"openstack_compute_instance_v2\" \"Ubuntu20\" {\n  name = \"container_camp_Ubuntu20_${count.index}\"\n  # ID of Featured-Ubuntu20\n  image_name  = var.image_name\n  # flavor_id is the size of the VM \n  # https://docs.jetstream-cloud.org/general/vmsizes/ \n  flavor_name  = \"m3.tiny\"\n  # this public key is set above in security section\n  key_pair  = var.public_key\n  security_groups   = [\"terraform_ssh_ping\", \"default\"]\n  count     = var.vm_number\n  metadata = {\n    terraform_controlled = \"yes\"\n  }\n  network {\n    name = var.network_name\n  }\n  #depends_on = [openstack_networking_network_v2.terraform_network]\n\n}\n# creating floating ips from the public ip pool\nresource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" {\n  pool = \"public\"\n    count     = var.vm_number\n}\n\n# assign floating ip to each Ubuntu20 VM\nresource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" {\n  floating_ip = \"${openstack_networking_floatingip_v2.terraform_floatip_ubuntu20[count.index].address}\"\n  instance_id = \"${openstack_compute_instance_v2.Ubuntu20[count.index].id}\"\n    count     = var.vm_number\n}\n</code></pre>"},{"location":"orchestration/terra/#outputtf","title":"<code>output.tf</code>","text":"<pre><code>################\n#Output\n################\n\noutput \"floating_ip_ubuntu20\" {\n  value = openstack_networking_floatingip_v2.terraform_floatip_ubuntu20.*.address\n  description = \"Public IP for Ubuntu 20\"\n}\n</code></pre>"},{"location":"orchestration/terra/#terraformtfvars","title":"<code>terraform.tfvars</code>","text":"<p>A <code>terraform.tfvars</code> file is used to define the values of input variables. It can also be renamed <code>*.auto.tfvars</code>.</p> <p>It serves as a convenient way to store and manage variable values that you don't want to hardcode in your <code>.tf</code> files or provide via command-line arguments. </p> <p>By using a <code>terraform.tfvars</code> file, you can easily customize and update the variable values for different environments or scenarios.</p> <p>The file should contain key-value pairs, where the key is the variable name and the value is the corresponding variable value. </p> <p>The syntax for defining variables in the <code>terraform.tfvars</code> file can be either HCL or JSON.</p> <p>Add your OpenStack credentials and other required information to <code>terraform.tfvars</code>:</p> <p>In HCL:</p> <pre><code>openstack_user_name   = \"your-openstack-username\"\nopenstack_password    = \"your-openstack-password\"\nopenstack_tenant_name = \"your-openstack-tenant-name\"\nopenstack_auth_url    = \"your-openstack-auth-url\"         \n</code></pre> <p>In JSON: <pre><code>{\n  \"openstack_user_name\": \"your-openstack-username\",\n  \"openstack_password\": \"your-openstack-password\",\n  \"openstack_tenant_name\": \"your-openstack-tenant-name\",\n  \"openstack_auth_url\": \"your-openstack-auth-url\"\n}         \n</code></pre></p> <p>When you run <code>terraform apply</code>, Terraform will automatically load the values from the <code>terraform.tfvars</code> file if it exists in the working directory. </p> <p>You can also create multiple <code>.tfvars</code> files and specify which one to use by passing the <code>-var-file</code> flag when executing Terraform commands:</p> <pre><code>terraform apply -var-file=\"custom.tfvars\"\n</code></pre> <p>Variables can also be passed directly into the terraform command-line using the <code>-var</code>. You can also combine the use of terraform.tfvars, *.auto.tfvars, and command line flags to set input variables; however, you should understand the rules for variable precedence before doing so.</p>"},{"location":"orchestration/terra/#intermediate-directories-and-files","title":"Intermediate directories and files","text":"<p>When <code>terraform apply</code> are executed, Terraform generates some new project files, notably</p> <p><code>.terraform/</code> - this directory will contain the <code>terraform-provider-openstack_version</code> and a <code>README.md</code>, <code>LICENCE</code>, and <code>CHANGELOG.md</code> </p> <p><code>terraform.lock.hcl</code> </p> <p><code>terraform.tfstate</code></p> <p><code>terraform.tfstate.backup</code></p>"},{"location":"orchestration/terra/#terraform-commands","title":"Terraform Commands","text":""},{"location":"orchestration/terra/#init","title":"<code>init</code>","text":"<p>Now, you are ready to initialize the Terraform project</p> <pre><code>cd ~/terraform\nterraform init\n</code></pre> Expected Response <pre><code>Initializing the backend...\n\nInitializing provider plugins...\n- Reusing previous version of terraform-provider-openstack/openstack from the dependency lock file\n- Using previously-installed terraform-provider-openstack/openstack v1.51.1\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n</code></pre>"},{"location":"orchestration/terra/#validate","title":"<code>validate</code>","text":"<p><code>validate</code> - scans your terraform directory and reports any syntax errors in your terraform</p> <pre><code>terraform validate\n</code></pre> Expected Response <pre><code>Success! The configuration is valid.\n</code></pre>"},{"location":"orchestration/terra/#apply","title":"apply","text":"<p>Using <code>terraform apply</code> will output the changes that will occur to your cloud. You can review the changes and decide to continue. Using the flag <code>-auto-approve</code> will also output the changes that will occur but will continue with the execution of the terraform as though you entered <code>yes</code>.</p> <pre><code>terraform apply\n</code></pre> Expected response <pre><code>Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0] will be created\n  + resource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" {\n      + floating_ip = (known after apply)\n      + id          = (known after apply)\n      + instance_id = (known after apply)\n      + region      = (known after apply)\n    }\n\n  # openstack_compute_instance_v2.Ubuntu20[0] will be created\n  + resource \"openstack_compute_instance_v2\" \"Ubuntu20\" {\n      + access_ip_v4        = (known after apply)\n      + access_ip_v6        = (known after apply)\n      + all_metadata        = (known after apply)\n      + all_tags            = (known after apply)\n      + availability_zone   = (known after apply)\n      + created             = (known after apply)\n      + flavor_id           = (known after apply)\n      + flavor_name         = \"m3.tiny\"\n      + force_delete        = false\n      + id                  = (known after apply)\n      + image_id            = (known after apply)\n      + image_name          = \"Featured-Ubuntu20\"\n      + key_pair            = \"tswetnam-terraform-key\"\n      + metadata            = {\n          + \"terraform_controlled\" = \"yes\"\n        }\n      + name                = \"container_camp_Ubuntu20_0\"\n      + power_state         = \"active\"\n      + region              = (known after apply)\n      + security_groups     = [\n          + \"default\",\n          + \"terraform_ssh_ping\",\n        ]\n      + stop_before_destroy = false\n      + updated             = (known after apply)\n\n      + network {\n          + access_network = false\n          + fixed_ip_v4    = (known after apply)\n          + fixed_ip_v6    = (known after apply)\n          + floating_ip    = (known after apply)\n          + mac            = (known after apply)\n          + name           = \"auto_allocated_network\"\n          + port           = (known after apply)\n          + uuid           = (known after apply)\n        }\n    }\n\n  # openstack_compute_secgroup_v2.terraform_ssh_ping will be created\n  + resource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" {\n      + description = \"Security group with SSH and PING open to 0.0.0.0/0\"\n      + id          = (known after apply)\n      + name        = \"terraform_ssh_ping\"\n      + region      = (known after apply)\n\n      + rule {\n          + cidr        = \"0.0.0.0/0\"\n          + from_port   = -1\n          + id          = (known after apply)\n          + ip_protocol = \"icmp\"\n          + self        = false\n          + to_port     = -1\n        }\n      + rule {\n          + cidr        = \"0.0.0.0/0\"\n          + from_port   = 22\n          + id          = (known after apply)\n          + ip_protocol = \"tcp\"\n          + self        = false\n          + to_port     = 22\n        }\n    }\n\n  # openstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0] will be created\n  + resource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" {\n      + address    = (known after apply)\n      + all_tags   = (known after apply)\n      + dns_domain = (known after apply)\n      + dns_name   = (known after apply)\n      + fixed_ip   = (known after apply)\n      + id         = (known after apply)\n      + pool       = \"public\"\n      + port_id    = (known after apply)\n      + region     = (known after apply)\n      + subnet_id  = (known after apply)\n      + tenant_id  = (known after apply)\n    }\n\n  # openstack_networking_secgroup_rule_v2.http_rule will be created\n  + resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" {\n      + direction         = \"ingress\"\n      + ethertype         = \"IPv4\"\n      + id                = (known after apply)\n      + port_range_max    = 80\n      + port_range_min    = 80\n      + protocol          = \"tcp\"\n      + region            = (known after apply)\n      + remote_group_id   = (known after apply)\n      + remote_ip_prefix  = \"0.0.0.0/0\"\n      + security_group_id = (known after apply)\n      + tenant_id         = (known after apply)\n    }\n\n  # openstack_networking_secgroup_rule_v2.https_rule will be created\n  + resource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" {\n      + direction         = \"ingress\"\n      + ethertype         = \"IPv4\"\n      + id                = (known after apply)\n      + port_range_max    = 443\n      + port_range_min    = 443\n      + protocol          = \"tcp\"\n      + region            = (known after apply)\n      + remote_group_id   = (known after apply)\n      + remote_ip_prefix  = \"0.0.0.0/0\"\n      + security_group_id = (known after apply)\n      + tenant_id         = (known after apply)\n    }\n\n  # openstack_networking_secgroup_rule_v2.service_rule will be created\n  + resource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" {\n      + direction         = \"ingress\"\n      + ethertype         = \"IPv4\"\n      + id                = (known after apply)\n      + port_range_max    = 8080\n      + port_range_min    = 8080\n      + protocol          = \"tcp\"\n      + region            = (known after apply)\n      + remote_group_id   = (known after apply)\n      + remote_ip_prefix  = \"0.0.0.0/0\"\n      + security_group_id = (known after apply)\n      + tenant_id         = (known after apply)\n    }\n\n  # openstack_networking_secgroup_v2.terraform_tcp_1 will be created\n  + resource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" {\n      + all_tags    = (known after apply)\n      + description = \"Security group with TCP open to 0.0.0.0/0\"\n      + id          = (known after apply)\n      + name        = \"terraform_tcp_1\"\n      + region      = (known after apply)\n      + tenant_id   = (known after apply)\n    }\n\nPlan: 8 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + floating_ip_ubuntu20 = [\n      + null,\n    ]\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n</code></pre> <p>You should be prompted</p> <pre><code> Do you want to perform these actions?\n      Terraform will perform the actions described above.\n      Only 'yes' will be accepted to approve.\n</code></pre> Expected Response after choosing 'yes'  <pre><code>openstack_networking_secgroup_v2.terraform_tcp_1: Creating...\nopenstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0]: Creating...\nopenstack_compute_secgroup_v2.terraform_ssh_ping: Creating...\nopenstack_compute_instance_v2.Ubuntu20[0]: Creating...\nopenstack_networking_secgroup_v2.terraform_tcp_1: Creation complete after 1s [id=4f0ab1d5-ca29-4f60-9a65-c38d2380719c]\nopenstack_networking_secgroup_rule_v2.service_rule: Creating...\nopenstack_networking_secgroup_rule_v2.https_rule: Creating...\nopenstack_networking_secgroup_rule_v2.http_rule: Creating...\nopenstack_networking_secgroup_rule_v2.http_rule: Creation complete after 1s [id=2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41]\nopenstack_networking_secgroup_rule_v2.service_rule: Creation complete after 2s [id=ead48696-4aae-436f-a12b-8bf3ceac253a]\nopenstack_networking_secgroup_rule_v2.https_rule: Creation complete after 2s [id=20225b99-fd2a-4a05-912c-61056fa21e3b]\nopenstack_compute_secgroup_v2.terraform_ssh_ping: Creation complete after 4s [id=1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f]\nopenstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0]: Creation complete after 7s [id=6af630bc-9c16-450c-b16f-0483294d8d75]\nopenstack_compute_instance_v2.Ubuntu20[0]: Still creating... [10s elapsed]\nopenstack_compute_instance_v2.Ubuntu20[0]: Creation complete after 15s [id=afc53214-d2bd-45d5-b8b5-b6886976df8c]\nopenstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0]: Creating...\nopenstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0]: Creation complete after 2s [id=149.165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/]\n\nApply complete! Resources: 8 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nfloating_ip_ubuntu20 = [\n  \"149.165.168.217\",\n]\n</code></pre> <p>Congratulations! You now have a running VM, you can check its status on Horizon, or try to connect to it over <code>ssh</code></p> <p>The <code>floating_ip_ubuntu20</code> should be the new VM's IP address (yours will be different than this example).</p> <p><code>ssh ubuntu@&lt;IP-ADDRESS&gt;</code> </p> <p>Make sure that </p>"},{"location":"orchestration/terra/#refresh","title":"<code>refresh</code>","text":"<p><code>refresh</code> - will update (refresh) the current terraform state</p> <p>This command is sometimes needed if haven't touched your terraform code in a while or there's a chance that your or other individuals manage your resources using horizon, exosphere, or the cli.</p> <pre><code>terraform refresh\n</code></pre> Expected Response <pre><code>\n</code></pre>"},{"location":"orchestration/terra/#show","title":"<code>show</code>","text":"<p><code>show</code> - will show the current terraform state, as stored in your state file</p> <p>This command is useful when you </p> <pre><code>terraform show\n</code></pre> Expected Response <pre><code>\n</code></pre>"},{"location":"orchestration/terra/#destroy","title":"destroy","text":"<p>When you are ready to complete the use of your VMs or your deployment, you can <code>destroy</code> the project.</p> <pre><code>terraform destroy\n</code></pre> Expected Response <pre><code>openstack_compute_secgroup_v2.terraform_ssh_ping: Refreshing state... [id=1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f]\nopenstack_compute_instance_v2.Ubuntu20[0]: Refreshing state... [id=afc53214-d2bd-45d5-b8b5-b6886976df8c]\nopenstack_networking_secgroup_v2.terraform_tcp_1: Refreshing state... [id=4f0ab1d5-ca29-4f60-9a65-c38d2380719c]\nopenstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0]: Refreshing state... [id=6af630bc-9c16-450c-b16f-0483294d8d75]\nopenstack_networking_secgroup_rule_v2.service_rule: Refreshing state... [id=ead48696-4aae-436f-a12b-8bf3ceac253a]\nopenstack_networking_secgroup_rule_v2.https_rule: Refreshing state... [id=20225b99-fd2a-4a05-912c-61056fa21e3b]\nopenstack_networking_secgroup_rule_v2.http_rule: Refreshing state... [id=2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41]\nopenstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0]: Refreshing state... [id=149.165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/]\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following\nsymbols:\n  - destroy\n\nTerraform will perform the following actions:\n\n  # openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0] will be destroyed\n  - resource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" {\n      - floating_ip = \"149.165.168.217\" -&gt; null\n      - id          = \"149.165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/\" -&gt; null\n      - instance_id = \"afc53214-d2bd-45d5-b8b5-b6886976df8c\" -&gt; null\n      - region      = \"IU\" -&gt; null\n    }\n\n  # openstack_compute_instance_v2.Ubuntu20[0] will be destroyed\n  - resource \"openstack_compute_instance_v2\" \"Ubuntu20\" {\n      - access_ip_v4        = \"10.0.24.205\" -&gt; null\n      - all_metadata        = {\n          - \"terraform_controlled\" = \"yes\"\n        } -&gt; null\n      - all_tags            = [] -&gt; null\n      - availability_zone   = \"nova\" -&gt; null\n      - created             = \"2023-03-23 23:09:42 +0000 UTC\" -&gt; null\n      - flavor_id           = \"1\" -&gt; null\n      - flavor_name         = \"m3.tiny\" -&gt; null\n      - force_delete        = false -&gt; null\n      - id                  = \"afc53214-d2bd-45d5-b8b5-b6886976df8c\" -&gt; null\n      - image_id            = \"27424df1-c3ea-4c4a-ad8b-6ea9a476f6f8\" -&gt; null\n      - image_name          = \"Featured-Ubuntu20\" -&gt; null\n      - key_pair            = \"tswetnam-terraform-key\" -&gt; null\n      - metadata            = {\n          - \"terraform_controlled\" = \"yes\"\n        } -&gt; null\n      - name                = \"container_camp_Ubuntu20_0\" -&gt; null\n      - power_state         = \"active\" -&gt; null\n      - region              = \"IU\" -&gt; null\n      - security_groups     = [\n          - \"default\",\n          - \"terraform_ssh_ping\",\n        ] -&gt; null\n      - stop_before_destroy = false -&gt; null\n      - tags                = [] -&gt; null\n      - updated             = \"2023-03-23 23:09:52 +0000 UTC\" -&gt; null\n\n      - network {\n          - access_network = false -&gt; null\n          - fixed_ip_v4    = \"10.0.24.205\" -&gt; null\n          - mac            = \"fa:16:3e:16:6e:8f\" -&gt; null\n          - name           = \"auto_allocated_network\" -&gt; null\n          - uuid           = \"e4ff98ff-d29a-48f4-a95b-88ebd6b0662f\" -&gt; null\n        }\n    }\n\n  # openstack_compute_secgroup_v2.terraform_ssh_ping will be destroyed\n  - resource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" {\n      - description = \"Security group with SSH and PING open to 0.0.0.0/0\" -&gt; null\n      - id          = \"1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f\" -&gt; null\n      - name        = \"terraform_ssh_ping\" -&gt; null\n      - region      = \"IU\" -&gt; null\n\n      - rule {\n          - cidr        = \"0.0.0.0/0\" -&gt; null\n          - from_port   = -1 -&gt; null\n          - id          = \"611e1c0a-edc1-462d-8e1a-5f8a72c4d968\" -&gt; null\n          - ip_protocol = \"icmp\" -&gt; null\n          - self        = false -&gt; null\n          - to_port     = -1 -&gt; null\n        }\n      - rule {\n          - cidr        = \"0.0.0.0/0\" -&gt; null\n          - from_port   = 22 -&gt; null\n          - id          = \"ce99a035-5df2-4131-ba90-268d045d0ff3\" -&gt; null\n          - ip_protocol = \"tcp\" -&gt; null\n          - self        = false -&gt; null\n          - to_port     = 22 -&gt; null\n        }\n    }\n\n  # openstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0] will be destroyed\n  - resource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" {\n      - address   = \"149.165.168.217\" -&gt; null\n      - all_tags  = [] -&gt; null\n      - fixed_ip  = \"10.0.24.205\" -&gt; null\n      - id        = \"6af630bc-9c16-450c-b16f-0483294d8d75\" -&gt; null\n      - pool      = \"public\" -&gt; null\n      - port_id   = \"71e9ae50-e281-4f98-afab-f1b6c1932806\" -&gt; null\n      - region    = \"IU\" -&gt; null\n      - tags      = [] -&gt; null\n      - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -&gt; null\n    }\n\n  # openstack_networking_secgroup_rule_v2.http_rule will be destroyed\n  - resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" {\n      - direction         = \"ingress\" -&gt; null\n      - ethertype         = \"IPv4\" -&gt; null\n      - id                = \"2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41\" -&gt; null\n      - port_range_max    = 80 -&gt; null\n      - port_range_min    = 80 -&gt; null\n      - protocol          = \"tcp\" -&gt; null\n      - region            = \"IU\" -&gt; null\n      - remote_ip_prefix  = \"0.0.0.0/0\" -&gt; null\n      - security_group_id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -&gt; null\n      - tenant_id         = \"db016a81886b4f918705e5dee2b24298\" -&gt; null\n    }\n\n  # openstack_networking_secgroup_rule_v2.https_rule will be destroyed\n  - resource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" {\n      - direction         = \"ingress\" -&gt; null\n      - ethertype         = \"IPv4\" -&gt; null\n      - id                = \"20225b99-fd2a-4a05-912c-61056fa21e3b\" -&gt; null\n      - port_range_max    = 443 -&gt; null\n      - port_range_min    = 443 -&gt; null\n      - protocol          = \"tcp\" -&gt; null\n      - region            = \"IU\" -&gt; null\n      - remote_ip_prefix  = \"0.0.0.0/0\" -&gt; null\n      - security_group_id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -&gt; null\n      - tenant_id         = \"db016a81886b4f918705e5dee2b24298\" -&gt; null\n    }\n\n  # openstack_networking_secgroup_rule_v2.service_rule will be destroyed\n  - resource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" {\n      - direction         = \"ingress\" -&gt; null\n      - ethertype         = \"IPv4\" -&gt; null\n      - id                = \"ead48696-4aae-436f-a12b-8bf3ceac253a\" -&gt; null\n      - port_range_max    = 8080 -&gt; null\n      - port_range_min    = 8080 -&gt; null\n      - protocol          = \"tcp\" -&gt; null\n      - region            = \"IU\" -&gt; null\n      - remote_ip_prefix  = \"0.0.0.0/0\" -&gt; null\n      - security_group_id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -&gt; null\n      - tenant_id         = \"db016a81886b4f918705e5dee2b24298\" -&gt; null\n    }\n\n  # openstack_networking_secgroup_v2.terraform_tcp_1 will be destroyed\n  - resource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" {\n      - all_tags    = [] -&gt; null\n      - description = \"Security group with TCP open to 0.0.0.0/0\" -&gt; null\n      - id          = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -&gt; null\n      - name        = \"terraform_tcp_1\" -&gt; null\n      - region      = \"IU\" -&gt; null\n      - tags        = [] -&gt; null\n      - tenant_id   = \"db016a81886b4f918705e5dee2b24298\" -&gt; null\n    }\n\nPlan: 0 to add, 0 to change, 8 to destroy.\n\nChanges to Outputs:\n  - floating_ip_ubuntu20 = [\n      - \"149.165.168.217\",\n    ] -&gt; null\n\nDo you really want to destroy all resources?\n  Terraform will destroy all your managed infrastructure, as shown above.\n  There is no undo. Only 'yes' will be accepted to confirm.\n\n  Enter a value: yes\n</code></pre> Expected Response after choosing 'yes'  <pre><code>openstack_networking_secgroup_rule_v2.service_rule: Destroying... [id=ead48696-4aae-436f-a12b-8bf3ceac253a]\nopenstack_networking_secgroup_rule_v2.https_rule: Destroying... [id=20225b99-fd2a-4a05-912c-61056fa21e3b]\nopenstack_compute_secgroup_v2.terraform_ssh_ping: Destroying... [id=1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f]\nopenstack_networking_secgroup_rule_v2.http_rule: Destroying... [id=2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41]\nopenstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0]: Destroying... [id=149.165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/]\nopenstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0]: Destruction complete after 3s\nopenstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0]: Destroying... [id=6af630bc-9c16-450c-b16f-0483294d8d75]\nopenstack_compute_instance_v2.Ubuntu20[0]: Destroying... [id=afc53214-d2bd-45d5-b8b5-b6886976df8c]\nopenstack_networking_secgroup_rule_v2.https_rule: Destruction complete after 6s\nopenstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0]: Destruction complete after 6s\nopenstack_networking_secgroup_rule_v2.service_rule: Still destroying... [id=ead48696-4aae-436f-a12b-8bf3ceac253a, 10s elapsed]\nopenstack_networking_secgroup_rule_v2.http_rule: Still destroying... [id=2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41, 10s elapsed]\nopenstack_networking_secgroup_rule_v2.service_rule: Destruction complete after 11s\nopenstack_compute_instance_v2.Ubuntu20[0]: Still destroying... [id=afc53214-d2bd-45d5-b8b5-b6886976df8c, 10s elapsed]\nopenstack_compute_instance_v2.Ubuntu20[0]: Destruction complete after 10s\nopenstack_networking_secgroup_rule_v2.http_rule: Destruction complete after 16s\nopenstack_networking_secgroup_v2.terraform_tcp_1: Destroying... [id=4f0ab1d5-ca29-4f60-9a65-c38d2380719c]\nopenstack_networking_secgroup_v2.terraform_tcp_1: Destruction complete after 9s\nopenstack_compute_secgroup_v2.terraform_ssh_ping: Destroying... [id=1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f]\nopenstack_compute_secgroup_v2.terraform_ssh_ping: Destruction complete after 2s\n\nDestroy complete! Resources: 8 destroyed.\n</code></pre>"},{"location":"orchestration/terra/#troubleshooting","title":"Troubleshooting","text":"My deployment cannot authenticate to the provider <p>Make sure that you run <code>source *-openrc.sh</code> to provide your OpenStack credentials. </p> <p>Also make sure that you are using the correct <code>~/.ssh/id_rsa.pub</code> key and that it has been injected to OpenStack</p> <code>terraform destroy</code> did not complete <p>there was an error like:</p> <pre><code>Error: Error deleting openstack_compute_secgroup_v2 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f: Bad request with: [DELETE https://js2.jetstream-cloud.org:8774/v2.1/os-security-groups/1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f], error message: {\"badRequest\": {\"code\": 400, \"message\": \"Security Group 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f in use.\\nNeutron server returns request_ids: ['req-e3d1548c-9d4c-4445-9642-952977a44853']\"}}\n</code></pre> <p>Try running <code>terraform destroy</code> one more time, occasionally Horizon times out while destroying deployments.</p>"},{"location":"resources/docker/","title":"Docker","text":""},{"location":"resources/docker/#useful-resources-related-to-docker","title":"Useful Resources related to Docker","text":"<p> Awesome Docker List</p> <p> Docker Labs</p> <p> Docker Community Forums</p> <p> Docker Hub</p> <p> Docker Official Documentation</p> <p> Docker on  StackOverflow</p> <p> Official  Twitter</p> <p> Play With Docker</p> <p> Docker Cloud</p>"},{"location":"resources/docker/#useful-resources-related-to-private-and-self-hosted-container-registries","title":"Useful Resources related to Private and Self-Hosted Container Registries","text":"<p> Harbor.io - Go Harbor Kubernetes Docker registry</p> <p>NVIDIA GPU Cloud</p> <p>BioContainers</p>"},{"location":"resources/docker/#useful-resources-related-to-docker-compose","title":"Useful Resources related to Docker-Compose","text":"<p> Awesome Docker Compose</p> <p> Official Documentation</p>"},{"location":"resources/k8s/","title":"Kubernetes","text":""},{"location":"resources/k8s/#useful-resources-related-to-kubernetes","title":"Useful resources related to  Kubernetes","text":"<p>Zero to JupyterHub with Kubernetes</p> <p> Nubenetes Awesome Kubernetes   &amp; Cloud - A curated list of awesome references collected since 2018. Microservices architectures rely on DevOps practices, automation, CI/CD (Continuous Integration &amp; Delivery), and API-focused designs.</p> <p> Golang awesome Kubernetes tools and resources</p> <p>Pacific Research Platform provides K8s through its Nautilus Clusters</p> <p>Admiralty - multi-cluster Kubernetes controller </p> <p>Rancher (K3s) Lightweight Kubernetes for Edge, IOT, CI deployments</p> <p>KEDA - K8s event driven autoscaler.</p>"},{"location":"resources/k8s/#useful-resources-related-to-automation","title":"Useful resources related to Automation","text":"<p>Terraform by Hashicorp</p> <p> Terraform Awesome List</p>"},{"location":"resources/post/","title":"General Notes and Discussions","text":"<p>This section summarizes general notes, discussions and questions raised during both the Basics and Advanced section of Container Camp.</p> <p>Before you Continue</p> <pre><code>This page is for archival and informative use only, reproduction (such as access to JetStream2 allocations, CodeSpaces) is limited.\n</code></pre>"},{"location":"resources/post/#basics","title":"Basics","text":""},{"location":"resources/post/#queries","title":"Queries","text":"<p>Q: What is an image?</p> <p>A: A file that lives in the cache on your computer... where 'cache' can be thought of like a desk. It's faster to retrieve a file from your desk than from the filing cabinet</p> <p>Q: What is a container?</p> <p>A: It's a virtualized run-time environment which starts from the image. A docker image is what you build. It has all of the software necessary to run the code. The Container is when you \"activate\" the image, an extra layer where you can work on top of the software you put in the image.</p> <p></p> <p>The built image will contain its own OS - it will make no difference where you build your container. When you build an image, you can specify the architecture of the machine you want it to run on.</p> <p>Manage resources for your container by using commands to stop, pause, restart, remove a container.</p> <p>Q: How do I work with data and containers?</p> <p>A: Containers do not contain large amounts of data, as these will take space in the writable layer of the container (see above image). Instead, it is suggested to use Volumes as a best practice. A Volume is a directory that lives outside of the container that can be attached to said container. Once attached, the contents of the directory will be viewable and accessible to the container. In order to attach the volume, one must specify the directory on the computer AND the destination folder, separated by a colon (:). The format is as follows <code>-v &lt;directory on computer&gt;:&lt;directory in container&gt;</code>.</p> <p>Q: Ports. What are ports and why do we need them?</p> <p>A: Ports are where network connections start and end. These are not physical, and these allow software to connect to a network. Our computers run hundreds of processes, and chance is a lot of these processes require a connection to the network. In order for these processes to access the network, ports are required. A single process is assigned a single port - and this is how these software can connect to the internet. The reason why we need to open a port for Docker, is because the Docker container is trying to communicate with the network, however it requires us, the user, to assign a port for it. Without us assigning the port to the Docker container, the connection to the network cannot happen. More information on ports can be found at: - CloudFlare: Understanding Ports - Internet Assigned Numbers Authority Port List - List of TCP and UDP port numbers</p> <p>Q: What is Docker's relationship with things like networking, ethernet, USB, HDMI? Are these things naturally closed or naturally open? Are there interfaces that you cannot access from docker?</p> <p>A: Docker is able to do Networking as it has its own networking subsystem (and commands). As this is an advanced topic, let me direct you to the official networking documentation here: https://docs.docker.com/network/</p> <p>Q: Is there a way to emulate a display in Docker so that certain rendering code (like the plotting libraries in python) don't break when run in a container?</p> <p>A: [unsure if this is what you were looking for] Docker is able to run GUI applications; A display can be specified using the <code>-e</code> (environment) flag such as <code>-e DISPLAY=$DISPLAY</code>. <code>$DISPLAY</code> can usually be specified to <code>$0</code>, targeting the primary display. This little blog post may be able to help you further.</p> <p>Q: What should we know about accessing GPUs from docker? Won't the hardware you're running on affect the runnability of a container, despite the containerization of the image?</p> <p>A: NVIDIA Docker now uses a special flag for Docker (rather than needing its own installation) https://github.com/NVIDIA/nvidia-docker.</p> <p></p> <p>Q: Is malware ever a problem with Dockerfiles? Can you run a malicious image?</p> <p>A: It seems that Docker (and Kubernetes) related malware are now a thing. From personal experience, I have never run into issues.</p> <p>Q: If containers are software, why should I bother using a container instead of the software itself?</p> <p>A: Containers offer 3 great solutions to common problems: (1) reproducibility (2) version control. Docker images contain all of the required software in the form of layers, including specific versions of libraries. This allows to easily share your image and software without worring about collaborators having to install the correct software and version. (3) portability, so you can run it anywhere.</p>"},{"location":"resources/post/#dockerfiles-hands-on","title":"Dockerfiles Hands-on","text":"<p>Go to an example directory in the <code>intro2docker</code> repository with a <code>Dockerfile</code> </p> <pre><code>cd alpine\n</code></pre> <p>Build the container using the <code>build</code> command. Make sure to include the <code>.</code></p> <pre><code>docker build -t test/alpine .\n</code></pre> <p>Note</p> <pre><code>The container should get the default `latest` tag if it is not specified in the `docker build` command with the name `test/alpine`\n</code></pre> <p>Start the container using the <code>run</code> command.</p> <pre><code>docker run --rm test/alpine:latest\n</code></pre> <p>To run the container and override its CMD, it will use its own shell <code>sh</code>:</p> <pre><code>docker run -it --rm test/alpine:latest sh\n</code></pre> <p>Dockerfiles are like your recipie book, and like every recipie book you have instructions. The instructions aren't for the user, but for Docker itself. These instruction are the capitalized commands you see at the beginning of lines, and these tell Docker what to do: </p> Instruction Command FROM Instructs to use a specific Docker image LABEL Adds metadata to the image RUN Executes a specific command ENV Sets environmental variables COPY Copies a file from a specified location to the image CMD Sets a command to be executed when running a container ENTRYPOINT Configures and run a container as an executable USER Used to set User specific information EXPOSE exposes a specific port <p>*the above list is nonexhaustive, visit the official Docker documentation for more information and further instructions.</p>"},{"location":"resources/post/#pushing-to-dockerhub","title":"Pushing to DockerHub","text":"<p>Build your docker image with </p> <pre><code>docker build -t &lt;Dockerhub username&gt;/&lt;Docker image&gt;:&lt;version&gt; .\n</code></pre> <p>then, log in to Docker with </p> <pre><code>docker login -u &lt;username&gt;\n</code></pre> <p>This will then ask for your Password; type in your password (it will NOT show you the password).</p> <p>If it does not login automatically, please follow the instructions here.</p> <p>Once you have logged in, push your docker to the DockerHub registry with</p> <pre><code>docker push &lt;Dockerhub username&gt;/&lt;Docker image&gt;:&lt;version&gt;\n</code></pre> <p>Your newly built Docker image now lives on DockerHub. You can view it at <code>https://hub.docker.com/r/&lt;username&gt;/&lt;Docker image&gt;</code></p>"},{"location":"resources/post/#assigning-users","title":"Assigning Users","text":"<p>Create a new folder called <code>ubuntu</code></p> <pre><code>mkdir ubuntu\n</code></pre> <p>Change into the folder</p> <pre><code>cd ubuntu\n</code></pre> <p>Create a <code>Dockerfile</code></p> <pre><code>ARG VERSION=18.04\n\nFROM ubuntu:$VERSION\n\nRUN apt-get update -y &amp;&amp; apt-get install -y gnupg wget python3 python3-distro &amp;&amp; \\\n    wget -qO - https://packages.irods.org/irods-signing-key.asc | apt-key add - &amp;&amp; \\\n    echo \"deb [arch=amd64] https://packages.irods.org/apt/ $(lsb_release -sc) main\" &gt;&gt; /etc/apt/sources.list.d/renci-irods.list &amp;&amp; \\\n    apt-get update &amp;&amp; apt-get install irods-icommands -y\n\nCOPY irods_environment.json /home/ubuntu/.irods/\n\nRUN useradd ubuntu &amp;&amp; \\\n    chown -R ubuntu:ubuntu /home/ubuntu\n\nUSER ubuntu\n</code></pre> <p>Create a file called <code>irods_environment.json</code> <pre><code>{\n    \"irods_host\": \"data.cyverse.org\", \n    \"irods_port\": 1247, \n    \"irods_zone_name\": \"iplant\"\n}\n</code></pre></p> <p>Build the container using your dockerhub username</p> <pre><code>docker build -t &lt;yourusername&gt;/ubuntu-irods:18.04 .\n</code></pre> <p>Run with  <pre><code>docker run -it --rm &lt;yourusername&gt;/ubuntu-irods:18.04\n</code></pre> Q: What did we do?</p> <p>A: We created an image whose the user is specified.</p> <p>Q: Why? </p> <p>A: When creating interactive containers, these containers are not built with root privileges. Assigning a specific user helps with defining the priviledges you want users to have.</p> <p>Q: Wait, what?</p> <p>A: When pulling a base image with the <code>FROM</code> instruction, sometimes the user is already defined. The only user with priviledges will be that already defined user. Therefore, in order to have the \"right\" priviledges, you have to assign the right user in your <code>Dockerfile</code>.</p>"},{"location":"resources/post/#rstudio-dockerfile","title":"RStudio Dockerfile","text":"<p>The above steps where necessary in order to understand why in this following step we need to define a user.</p> <p>Navigate to <code>rstudio/verse</code> with</p> <pre><code>cd rstudio/verse\n</code></pre> <p>and create a Dockerfile:</p> <p><pre><code>FROM rocker/verse:4.2.0\n\n# Install your own stuff below\nRUN install2.r --error \\    \n    # Added Packages\n    PerformanceAnalytics \\\n    boot \\\n    devtools \\\n    dlm \\\n    dplyr \\\n    foreign \\\n    lubridate \\\n    plotly \\\n    truncreg \\\n    ggridges \n</code></pre> Build the Docker image with:  <pre><code>docker build -t &lt;yourusername&gt;/rstudio:tag .\n</code></pre> Execute with</p> <pre><code>docker run --rm -p 8787:8787 -e DISABLE_AUTH=true &lt;username&gt;/rstudio:&lt;version&gt;\n</code></pre>"},{"location":"resources/post/#docker-commands-cheat-sheets","title":"Docker Commands Cheat Sheets:","text":"<ul> <li>https://cyverse-learning-materials.github.io/container-camp/docker/intro/#docker-commands</li> <li>https://dockerlabs.collabnix.com/docker/cheatsheet/</li> <li>https://github.com/wsargent/docker-cheat-sheet</li> <li>https://spacelift.io/blog/docker-commands-cheat-sheet</li> </ul>"},{"location":"resources/post/#advanced","title":"Advanced","text":""},{"location":"resources/post/#github-actions","title":"Github Actions:","text":"<ul> <li>GitHub not only can host your code, but it can be used to create Docker images right within the GitHub repo using GitHub Actions.</li> <li>You first require to connect your DockerHub and your GitHub accounts:<ol> <li>Navigate to Settings (in your repository)</li> <li>Select Secrets &gt; Actions.</li> <li>Add a New Repository Secret and name it \"DOCKER_HUB_USERNAME\"; Paste here your Docker Account name.</li> <li>Add a New Repository Secret and name it \"DOCKER_HUB_ACCESS_TOKEN\", and DO NOT CLOSE IT</li> <li>Navigate to DockerHub and click Account Settings &gt; Security &gt; New Access Token</li> <li>Add a name you can recognize and COPY the password and add it to \"DOCKER_HUB_ACCESS_TOKEN\" (step 4). Now save and close.</li> </ol> </li> </ul> <p>Your GitHub and DockerHub are connected!</p> <ul> <li>To create a new GitHub Action, navigate to the Action button on the top of your repository (create a repository first!), and click on New Workflow; select Docker Image.</li> <li>Delete the template pase the following:</li> </ul> <p><pre><code>name: &lt;NAME OF ACTION&gt;\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n      -\n        name: Checkout \n        uses: actions/checkout@v2\n      -\n        name: Login to Docker Hub\n        uses: docker/login-action@v1\n        with:\n          username: ${{ secrets.DOCKER_HUB_USERNAME }}\n          password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v2\n        with:\n          context: .\n          file: ./Dockerfile\n          push: true\n          tags: ${{ secrets.DOCKER_HUB_USERNAME }}/&lt;NAME OF IMAGE ON DOCKERHUB&gt;:&lt;VERSION&gt;\n</code></pre> - For the above code, modify all content in \" as you deem necessary.</p> <p>Remember to add a <code>Dockerfile</code> to your main repository!</p>"},{"location":"resources/post/#working-on-the-cloud","title":"Working on the Cloud","text":"<p>We discussed working with JetStream2. - Access JetStream2 exoshpere - Access JetStream2 CyVerse (CACAO)</p> <p>Summary of running JS2 GPU Virtual Machines (VM):</p> <ol> <li>Navigate to : https://jetstream2.exosphere.app/</li> <li>Click Create &gt; Instance</li> <li>Select Ubuntu 20.04</li> <li>Select a <code>g3.small</code> Instance and select (enable web desktop), click create.</li> </ol> <p>Wait for VM to spin up, test for functionality by: 1. click on web shell 2. From web shell, run <code>nvidia-smi</code> (if it returns a table, you are good to go)</p> <p>Running xpra on JS2:</p> <pre><code>$ export DISPLAY=:0\n$ xinit &amp;\n$ docker run --gpus all --rm -it -p 9876:9876 -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY -e XAUTHORITY -e QT_X11_NO_MITSHM=1 -e NVIDIA_DRIVER_CAPABILITIES=all harbor.cyverse.org/vice/xpra/cudagl:20.04\n</code></pre> <p>Running jupyter-pytorch on JS2: <pre><code>$ docker run --gpus all -p 8888:8888 -e REDIRECT_URL=http://localhost:8888 harbor.cyverse.org/vice/jupyter/pytorch:latest\n</code></pre></p>"},{"location":"resources/post/#adding-tools-to-cyverse","title":"Adding tools to CyVerse","text":"<p>Running an app in CyVerse is a 2 step process: 1. Adding a Tool 2. Using the Tool for the App</p>"},{"location":"resources/post/#adding-the-tool","title":"Adding the Tool:","text":"<ol> <li>Navigate to Apps &gt; Tools &gt; More Actions &gt; Add Tools</li> <li>Add Name, Version (for CyVerse, first block)</li> <li>Either paste your docker image DockerHub URL or the <code>&lt;repo&gt;/&lt;image&gt;</code>; add the <code>&lt;tag&gt;</code> in the Tag field.</li> <li>Specifically for RStudio, add Port 80 (due to reverse port).</li> </ol>"},{"location":"resources/post/#creating-the-app","title":"Creating the App:","text":"<ol> <li>Navigate to Apps, and select Create then Create New App</li> <li>Add a unique App Name and Description.</li> <li>Click on Select Tool and select the tool you just created, and select interactive.</li> <li>Click Next (and next, and next...), then click Save and Launch.</li> </ol> <p>Your app should be avaiable to access in a few minutes.</p>"},{"location":"resources/post/#singularity","title":"Singularity","text":"<p>A solution to run containers on a cluster/HPC.</p> <p>Q: Why can't I run docker on the HPC?</p> <p>A: Docker requires root proviledges (<code>sudo</code>), and these cannot be executed on the HPC. The HPC is a shared space, imagine everyone having root priviledges! All the possible unforseen changes to the system made by everyone will lead to a system error.</p> <p>Using the UArizona HPC as an example:</p> <ol> <li>with terminal, <code>ssh</code> into the HPC with <code>ssh &lt;/user&gt;@hpc.arizona.edu</code>, enter your password (and DUO auth).</li> <li>Choose your cluser node (Puma is suggested, accessed with <code>puma</code>), type <code>interactive</code> and wait for your interactive session to spin up.</li> </ol> <p>Follow lesson examples at TACC singularity basics.</p>"},{"location":"resources/post/#submitting-jobs-on-an-hpc-system-running-the-slurm-manager","title":"Submitting jobs on an HPC system running the SLURM manager","text":"<p>SLURM is a job scheduler/manager, and in order to run jobs on a cluster, you need to write a shell script (<code>.sh</code>) with a list of instructions.</p> <p>Q: Why can't I just run things on my node?</p> <p>A: As HPCs are shared machines, there are numbers of submitted concurrent jobs. Ultimately, the best way to run all of these jobs, is to schedule when to run all jobs submitted by users.</p> <p>Said script requires a list of SLURM instructions (\"batch commands\") prior to the actual job commands. Following is an example of said script:</p> <pre><code>#!/bin/bash\n\n#SBATCH -J myjob                         # Job name\n#SBATCH -o output.%j                     # Name of stdout output file (%j expands to jobId)\n#SBATCH -p development                   # Queue name\n#SBATCH -N 1                             # Total number of nodes requested (56 cores/node)\n#SBATCH -n 1                             # Total number of mpi tasks requested\n#SBATCH -t 02:00:00                      # Run time (hh:mm:ss) - 4 hours\n#SBATCH --reservation ContainerTraining  # a reservation only active during the training\n\nsingularity exec docker://python:latest /usr/local/bin/python\n</code></pre> <p>For more examples see the TACC documentation on batch commands.</p>"},{"location":"resources/post/#open-science-grid-distributed-high-throughput-computing","title":"Open Science Grid: Distributed High Throughput Computing","text":"<p>OSG: consortium of rsearchers and institutions who share compute and data resources for distributed HTC in support for open science.</p> <p>(OSG requires you to sign up through a meeting*)</p> <p>Using Docker images/containres offers consistent, complete and reproducible environment across the OSG: \"running containers on the OSG is like bringing your own utensils to someone else's kitchen\".</p> <p>OSG keeps dockers in extracted format in the cvmfs (official cvmfs documentation), making it \"snappier\" to load and run the image. IT stores each of the layers in an uncompressed format, thus you users only spin up the necessary layers. </p>"},{"location":"resources/post/#running-singularity","title":"Running Singularity","text":""},{"location":"resources/post/#installing-singularity","title":"Installing Singularity","text":"<p>Installing Singularity requires Go: see instructions in the Singularity documentation.</p> <p>In summary: <pre><code>$ wget https://go.dev/dl/go1.18.2.linux-amd64.tar.gz\n$ sudo tar -C /usr/local -xzvf go$VERSION.1.18.2-amd64.tar.gz\n$ echo 'export PATH=/usr/local/go/bin:$PATH' &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc\n</code></pre> After installing Go, you can install Singularity: <pre><code>$ export VERSION=3.9.5 &amp;&amp; # adjust this as necessary \\\n    wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz &amp;&amp; \\\n    tar -xzf singularity-ce-${VERSION}.tar.gz &amp;&amp; \\\n    cd singularity-ce-${VERSION}\n$ ./mconfig &amp;&amp; \\\n    make -C builddir &amp;&amp; \\\n    sudo make -C builddir install\n</code></pre></p>"},{"location":"resources/post/#running-docker-images-with-singularity","title":"Running Docker Images with Singularity","text":"<p>Singularity is able to create a <code>.sif</code> (singularity Image) from a Docker Image.</p> <p>To pull Docker Images with Singularity do <pre><code>$ singularity pull --name &lt;name&gt;.sif docker://&lt;docker_user&gt;/&lt;container&gt;:&lt;version&gt;\n</code></pre> To run a Docker Image with Singularity do <pre><code>$ singularity run docker://&lt;docker_user&gt;/&lt;container&gt;:&lt;version&gt;\n</code></pre></p> <p>Singularity also comes with an <code>exec</code> function, which executes a command within the container <pre><code>singularity exec --bind /data:/mnt &lt;my_container&gt;.sif ls /mnt\n</code></pre> In the above example, we see the <code>--bind</code> flag, which mounts a volume (the <code>/data</code> folder) to the container. The command also lists (<code>ls</code>) the content of the <code>data</code> folder.</p> <p>Reminder: it is important that you don't forget the version!!</p>"},{"location":"resources/post/#writing-the-singularity-recipie","title":"Writing the Singularity recipie","text":"<p>An example of a recipie is here below: <pre><code>Bootstrap: docker\nFrom: ubuntu:16.04\n%help\n    This is the container help section.\n\n%post\n    apt-get -y update\n    apt-get -y install fortune cowsay lolcat\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    fortune | cowsay | lolcat\n</code></pre> Singularity, like Docker, has a list of instructions. Here is the equivalent of Docker instructions:</p> Dockerfile FROM ARG COPY RUN ENV LABEL CMD Singularity <code>Bootstrap:</code> <code>%post</code> <code>%files</code> <code>%post</code> <code>%environment</code> <code>%label</code> <code>%runscript</code> <p>To build the image, do <pre><code>$ sudo singularity build &lt;name_of_container&gt;.sif &lt;singularity recipie&gt;\n</code></pre></p> <p>Notice how the above command uses <code>sudo</code>!</p> <p>Singularity has its own Hub (like DockerHub): https://cloud.sylabs.io/</p> <p>The SyLabs singularity allows to build singularity Images directly in the GUI online.</p>"},{"location":"resources/post/#kubernetes-k8s","title":"Kubernetes (K8s)","text":"<p>Q: What is Kubernentes?</p> <p>A: Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.</p> <p>Here is an image of how a K8s cluseter looks like:</p> <p></p> <p>Image: A control plane (could be made of a single machine) manages and distributes jobs to worker nodes.</p>"},{"location":"resources/post/#installing-k8s-on-a-unix-system","title":"Installing K8s (on a UNIX system)","text":"<p>Installation of K8s has a number of documentations, here are the basic instructions.</p> <pre><code>$ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\n$ chmod +x kubectl\n$ sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"resources/post/#initiating-k8s-head","title":"Initiating K8s head","text":"<p>K8s uses <code>kubectl</code> to control various pods.</p> <p>Prior to running <code>kubectl</code>, during the installation, a folder named <code>kube</code> was created in your <code>~/.</code> folder. There is an unpopulated <code>config</code> file in <code>~/.kube/</code> - for today's example, populate the <code>config</code> with</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: &lt;authority data&gt;\n    server: &lt;IP:port&gt;\n  name: default\ncontexts:\n- context:\n    cluster: default\n    user: default\n  name: default\ncurrent-context: default\nkind: Config\npreferences: {}\nusers:\n- name: default\n  user:\n    client-certificate-data: &lt;certificate data&gt;\n    client-key-data: &lt;key data&gt;\n</code></pre> <p>Warning</p> <pre><code>The above code has been modified `&lt;data&gt;` to protect information. The K8s head isn't online at the moment, and certificates change between config files.\n</code></pre>"},{"location":"resources/post/#interacting-with-pods-basically-a-container","title":"Interacting with Pods (basically a container)","text":"<p>To launch a pod you require a pod config file. An example is</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-&lt;change-this-to-your-username&gt;\nspec:\n  containers:\n  - name: mypod\n    image: alpine:3.14\n    resources:\n      limits:\n        memory: 100Mi\n        cpu: 100m\n      requests:\n        memory: 100Mi\n        cpu: 100m\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\n</code></pre> <p>To create your pod, do</p> <pre><code>kubetcl create -f pod1.yml\n</code></pre> <p>To initiate your pod, do</p> <pre><code>kubectl exec -it pod-&lt;change-this-to-your-username&gt; /bin/sh\n</code></pre> <p>The above command will enter the shell of the pod.</p> <p>You can see what pods are running with</p> <pre><code>kubectl get pods\n</code></pre> <p>To remove a pod, do</p> <pre><code>kubectl delete -f pod1.yml\n</code></pre> <p>These are the absolute basics of Kubernentes. To read more, refer to the Kubernetes official documentation (and other massive number of online resources, Google is your friend!).</p>"},{"location":"resources/post/#cacao-alpha-stage","title":"CACAO (alpha stage)","text":"<p>CACAO is \"An event-driven multi-cloud service that enables resarchers and educators to effortlessly manage, scale and share their tools and workflows to any research-capable cloud using declarative templates\" - Edwin Skidmore</p>"},{"location":"resources/post/#accessing-cacao-creating-an-ssh-key","title":"Accessing CACAO (creating an SSH key)","text":"<p>CACAO can be access at https://js2.cyverse.org/. You can access the CACAO deployments with an SSH key (or web browser). In order to create an ssh key, do</p> <p><pre><code>$ ssh-keygen -t rsa -b 8192 -f joking\n</code></pre> Creates a file public key (your keys are usually in <code>~/.ssh/</code>).</p> <p>To create a Kubernetes cluster, select \"+ Deployment\" and ensure that you're choosing Ubuntu 20 as your OS.</p> <p>To launch a multi VM running JupyterHub, choose \"+ Deployment\" and \"launch a multi-vm zero-to-jupyterhub\".</p> <p>Note: CACAO is still in Alpha phase. Read more on CACAO here. </p>"},{"location":"resources/singularity/","title":"Singularity","text":""},{"location":"resources/singularity/#useful-resources-related-to-singularity","title":"Useful Resources related to Singularity","text":"<p>SyLabs Singularity Community Edition (CE)</p> <p>Singularity-HPC</p> <p> Autamus Singularity Image Repository</p> <p> Carpentries Singularity Incubator</p>"},{"location":"resources/singularity/#campus-hpc-resources-for-singularity","title":"Campus HPC resources for Singularity","text":"<p>OpenScienceGrid Singularity Documentation</p> <p>TACC HPC Singularity Introduction</p> <p>TACC Containers .pdf</p> <p>NIH HPC</p> <p>University of Arizona Singularity Tutorial</p>"},{"location":"resources/vscode/","title":"VSCode","text":""},{"location":"resources/vscode/#setting-up-vscode-for-kubernetes-management","title":"Setting Up VSCode for Kubernetes Management","text":"<p>If you're diving into Kubernetes (K8s) for the first time, Visual Studio Code (VSCode) is an excellent platform to manage your pods and clusters. </p> <p>Why use VSCode for Kubernetes?</p> <p>VSCode is a developer-friendly Integrated Development Environment (IDE) that offers a plethora of features to streamline the software development process. With its vast marketplace, you can integrate various plugins and extensions, including those tailored for Kubernetes. This flexibility allows you to interact with Kubernetes clusters directly from the VSCode interface, making it a top choice for Kubernetes management.</p>"},{"location":"resources/vscode/#1-setting-up-your-environment","title":"1. Setting Up Your Environment:","text":"<ul> <li>Prerequisites: Ensure you have Docker and <code>kubectl</code> installed. You can verify their installation by running Docker and <code>kubectl</code> commands from the shell.</li> <li>Kubernetes Cluster: You can create a local Kubernetes cluster with <code>minikube</code> or use Azure Kubernetes Service (AKS) for an Azure-based cluster. For this tutorial, we'll focus on AKS. Ensure you have an Azure account ready.</li> </ul>"},{"location":"resources/vscode/#2-installing-the-kubernetes-extension-in-vscode","title":"2. Installing the Kubernetes Extension in VSCode:","text":"<ul> <li>Open VSCode and navigate to the Extensions view (Ctrl+Shift+X).</li> <li>Search for \"kubernetes\" and select the Microsoft Kubernetes extension to install.</li> </ul>"},{"location":"resources/vscode/#3-containerizing-and-publishing-your-application","title":"3. Containerizing and Publishing Your Application:","text":"<ul> <li>Use the Microsoft Docker Extension in VSCode to build your project, generate a Docker image, and push it to a container registry.</li> </ul>"},{"location":"resources/vscode/#4-creating-and-configuring-a-kubernetes-cluster","title":"4. Creating and Configuring a Kubernetes Cluster:","text":"<ul> <li>With the Kubernetes extension installed, you'll see KUBERNETES in the VSCode Explorer.</li> <li>Click on \"More\" and choose \"Create Cluster\".</li> <li>Follow the on-screen instructions to set up your Azure Kubernetes cluster.</li> </ul>"},{"location":"resources/vscode/#5-deploying-your-application","title":"5. Deploying Your Application:","text":"<ul> <li>The Kubernetes extension in VSCode provides autocompletion and code snippets for the Kubernetes manifest file.</li> <li>Once your manifest file is ready, open the Command Palette (Ctrl+Shift+P) and run \"Kubernetes: Create\". This will deploy your application to the Kubernetes cluster.</li> </ul>"},{"location":"resources/vscode/#6-monitoring-your-deployment","title":"6. Monitoring Your Deployment:","text":"<ul> <li>After deployment, use the Kubernetes extension to check the status of your application. Navigate to Workloads in the Explorer, right-click on Pods, and choose \"Get\" to see the application's status.</li> </ul>"},{"location":"resources/vscode/#7-additional-extensions-for-enhanced-kubernetes-development","title":"7. Additional Extensions for Enhanced Kubernetes Development:","text":"<ul> <li>YAML: Provides full YAML support in VSCode, including validation, error detection, and auto-completion.</li> <li>Cloud Code: Developed by Google Cloud, it supports Kubernetes and Cloud Run application development.</li> <li>Bridge to Kubernetes: Allows remote debugging and modification of Kubernetes applications.</li> <li>Azure Kubernetes Service: Manage Kubernetes clusters on Azure.</li> <li>OpenShift Connector: Simplifies building, deploying, and managing applications on Kubernetes or OpenShift.</li> </ul> <p>Note: With the right extensions and tools, VSCode is a powerful platform for managing K8s, making your DevOps tasks smoother and more efficient.</p>"},{"location":"singularity/advanced/","title":"Advanced use of Singularity","text":""},{"location":"singularity/advanced/#singularity-cli-continued","title":"Singularity CLI continued","text":""},{"location":"singularity/advanced/#build-singularity-sif-images","title":"Build Singularity <code>.sif</code> images","text":"<p>Similar to Docker which uses a <code>Dockerfile</code> to build its images, Singularity uses a file called <code>Singularity</code>.</p> <p>Important</p> <ul> <li>Singularity images use the <code>.sif</code> extension and appear as a single compressed file, versus Docker which uses many cached file layers to create a single image. </li> <li><code>.sif</code> files also use layers, but these are not apparent.</li> <li><code>.sif</code> images are cached in the folder where you build them, or designate them.</li> <li>When building from <code>docker://</code> the Docker image layers are downloaded and cached by Singularity in a <code>/.Singularity</code> folder on your build environment.</li> </ul>"},{"location":"singularity/advanced/#build","title":"build","text":""},{"location":"singularity/advanced/#create-a-sif-image-using-a-docker-image-as-the-template","title":"Create a <code>.sif</code> image using a Docker image as the template","text":"<p>As we've learned from the HPC and HTC groups, building Singularity images is not necessarily the most accessible and reproducible method for managing containers.</p> <p>Most groups suggest that you build your containers with Docker, and host them on a Docker Registry first.</p> <p>The market dominance of Docker and its wide acceptance as a container format, has led us to use Singularity with Docker in most cases.</p> <p>We've already covered how you can pull an existing image from Docker Hub, but we can also build a Singularity image from the Docker Hub using the build command:</p> <pre><code>$ sudo singularity build --sandbox ubuntu-latest/  docker://ubuntu\n</code></pre> <p>Test the new <code>.sif</code> image:</p> <pre><code>$ singularity shell --writable ubuntu-latest/\n\nSingularity ubuntu-latest.sif:~&gt; apt-get update\n</code></pre> <p>Does it work?</p> <pre><code>$ sudo singularity shell ubuntu-latest.sif\n\nSingularity: Invoking an interactive shell within container...\n\nSingularity ubuntu-latest.sif:~&gt; apt-get update\n</code></pre> <p>When I try to install software to the image without <code>sudo</code> it is denied, because <code>root</code> is the owner inside the container. When I use <code>sudo</code> I can install software into the container. The software remains in the sandbox container after closing the container and restart.</p> <p>In order to make these changes permanant, I need to rebuild the sandbox as a <code>.sif</code> image</p> <pre><code>$ sudo singularity build ubuntu-latest.sif ubuntu-latest/\n</code></pre>"},{"location":"singularity/advanced/#creating-singularity-sif-from-scratch","title":"Creating Singularity <code>.sif</code> from scratch","text":"<p>The contents of the <code>Singularity</code> file differ from <code>Dockerfile</code></p> <ul> <li><code>%help</code> - create text for a help menu associated with your container</li> <li><code>%setup</code> - executed on the host system outside of the container, after the base OS has been installed.</li> <li><code>%files</code> - copy files from your host system into the container</li> <li><code>%labels</code> - store metadata in the container</li> <li><code>%environment</code> - loads environment variables at the time the container is run (not built)</li> <li><code>%post</code> - set environment variables during the build</li> <li><code>%runscript</code> - executes a script when the container runs</li> <li><code>%test</code> - runs a test on the build of the container</li> </ul> Dockerfile FROM ARG COPY RUN ENV LABEL CMD Singularity <code>Bootstrap:</code> <code>%post</code> <code>%files</code> <code>%post</code> <code>%environment</code> <code>%label</code> <code>%runscript</code>"},{"location":"singularity/advanced/#writing-the-singularity-file","title":"Writing the <code>Singularity</code> file","text":"<p>SyLabs User-Guide</p> <p>A <code>Singularity</code> file can be hosted on GitHub and will be auto-detected by Singularity-Hub when you set up your container Collection.</p> <p>When you are building locally, you can name the <code>Singularity</code> file whatever you wish, but a better practice is to put it in a specified directory and name it <code>Singularity</code>. </p> <p>Building your own containers requires that you have sudo privileges - therefore you'll need to develop these on your local machine or on a VM that you can gain root access on.</p>"},{"location":"singularity/advanced/#header","title":"Header","text":"<p>The top of the file, selects the base OS for the container, just like <code>FROM</code> in Docker.</p> <p><code>Bootstrap:</code> references another registry (e.g. <code>docker</code> for DockerHub, <code>debootstrap</code>, or <code>library</code> for Sylabs Container Library).</p> <p><code>From:</code> selects the tag name.</p> <p>Using <code>debootstrap</code> with a build that uses a mirror:</p> <pre><code>BootStrap: debootstrap\nOSVersion: jammy\nMirrorURL: http://us.archive.ubuntu.com/ubuntu/\n</code></pre> <p>Using CentOS-like container:</p> <pre><code>Bootstrap: yum\nOSVersion: 7\nMirrorURL: http://mirror.centos.org/centos-7/7/os/x86_64/\nInclude:yum\n</code></pre> <p>Note: to use yum to build a container you should be operating on a RHEL system, or an Ubuntu system with yum installed.</p> <p>Using a localimage to build:</p> <pre><code>Bootstrap: localimage\nFrom: /path/to/container/file/or/directory\n</code></pre> <p>The container registries which Singularity uses are listed in the Introduction Section 3.1.</p> <ul> <li>The Singularity file uses sections to specify the dependencies,     environmental settings, and runscripts when it builds.</li> </ul>"},{"location":"singularity/advanced/#help","title":"help","text":"<p><code>%help</code> section can be as verbose as you want</p> <pre><code>Bootstrap: docker\nFrom: ubuntu\n\n%help\nThis is the container help section.\n</code></pre>"},{"location":"singularity/advanced/#setup","title":"setup","text":"<p><code>%setup</code> commands are executed on the localhost system outside of the container - these files could include necessary build dependencies. We can copy files to the <code>$SINGULARITY_ROOTFS</code> file system can be done during setup</p>"},{"location":"singularity/advanced/#files","title":"files","text":"<p><code>%files</code> include any files that you want to copy from your localhost into the container.</p>"},{"location":"singularity/advanced/#post","title":"post","text":"<p><code>%post</code> includes all of the environment variables and dependencies that you want to see installed into the container at build time.</p> <pre><code>%post\n    apt-get -y update\n    apt-get -y install fortune cowsay lolcat\n</code></pre>"},{"location":"singularity/advanced/#environment","title":"environment","text":"<p><code>%environment</code> includes the environment variables which we want to be run when we start the container</p> <pre><code>%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n</code></pre>"},{"location":"singularity/advanced/#runscript","title":"runscript","text":"<p><code>%runscript</code> does what it says, it executes a set of commands when the container is run.</p> <pre><code>%runscript\n    fortune | cowsay | lolcat\n</code></pre>"},{"location":"singularity/advanced/#labels","title":"labels","text":"<p><code>%labels</code> are used similar to Dockerfile which allow you to add metadata to the image file in key-value pairs</p> <pre><code>%labels\nAuthor Your Name\nEmail email@address.org\nVersion v2022\nCustomLabel statement here\n</code></pre> <p>Labels can be read using the <code>inspect</code> command</p>"},{"location":"singularity/advanced/#example-file","title":"Example File","text":"<p>Example <code>Singularity</code> file bootstrapping an Ubuntu (22.04) image.</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:22.04\n\n%post\n   apt-get -y update\n   apt-get -y install fortune cowsay lolcat\n\n%environment\n   export LC_ALL=C\n   export PATH=/usr/games:$PATH\n\n%runscript\n   fortune | cowsay | lolcat\n\n%labels\n   Maintainer Your Name\n   Version v2022\n</code></pre> <p>Build the container:</p> <pre><code>singularity build cowsay.sif Singularity\n</code></pre> <p>Run the container:</p> <pre><code>singularity run cowsay.sif\n</code></pre>"},{"location":"singularity/advanced/#sandbox","title":"Sandbox","text":"<p>Sandboxing is another approach to building up a container image.</p>"},{"location":"singularity/hpc/","title":"Singularity and High Performance Computing","text":"<p>High Performance Computing resources fill an important role in research computing and can support container execution through runtimes such as Singularity or, hopefully soon, rootless Docker, among other options.</p> <p>Conducting analyses on HPC clusters happens through different patterns of interaction than running analyses on a cloud VM. When you login, you are on a node that is shared with lots of people, typically called the \"login node\". Trying to run jobs on the login node is not \"high performance\" at all (and will likely get you an admonishing email from the system administrator). Login nodes are intended to be used for moving files, editing files, and launching jobs.</p> <p>Importantly, most jobs run on an HPC cluster are neither interactive, nor realtime. When you submit a job to the scheduler, you must tell it what resources you need (e.g. how many nodes, how much RAM, what type of nodes, and for how long) in addition to what you want to run. Then the scheduler finally has resources matching your requirements, it runs the job for you. If your request is very large, or very long, you may never make it out of the queue.</p> <p>For example, on a VM if you run the command:</p> <pre><code>singularity exec docker://python:latest /usr/local/bin/python\n</code></pre> <p>The container will immediately start.</p> <p>On an HPC system, your job submission script would look something like:</p> <pre><code>#!/bin/bash\n#\n#SBATCH -J myjob                      # Job name\n#SBATCH -o output.%j                  # Name of stdout output file (%j expands to jobId)\n#SBATCH -p development                # Queue name\n#SBATCH -N 1                          # Total number of nodes requested (68 cores/node)\n#SBATCH -n 17                         # Total number of mpi tasks requested\n#SBATCH -t 02:00:00                   # Run time (hh:mm:ss) - 4 hours\n\nmodule load singularity/3/3.1\nsingularity exec docker://python:latest /usr/local/bin/python\n</code></pre> <p>This example is for the Slurm scheduler. Each of the #SBATCH lines looks like a comment to the bash kernel, but the scheduler reads all those lines to know what resources to reserve for you.</p> <p>It is usually possible to get an interactive session as well, by using an interactive flag, -i.</p> <p>Warning</p> <pre><code>Every HPC cluster is a little different, but they almost universally\nhave a \"User's Guide\" that serves both as a quick reference for helpful\ncommands and contains guidelines for how to be a \"good citizen\" while\nusing the system. For TACC's Stampede2 system, see the [user guide](https://portal.tacc.utexas.edu/user-guides/stampede2). For The\nUniversity of Arizona, see the [user guide](https://docs.hpc.arizona.edu/).\n</code></pre>"},{"location":"singularity/hpc/#how-do-hpc-systems-fit-into-the-development-workflow","title":"How do HPC systems fit into the development workflow?","text":"<p>A few things to consider when using HPC systems:</p> <ol> <li>Using <code>sudo</code> is not allowed on HPC systems, and building a     Singularity container from scratch requires sudo. That means you     have to build your containers on a different development system. You     can pull a docker image on HPC systems</li> <li>If you need to edit text files, command line text editors don't     support using a mouse, so working efficiently has a learning curve.     There are text editors that support editing files over SSH. This     lets you use a local text editor and just save the changes to the     HPC system.</li> </ol> <p>These constraints make HPC systems perfectly suitable for execution environments, but currently a limiting choice for a development environment. We usually recommend your local laptop or a VM as a development environment where you can iterate on your code rapidly and test container building and execution.</p>"},{"location":"singularity/hpc/#singularity-and-mpi","title":"Singularity and MPI","text":"<p>Singularity supports MPI fairly well. Since (by default) the network is the same insde and outside the container, the communication between containers usually just works. The more complicated bit is making sure that the container has the right set of MPI libraries. MPI is an open specification, but there are several implementations (OpenMPI, MVAPICH2, and Intel MPI to name three) with some non-overlapping feature sets. If the host and container are running different MPI implementations, or even different versions of the same implementation, hilarity may ensue.</p> <p>The general rule is that you want the version of MPI inside the container to be the same version or newer than the host. You may be thinking that this is not good for the portability of your container, and you are right. Containerizing MPI applications is not terribly difficult with Singularity, but it comes at the cost of additional requirements for the host system.</p> <p>Warning</p> <pre><code>Many HPC Systems, like Stampede2 at TACC and Ocelote at UAHPC, have\nhigh-speed, low-latency networks that have special drivers. Infiniband,\nAres, and OmniPath are three different specs for these types of\nnetworks. When running MPI jobs, if the container doesn't have the right\nlibraries, it won't be able to use those special interconnects to\ncommunicate between nodes.\n</code></pre>"},{"location":"singularity/hpc/#base-docker-images","title":"Base Docker images","text":"<p>Depending on the system you will use, you may have to build your own MPI enabled Singularity images (to get the versions to match).</p> <p>When running at TACC, there is a set of curated Docker images for use in the FROM line of your own containers. You can see a list of availabe images at https://hub.docker.com/u/tacc</p> <p>Specifically, you can use the <code>tacc/tacc-ubuntu18-mvapich2.3-psm2</code> image to satisfy the MPI architecture and version requirements for running on Stampede2.</p> <p>Because you may have to build your own MPI enabled Singularity images (to get the versions to match), here is a 3.1 compatible example of what it may look like:</p> <p>BootStrap: debootstrap OSVersion: xenial MirrorURL: http://us.archive.ubuntu.com/ubuntu/</p> %runscript <p>echo \"This is what happens when you run the container...\"</p> %post <p>echo \"Hello from inside the container\" sed -i 's/$/ universe/' /etc/apt/sources.list apt update apt -y --allow-unauthenticated install vim build-essential wget gfortran bison libibverbs-dev libibmad-dev libibumad-dev librdmacm-dev libmlx5-dev libmlx4-dev wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.1.tar.gz tar xvf mvapich2-2.1.tar.gz cd mvapich2-2.1 ./configure --prefix=/usr/local make -j4 make install /usr/local/bin/mpicc examples/hellow.c -o /usr/bin/hellow</p> <p>You could also build in everything in a Dockerfile and convert the image to Singularity at the end.</p> <p>Once you have a working MPI container, invoking it would look something like:</p> <pre><code>mpirun -np 4 singularity exec ./mycontainer.sif /app.py arg1 arg2\n</code></pre> <p>This will use the host MPI libraries to run in parallel, and assuming the image has what it needs, can work across many nodes.</p> <p>For a single node, you can also use the container MPI to run in parallel (usually you don't want this)</p> <pre><code>singularity exec ./mycontainer.sif mpirun -np 4 /app.py arg1 arg2\n</code></pre>"},{"location":"singularity/hpc/#example-containerized-mpi-app","title":"Example Containerized MPI App","text":"<p>In your Docker development environment, make a new directory in which to build up a new image and download (or copy and paste) two files in that directory:</p> <p>https://raw.githubusercontent.com/TACC/containers_at_tacc/master/docs/scripts/Dockerfile.mpi</p> <p>https://raw.githubusercontent.com/TACC/containers_at_tacc/master/docs/scripts/pi-mpi.py</p> <p>Take a look at both files. <code>pi-mpi.py</code> is a simple MPI Python script that approximates pi (very inefficiently) by random sampling. <code>Dockerfile.mpi</code> is an updated Dockerfile that uses the TACC base image to satisfy all the MPI requirements on Stampede2.</p> <p>Next, try building the new container.</p> <pre><code>$ docker build -t USERNAME/pi-estimator:0.1-mpi -f Dockerfile.mpi .\n</code></pre> <p>Don't forget to change USERNAME to your DockerHub username.</p> <p>Once you have successfully built an image, push it up to DockerHub with the <code>docker push</code> command so that we can pull it back down on Stampede2.</p>"},{"location":"singularity/hpc/#running-an-mpi-container-on-stampede2","title":"Running an MPI Container on Stampede2","text":"<p>To test, we can grab an interactive session that has two nodes. That way we can see if we can make the two nodes work together. On TACC systems, the \"idev\" command will start an interactive session on a compute node:</p> <pre><code>$ idev -m 60 -p normal -N 2 -n 128\n</code></pre> <p>Once you have nodes at your disposal and a container on DockerHub, invoking it would look something like:</p> <pre><code>module load tacc-singularity\ncd $WORK\nsingularity pull docker://USERNAME/pi-estimator:0.1-mpi\ntime singularity exec pi-estimator_0.1-mpi.sif pi-mpi.py 10000000\ntime ibrun singularity exec pi-estimator_0.1-mpi.sif pi-mpi.py 10000000\n</code></pre> <p>Warning</p> <pre><code>TACC uses a command called `ibrun` on all of its systems that\nconfigures MPI to use the high-speed, low-latency network. If you are\nfamiliar with MPI, this is the functional equivalent to `mpirun`\n</code></pre> <p>The first <code>singularity exec pi-estimator_0.1-mpi.sif pi-mpi.py 10000000</code> command will use 1 CPU core to sample ten million times. The second command, using <code>ibrun</code> will run 128 processes that sample ten million times each and pass their results back to the \"rank 0\" MPI process to merge the results.</p> <p>This will use the host MPI libraries to run in parallel, and assuming the image has what it needs, can work across many nodes.</p> <p>As an aside, for a single node you can also use the container MPI to run in parallel (but usually you don't want this).</p> <p>When you are don with your interactive session, don't forget to <code>exit</code> to end the session and go back to the login node.</p>"},{"location":"singularity/hpc/#singularity-and-gpu-computing","title":"Singularity and GPU Computing","text":"<p>GPU support in Singularity is very good.</p> <p>Since Singularity supported docker containers, it has been fairly simple to utilize GPUs for machine learning code like TensorFlow. We will not do this as a hands-on exercise, but in general the procedule is as follows.</p> <pre><code># Load the singularity module\nmodule load singularity/3/3.1\n\n# Pull your image\nsingularity pull docker://nvidia/caffe:latest\n\nsingularity exec --nv caffe-latest.sif caffe device_query -gpu 0\n</code></pre> <p>Please note that the --nv flag specifically passes the GPU drivers into the container. If you leave it out, the GPU will not be detected.</p> <pre><code># this is missing the --nv flag and will not work\nsingularity exec caffe-latest.sif caffe device_query -gpu 0\n</code></pre> <p>The main requirement for GPU containers to work is that the version of the host drivers matches the major version of the library inside the container. So, for example, if CUDA 10 is on the host, the container needs to use CUDA 10 internally.</p> <p>For TensorFlow, you can directly pull their latest GPU image and utilize it as follows.</p> <pre><code># Change to your $WORK directory\ncd $WORK\n#Get the software\ngit clone https://github.com/tensorflow/models.git ~/models\n# Pull the image\nsingularity pull docker://tensorflow/tensorflow:latest-gpu\n# Run the code\nsingularity exec --nv tensorflow-latest-gpu.sif python $HOME/models/tutorials/image/mnist/convolutional.py\n</code></pre> <p>The University of Arizona HPS Singularity examples.</p>"},{"location":"singularity/intro/","title":"Introduction to Singularity","text":"<p>In this section we're going to be working with Singularity Community Edition (CE) </p> Wait, what is \"Apptainer\", and what is the difference between SingularityCE and Apptainer? <p>The Singularity project was split into multiple projects managed by different organizations since it was created in 2017.</p> <p>In a nutshell:</p> <ul> <li>Greg Kurtzer founded the Singularity project while at the Lawrence Berkeley National Laboratory</li> <li>Kurtzer created Sylabs, a private company, around Singularity </li> <li>Kurtzer left Sylabs to focus on CIQ, another private company, and moved Singularity to HPCng (a Community Org)</li> <li>Sylabs forked Singularity for control and professionially licensed support creating Singularity Community Edition\"</li> <li>HPCng gave the official project to Linux Foundation and renamed it \"Apptainer\"</li> <li>Apptainer is being marketed by CIQ</li> </ul> <p>At the present time, Apptainer and Singularity CE have highly similar syntax and will run Singularity <code>.sif</code> images interoperably</p> Docker vs SingularityCE &amp; Apptainer <p> Apptainer and SingularityCE are 100% compatible with Docker but they do have some distinct differences</p> <p> Docker</p> <p> Docker containers run as <code>root</code></p> <ul> <li>This privilege is almost never supported by administrators of High Performance Computing (HPC) centers. Meaning Docker is not, and will likely never be, installed natively on your HPC cluster.</li> </ul> <p> uses compressed layers to create one image</p> <p> SingularityCE &amp; Apptainer:</p> <p>  Same user and group identity inside as outside the container</p> <p>  User only has <code>root</code> privileges if elevated with <code>sudo</code> when the container is run</p> <p>  Can run and modify any existing Docker image</p> <ul> <li>These key differences allow Singularity to be installed on most HPC centers. Because you can run virtually all Docker containers in Singularity, you can effectively run Docker on an HPC. </li> </ul>"},{"location":"singularity/intro/#singularityce-installation","title":"SingularityCE Installation","text":"<p>Sylabs Singularity Community Edition (CE) homepage: https://www.sylabs.io/docs/</p> <p>Apptainer Linux Foundation homepage: https://apptainer.org/</p>"},{"location":"singularity/intro/#conda","title":"Conda","text":"<p>SingularityCE or Apptainer can both be installed from Conda:</p> <pre><code>conda install -c conda-forge singularityce\n</code></pre> <pre><code>conda install -c conda-forge apptainer\n</code></pre>"},{"location":"singularity/intro/#install-locally","title":"Install Locally","text":"<p>To Install Singularity follow the instructions for your specific OS: </p> <p> SingularityCE</p>"},{"location":"singularity/intro/#module-loading-on-hpc","title":"Module loading on HPC","text":"<p>If you are interested in working with SingularityCE on HPC, you may need to contact your systems administrator and request they install SingularityCE. Because SingularityCE ideally needs setuid, your admins may have some qualms about giving SingularityCE this privilege. If that is the case, you might consider forwarding this letter to your admins.</p> <p>Most HPC systems are running Environment Modules with the simple command <code>module</code>.</p> <p>You can check to see what is available:</p> <pre><code>$ module avail singularity\n</code></pre> <p>If Singularity is listed as being installed, load a specific version, e.g.:</p> <pre><code>$ module load singularity/3/3.9\n</code></pre>"},{"location":"singularity/intro/#install-in-codespaces","title":"Install in CodeSpaces","text":"<p>Let's use Conda (or optionally, Mamba)</p> <pre><code>conda install -c conda-forge singularityce\n\nmamba install -c conda-forge singularityce\n</code></pre>"},{"location":"singularity/intro/#singularity-cli","title":"Singularity CLI","text":"<p>Singularity\u2019s command line interface allows you to build and interact with containers transparently. You can run programs inside a container as if they were running on your host system. You can easily redirect IO, use pipes, pass arguments, and access files, sockets, and ports on the host system from within a container.</p>"},{"location":"singularity/intro/#help","title":"help","text":"<p>The <code>help</code> command gives an overview of Singularity options and subcommands as follows:</p> <pre><code> $ singularity help pull\nPull an image from a URI\n\nUsage:\n  singularity pull [pull options...] [output file] &lt;URI&gt;\n\nDescription:\n  The 'pull' command allows you to download or build a container from a given\n  URI. Supported URIs include:\n\n  library: Pull an image from the currently configured library\n      library://user/collection/container[:tag]\n\n  docker: Pull a Docker/OCI image from Docker Hub, or another OCI registry.\n      docker://user/image:tag\n\n  shub: Pull an image from Singularity Hub\n      shub://user/image:tag\n\n  oras: Pull a SIF image from an OCI registry that supports ORAS.\n      oras://registry/namespace/image:tag\n\n  http, https: Pull an image using the http(s?) protocol\n      https://library.sylabs.io/v1/imagefile/library/default/alpine:latest\n\nOptions:\n      --arch string      architecture to pull from library (default \"amd64\")\n      --dir string       download images to the specific directory\n      --disable-cache    dont use cached images/blobs and dont create them\n      --docker-login     login to a Docker Repository interactively\n  -F, --force            overwrite an image file if it exists\n  -h, --help             help for pull\n      --library string   download images from the provided library\n      --no-cleanup       do NOT clean up bundle after failed build, can be\n                         helpful for debugging\n      --no-https         use http instead of https for docker:// oras://\n                         and library://&lt;hostname&gt;/... URIs\n\n\nExamples:\n  From Sylabs cloud library\n  $ singularity pull alpine.sif library://alpine:latest\n\n  From Docker\n  $ singularity pull tensorflow.sif docker://tensorflow/tensorflow:latest\n\n  From Shub\n  $ singularity pull singularity-images.sif shub://vsoch/singularity-images\n\n  From supporting OCI registry (e.g. Azure Container Registry)\n  $ singularity pull image.sif oras://&lt;username&gt;.azurecr.io/namespace/image:tag\n\n\nFor additional help or support, please visit https://www.sylabs.io/docs/\n</code></pre>"},{"location":"singularity/intro/#search","title":"search","text":"<p>Just like with Docker, you can <code>search</code> the Singularity container registries for images.</p> <pre><code>singularity search tensorflow\n</code></pre>"},{"location":"singularity/intro/#pull","title":"pull","text":"<p>The easiest way to use a Singularity is to <code>pull</code> an existing container from one of the Registries.</p> <pre><code>singularity pull library://lolcow\n</code></pre> <pre><code>singularity pull docker://&lt;yourusername&gt;/cowsay\n</code></pre>"},{"location":"singularity/intro/#downloading-pre-built-images","title":"Downloading pre-built images","text":"<p>You can use the <code>pull</code> command to download pre-built images from a number of Container Registries, here we'll be focusing on the Singularity-Hub or DockerHub.</p> <p>Container Registries:</p> <ul> <li><code>library://</code> - images hosted on Sylabs Cloud</li> <li><code>docker://</code> - images hosted on Docker Hub</li> <li><code>localimage://</code> - images saved on your machine</li> <li><code>yum://</code> - yum based systems such as CentOS and Scientific Linux</li> <li><code>debootstrap://</code> - apt based systems such as Debian and Ubuntu</li> <li><code>arch://</code> - Arch Linux</li> <li><code>busybox://</code> - BusyBox</li> <li> <p><code>zypper://</code> - zypper based systems such as Suse and OpenSuse</p> </li> <li> <p><code>shub://</code> - (archived) images hosted on Singularity Hub, no longer maintained</p> </li> </ul>"},{"location":"singularity/intro/#pulling-an-image-from-singularity-hub","title":"Pulling an image from Singularity Hub","text":"<p>Similar to previous example, in this example I am pulling a base Ubuntu container from Singularity-Hub:</p> <pre><code>$ singularity pull shub://singularityhub/ubuntu\nWARNING: Authentication token file not found : Only pulls of public images will succeed\n88.58 MiB / 88.58 MiB [===============================================================================================] 100.00% 31.86 MiB/s 2s\n</code></pre> <p>You can rename the container using the --name flag:</p> <pre><code>$ singularity pull --name ubuntu_test.simg shub://singularityhub/ubuntu\nWARNING: Authentication token file not found : Only pulls of public images will succeed\n88.58 MiB / 88.58 MiB [===============================================================================================] 100.00% 35.12 MiB/s 2s\n</code></pre> <p>The above command will save the alpine image from the Container Library as <code>alpine.sif</code></p>"},{"location":"singularity/intro/#pulling-an-image-from-docker-hub","title":"Pulling an image from Docker Hub","text":"<p>This example pulls an <code>ubuntu:16.04</code> image from DockerHub and saves it to the working directory.</p> <pre><code>$ singularity pull docker://ubuntu:20.04\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 8f6b7df711c8 done\nCopying blob 0703c52b8763 done\nCopying blob 07304348ce1b done\nCopying blob 4795dceb8869 done\nCopying config 05ac933964 done\nWriting manifest to image destination\nStoring signatures\n2020/03/09 16:14:12  info unpack layer: sha256:8f6b7df711c8a4733138390ff2aba1bfeb755bf4736c39c6e4858076c40fb5eb\n2020/03/09 16:14:13  info unpack layer: sha256:0703c52b8763604318dcbb1730c82ad276a487335ecabde2f43f69a6222e8090\n2020/03/09 16:14:13  info unpack layer: sha256:07304348ce1b6d24f136a3c4ebaa800297b804937a6942ce9e9fe0dac0b0ca74\n2020/03/09 16:14:13  info unpack layer: sha256:4795dceb8869bdfa64f3742e1df492e6f31baf9cfc36f1a042a8f981607e99a2\nINFO:    Creating SIF file...\nINFO:    Build complete: ubuntu_20.04.sif\n</code></pre> <p>Warning</p> <p>Pulling Docker images reduces reproducibility. If you were to pull a Docker image today and then wait six months and pull again, you are not guaranteed to get the same image. If any of the source layers has changed the image will be altered. If reproducibility is a priority for you, try building your images from the Container Library.</p>"},{"location":"singularity/intro/#pulling-an-image-from-sylabs-cloud-library","title":"Pulling an image from Sylabs cloud library","text":"<p>Let\u2019s use an easy example of <code>alpine.sif</code> image from the container library</p> <p>Tip</p> <p>You can use <code>singularity search &lt;name&gt;</code> command to locate groups, collections, and containers of interest on the Container Library</p>"},{"location":"singularity/intro/#interact-with-images","title":"Interact with images","text":"<p>You can interact with images in several ways such as <code>shell</code>, <code>exec</code> and <code>run</code>.</p> <p>For these examples we will use a <code>cowsay_latest.sif</code> image that can be pulled from the Docker Hub.</p> <pre><code>$ singularity pull docker://tswetnam/cowsay\nINFO:    Downloading library image\n 67.00 MiB / 67.00 MiB [=====================================================================================================] 100.00% 5.45 MiB/s 12s\nWARNING: unable to verify container: cowsay_latest.sif\nWARNING: Skipping container verification\n\n$ sudo singularity run cowsay_latest.sif\n ________________________________________\n/ Expect a letter from a friend who will \\\n\\ ask a favor of you.                    /\n ----------------------------------------\n    \\   ^__^\n     \\  (oo)\\_______\n        (__)\\       )\\/\\\n        ||----w |\n        ||     ||\n</code></pre>"},{"location":"singularity/intro/#shell","title":"shell","text":"<p>The <code>shell</code> command allows you to spawn a new shell within your container and interact with it as though it were a small virtual machine.</p> <pre><code>$ singularity shell cowsay_latest.sif\n  Singularity cowsay_latest.sif:~&gt;\n</code></pre> <p>The change in prompt indicates that you have entered the container (though you should not rely on that to determine whether you are in container or not).</p> <p>Once inside of a Singularity container, you are the same user as you are on the host system.</p> <pre><code>$ Singularity cowsay_latest.sif:~&gt; whoami\ntswetnam\n</code></pre> <p>Warning</p> <p><code>shell</code> also works with the library://, docker://, and shub:// URIs. This creates an ephemeral container that disappears when the shell is exited.</p>"},{"location":"singularity/intro/#exec","title":"exec","text":"<p>The exec command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the <code>cowsay</code> program within the cowsay_latest.sif container:</p> <pre><code>$ singularity exec cowsay_latest.sif cowsay container camp rocks\n______________________\n&lt; container camp rocks &gt;\n ----------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p><code>exec</code> also works with the library://, docker://, and shub:// URIs. </p> <pre><code>singularity exec library://lolcow container camp 2022\n</code></pre> <p>This  creates an ephemeral container that executes a command and disappears.</p>"},{"location":"singularity/intro/#run","title":"run","text":"<p>Singularity containers contain runscripts. These are user defined scripts that define the actions a container should perform when someone runs it. The runscript can be triggered with the <code>run</code> command, or simply by calling the container as though it were an executable.</p> <pre><code>singularity run lolcow_latest.sif\n _________________________________________\n/  You will remember, Watson, how the     \\\n| dreadful business of the Abernetty      |\n| family was first brought to my notice   |\n| by the depth which the parsley had sunk |\n| into the butter upon a hot day.         |\n|                                         |\n\\ -- Sherlock Holmes                      /\n -----------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"singularity/intro/#inspect","title":"inspect","text":"<p>The <code>inspect</code> command will provide information about labels, metadata, and environmental variables.</p> <pre><code>singularity inspect lolcow.sif\n</code></pre> <pre><code>singularity inspect library://lolcow\n</code></pre>"},{"location":"singularity/intro/#build","title":"build","text":"<p>See Next Section for details of <code>build</code></p>"}]}